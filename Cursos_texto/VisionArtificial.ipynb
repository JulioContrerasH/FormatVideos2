{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el modelo\n",
    "model = whisper.load_model(\"medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model  # o cualquier otra variable que ocupe memoria GPU\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_min_sec(time_in_seconds):\n",
    "    # Devuelve solo los minutos como un entero.\n",
    "    return time_in_seconds // 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.transcribe(\"/home/contrerasnetk/Documents/Classes/VisionArtificial/1.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intervalo 0-10 minutos:   Y vamos a compartir la pantalla.  La número dos.  Vale, estamos todos. Podéis escribirme en el chat. Podéis interrumpirme también en el momento que quieran. Vale.  Que para eso son las clases.  Pues nada, bienvenidos al tema de percepción computacional o visión artificial.  Lo digo así porque tengo un grupo que está dentro de la materia de percepción computacional y otro grupo de visión artificial.  Ambas materias son similares y forman parte del máster de inteligencia artificial aquí en UMI.  Vale.  Esta materia la compartimos con el doctor Abdel Mohaid, que es el profesor que imparte la materia y también la revisa.  Y los doctores Julio Suarez Alvachez. Bueno, y el otro es Francisco. Se me ha ido el nombre, lo siento. Error mío.  Estos dos son profesores revisores. Vale. Y mi persona que soy el doctor Javier Rodrigo Villas-Santorrazas, que también imparto y corrigo.  Las materias. Vale.  Hoy vamos a abarcar lo que sería la presentación de la materia. Vale. Y vamos a ver los temas 1 y 2 también. Vale.  Entonces, comenzamos con la presentación de la materia. Me gustaría comentarles que el doctor Abdel, pues tiene una formación en ciencia de la computación e inteligencia artificial por la Universidad del País Vasco.  Es docente de las materias de percepción computacional, métodos numéricos, modelados sistemas dinámicos y dentro de sus líneas de investigación está el reconocimiento de patrones y la extracción de características y aprendizaje automático.  Vale. Igualmente el doctor Julio Suárez también es doctor en economía y empresas y tiene un doctorado también conjunto en tecnologías informáticas avanzadas.  Dentro del UNI, el dado ciencia en la parte de visión artificial, es director de TFMES y de otra materia más que ahora mismo no la recuerdo muy bien, pero vamos, que está muy capacitado para este tema.  Dentro de sus líneas de investigación está la parte de business intelligence y big data, la parte de gestión de conocimiento y dentro también la parte consultoría IT.  El majestadero Jaime González tiene un máster en inteligencia artificial en esta misma universidad, en esta misma casa.  Es docente también corrector, como sabéis, de la parte de visión artificial o percepción computacional, como quiera llamarlo.  También forma parte del grupo de directores de TFMES y dentro de sus líneas de investigación, pues muy similares a Julio, respecto a la gestión del conocimiento, data business intelligence y la parte de computación.  Por mi parte, yo soy Javier, soy doctor en telecomunicaciones y procesamiento de señales en la Universidad Politécnica de Madrid.  La docencia que he dado aquí es percepción computacional, visión artificial, NLP en el anterior cuatrimestre también, que no lo he agregado, y también soy director de TFMES.  De todas las líneas que manejo yo, pues al ser un profesor que da docencia y aparte está en la empresa privada, pues tengo líneas muy abiertas, sobre todo en el anto de proyectos de defensa, que es más, la mayoría de los ejemplos que vaya dando alrededor de la clase, pues irán en el área de proyectos de defensa, llamenlo OTAN, EDF, etc.  Y también proyectos de health care médicos, que en mi vida pasada también estaba en una empresa de proyectos enfocados en la parte de la medicina.  Manejo sistemas envervidos embarcados, GPU-CPU, sobre todo en el área de los medicamentos, en la área de la medicina, en el área de los medicamentos, en el área de la medicina, en el área de la medicina.  El contenido que tiene la materia se divide en seis grupos, lo que sería sistemas de percepción, que componen los temas sonidos, la digitación de ruido, el preprocesamiento de imágenes, el procesamiento de imágenes, preprocesamiento de imágenes,  sistemas de percepción que componen los temas sonidos,  la digitación de ruido, el procesamiento de imágenes,  el procesamiento de imágenes, reprocesamiento y después  procesamiento de imágenes, extracción de características  y toma de decisiones.  En cada tema nos vamos a ir planteando,  bueno, en cada grupo de temas nos vamos a ir planteando  preguntas que las vamos a ir resolviendo en el transcurso  de la pregunta, en el transcurso de las clases, perdón.  Y cada grupo va a servir para que ustedes comiencen a tener  una experiencia en esa área.  De cara a colocarlo en un currículum,  pues sepan que ya van a poder, por las características del  máster, agregar que sois expertos en una determinada  materia, en una determinada área.  ¿Por qué les digo esto?  Porque no necesariamente el hecho de saber inteligencia  artificial no significa que dejen a un lado el hecho de  conocer las herramientas necesarias para poder alimentar  esas imágenes y demás, sino también para interpretar o  comenzar a obtener esas características que les van a  servir para pasarlo a la red neuronal.  Entonces, el tema uno se verá la parte auditivo visual que  veremos hoy.  En el bloque dos serían los temas 3, 4, 5,  donde se ve un poco el muestre de cuantificación de señales,  las fuentes y tipo de ruido, la detección y cancelación de  anomalías.  Porque si se dan cuenta, al día de hoy todo lo que nos viene a  los ojos, al oído, siempre tiene ruido o anomalías.  Entonces, hay que comenzar a quitarlas,  a filtrarlas para poderlas meter dentro de nuestro procesado,  perdón, dentro de nuestra red neuronal.  Entonces, vamos a aprender todo esto.  Igualmente, vamos a ver en el bloque tres las transformaciones  de intensidad y filtrados espaciales,  utilizando procesado de histogramas,  operaciones ariméticas, formaciones de intensidad y  algunas otras operaciones.  Igualmente, el bloque cuatro tendremos la morfología  matemática, a lo mejor un poco más con conocimiento un poco  más profundo en la parte matemática,  más de cálculo para poder analizar también la  transformada FUYER y sacar algunas técnicas de  segmentación de imágenes a través de transformadas  parlet también.  En la extracción de características que son los  temas 11, 12 y 13, pues vamos a poder implementar todas estas  herramientas que vamos a ir mostrando a lo largo del curso  para poder extraer esas características,  esos pequeños detalles que van a servir a la hora de alimentar  nuestra red neuronal.  Igualmente, pues sería la opción de detectar texturas,  rugosidades en imagen, que nos sirvan para poder capturar  información.  Sacar transformadas rápidas de FUYER,  usar transformadas wavelets para poder tener un análisis  tiempo frecuencial más minucioso, etcétera.  Y finalmente, pues, por ejemplo, vamos a ser capaces de poder  escuchar el sonido, por ejemplo, del corazón,  aprender a clasificarlo, utilizar algunos algoritmos y poder  discernir entre lo que es normal o que es anormal,  o incluso analizar una imagen y poder encontrar  el ruido que hace que la imagen esté más blurry, más borrosa,  filtrarla y poder separar tanto ruido como imagen buena.  Con todo esto, pues, indicarles, vamos a tener una serie de  actividades que van a presentarse a lo largo del curso,  también la evaluación.  Las actividades, pues, son dos laboratorios con fechas  de finalización de presentación de estos laboratorios,  pues, una del 6 de mayo y la otra el 17 de junio, ¿vale?  Son laboratorios muy enfocados en la parte de visión artificial,  computer vision y demás.  Están muy enfocados en esa área, ¿vale?  Y tenemos un trabajo grupal, ¿vale?  Que el deadline es el 27 de mayo, donde se presentan más que todos  los filtros espaciales y morfológicos.  Todo esto lo vamos a ver en clase, entonces,  no es algo que sea muy de sorpresa a la hora de resolver las actividades.  ¿Vale?  Los exámenes están previstos para la semana del 8 al 12 de julio  y el examen extraordinario, pues, para el 8 y 10 de septiembre, ¿vale?  De todas formas, tenéis a vuestros mentores o tutores que les ha  facilitado la universidad por si tenéis alguna duda de estas fechas y demás, ¿vale?  Igualmente, en el foro vamos a ir exponiendo nuestras dudas,  las dudas que tengan, a lo mejor contestando algunas preguntas que  haga yo en clase y demás, para que a la hora de hacer la evaluación,  pues, pueda yo saber cuáles han sido los alumnos que más han participado y,  bueno, si les faltase un 0,000, un punto para redondear y tener un aprobado,\n",
      "Intervalo 10-20 minutos:  pues, que sepa es que yo siempre valoro eso, vuestra participación.  A ver, Felipe, que dice, buen día, Javier,  ¿qué trasfondo matemático se quiere para abordar este curso?  A ver, los conocimientos básicos, saber integrar, derivar, básicamente,  porque tampoco nos vamos a ir a enseñar que es una librada, una transformada y demás.  No son cosas que en principio por vuestra formación debería saberlo,  pero si tenéis alguna duda o deficiencia en ello,  pues, decídmelo en los foros para hacer una pequeña explicación o incluso lo que yo  suelo hacer en todas las presentaciones, bueno, casi todas las presentaciones que doy,  es colocar vídeos explicativos porque como todo está en la red y tampoco vale la pena,  que, bueno, no es que valga la pena, pero es que en los 45 minutos de clase  tampoco puedo ir a muy grano fino en algunas explicaciones.  Entonces, coloco algunos vídeos de distintos autores que están referenciados donde ustedes  pueden un poco profundizar. Es más, en el cuatro mese anterior alguno de estos vídeos  les ha ayudado más a entender alguna transformada o incluso a resolver ejercicios o parte del examen,  ¿vale? Entonces, por ese lado no tengáis problema.  A ver, buen día, doctor. Bueno, doctor, soy Javier, o llámame Javi, ¿vale? Soy latino igual que ustedes,  yo soy boliviano, pero llevo años aquí en España, pero llámame Javi o Javier. Doctor,  dejemos el formalismo para otras cosas. Estamos en relación alumno-profesor con Javier y yo estoy  segurado. Vamos a desarrollar modelos propios, usar modelos de visión como los que están en  Face. A ver, ten en cuenta que esto de visión artificial o de percepción computacional no  vamos a desarrollar redes como tal, ¿vale? Se puede decir que nosotros nos vamos a centrar  en todo lo que sería el data quotation, o sea, el preprocesado antes de la red neuronal. ¿Por qué?  Porque es importante tener muy clara esa parte a la hora de abordar el problema, porque a fin de  cuentas para hacer toda la parte de algoritmos de computer vision, pues puedes desde un YOLO,  hasta un Jumbo in Face o lo que sea, hasta un MobileNet y demás, que ya me realiza las cosas.  Pero si no conozco primero las tipas de cómo lo hace y demás, pues para mí hacer esa implementación  pues va a ser un poco sin sentido, porque es algo que me va a quitar, bueno, no me va a quitar  conocimiento, pero no me va a aportar nada más que, bueno, implemento esto y ya está. O sea,  cómo funciona la caja negra, una entrada y otra salida, pero si mi entrada tiene mucho ruido,  no está bien puesta y demás, pues no me sirve. O si tengo pocos datos, ¿cómo podría ser para  quitarles el ruido para que me sirva como más o menos, o para que me sirva para aumentar mis  datos que tenga? O básicamente, ¿cómo obtener imágenes idóneas para poder alimentar la red neuronal?  Porque ustedes saben también que mientras más ruidos tenga la red neuronal en la entrada o menos  imágenes óptimas tenga, pues mi red neuronal va a ser lo que medianamente pueda. Pero si dejo todo  comidito, bien digerido, hacer el data correction de tal forma que mi red neuronal ingeste una cantidad  buena de datos, pues mira, es lo mejor. Y por otro lado, recuerden, si tengo mil imágenes para hacer  mis entrenamientos y esas mil imágenes son mil imágenes que están con basura, ruido y demás  cosas, pues lo que haga la red neuronal con esas mil va a ser básicamente nada.  ¿Transformada rápida de Fourier? Sí, sí vamos a ver. La asistencia ahora lo vamos a hablar.  Profesor, bueno, Javier, recuerda, ¿qué tanto tenemos profundizar en el cálculo diferencial  integral? Pues yo les digo, siendo sinceros, contar que sepamos bien aplicar las fórmulas y  sepamos para qué la integral, para qué la derivada y demás es suficiente. ¿Por qué? Porque a la hora  de realizar el examen yo no te voy a decir, sácame la integral de esto, sácame la derivada. No,  yo te voy a decir, trata de implementar esto, que me coloques la ecuación y me la desarrolles con  los valores que tengas que hacer y me la dejes ahí. No te voy a decir que me la resuelvas por  partes. ¿Por qué? Porque se desarrolló, lo puedes hacer en MATLAB, lo puedes hacer en Python,  en cualquier lenguaje de programación. Pero si sabes el concepto de cómo implementar esa ecuación,  por ejemplo, esa ecuación de transformada de Fourier, esa ecuación de anomalías o ese filtro  cómo debería implementarse y me lo pones bonito en forma ordenada, para mí eso ya es una pregunta  correcta. Aunque al final te hayas equivocado en resolver la ecuación y demás, es decir que en  vez de 0,1 has puesto 0,01 y solo por copiarlo mal en la calculadora, te has dado 0, pero el  resto está bien. Para mí esa es una respuesta correcta. Entonces no es que vayamos a profundizar,  vamos a utilizar todas las herramientas y a la hora de dar el examen vamos a ver la mejor forma de  mostrar los conocimientos sin profundizar en temas que no vienen al caso. Porque ya les he  dicho, eso se puede resolver de una forma u otra utilizando MATLAB, Python y demás. Yo tengo una  formación eléctrica y electromecánica, pero la matemática se varía en uso lo practico, presupuesto,  pues mira, contar que sepan manejar un poco la calculadora, te digo es así, realiza, no nos vayamos  por la ronda. Si digo el hecho que sepamos un poco de conocimientos de integrales, o sea la parte  matemática de la parte de cálculo es para que no les parezca chino que es una sumatoria que es la  integral que la derivada y demás. Para los análisis de mediciones de presión acústica,  como los numeros sonómetros, hay algoritmos, ¿qué será parte de las funciones de este año de mi trabajo  de metrología y energía? Pues sí, pero bueno, ya vamos a ir viendo. Lo que sí te pido es que si tienes  alguna pregunta específica justamente en la parte que veíamos, que concuerda con tu trabajo y demás,  pues dilo, podemos buscar ejemplos para que se pueda resolver en clase y así no solo tú,  sino todos ustedes y podamos sacar la parte práctica de lo que estamos viendo en teoría.  ¿Y qué lenguaje vamos a utilizar? Pues por mi parte podemos utilizar el lenguaje que nosotros  tenemos. Yo sé Python, C++, por lo tanto Octave, R también lo controlo, C, C++,  no es una de mis partes, pero también lo sé, pero la cuestión es que sepamos hacer todo eso.  Yo la mayoría de los ejemplos los voy a plantear siempre en Matlab o en Python,  porque saben que toda la parte de OpenCV y Computer Vision nació del lado de,  bueno, en C++ y después se pasó a Matlab y de ahí comenzó la transformación a Python,  pero Matlab es la forma más rápida desde mi punto de vista para resolverlo.  ¿Qué se puede hacer en Python? Pues bien lo podemos hacer en Python. En los laboratorios que  tengamos, si yo lo haré en Matlab, si alguno quiere compartir su pantalla y mostrar cómo lo ha  desarrollado en Python, lo puede hacer, yo no tengo ningún problema. Lo que importa es que ustedes  sepan utilizar las herramientas, no importa el lenguaje de programación, y entiendan para qué  nos sirve todo esto. Sí, eso es lo principal. Simulink, pues bueno, si quieres, dependiendo  cómo te lleves con el Simulink, yo soy más de picarle al código, a tecle tecle, como quien dice,  pero vamos, como queráis. Con respecto a los cálculos, para resolver una ecuación a partir  de código, pues a ver, no te voy a decir que me resolvas una ecuación diferencial,  pero sí te voy a decir que puedes utilizar tales métodos en Matlab o en Python para resolver ese  problema de filtrado. Algo de detección, vamos a ver un poco de detecciones, pero la detección,  no desde el ámbito de la red neuronal, sino como se decía antes, a través de detección de  características. Vale. A ver, damos un segundo, la plataforma de UNI y hasta los temas sumidos de  Abtl subidos. Sí, solo que yo los voy a enfocar desde mi punto de vista, que puede que sea un poco  distinto al profesor Abtl. ¿A qué me refiero con esto? Que cada quien tiene la forma de explicar  las cosas. Sí, entonces yo lo explico de una manera, él lo explica de otra forma. Lo que importa  es que todos los alumnos sepan entendernos. Renzo, sí, OpenCV es la base. ¿Tenemos algún  recurso para usar el Matlab por parte de la UNI? A ver, creo yo que por tener la cuenta de UNI,  pueden optar a tener el Matlab de modo académico y también el Office de modo académico. Lo digo  de cara a vuestros Tfm. Entonces creo que les abre la puerta. Creo que no son todas las toolboxes,  pero creo que tienen esa opción. Si no, pues Python. Si no, también el lenguaje que queráis,  pero Matlab creo que tienen eso. Subí la presentación antes de las clases para tomar  notas. Pues José, normalmente yo lo subo después. Vale. Más que todo porque a veces saco algún ejemplo  de mi vida cotidiana del trabajo. Vale. Que puede ser muy importante a la hora de explicarles como  ejemplo. Pero si no, pues te atreveré a hacer. Vale. Bueno, seguimos que nos hemos quedado aquí,\n",
      "Intervalo 20-30 minutos:  tenemos que ir avanzando. ¿Vale? ¿Hay alguna versión o edición mínima de Matlab recomendada?  Tenerse el Matlab online. Vale. Si te refieres a eso. Para la evaluación, continua. Ya saben que  el matlab es el 40% y el examen 60. Las tres actividades se puntúan el grupo al 3 puntos,  el laboratorio 5 y el otro laboratorio individual 5 puntos. Vale. Y los otros dos puntos,  lo que sería la asistencia. En total 15 puntos. Pero todo esto nos lleva a tener como máximo 10  puntos. Es decir, cuando lleguen ya a 10 puntos, sientanse tranquilos que su evaluación continuada  sería del 40% y ya sería estudiar el examen para tener el 60. Vale. ¿Hay alguna versión? Bueno,  esto es lo que me habían dicho de la puntuación. Ya sabéis. Entonces, si resuelven bien las  actividades, ya tenéis 13 puntos. Por asistir a los cursos que creo que son dos asistencias  virtuales, pues ya tienen los dos puntos. Entonces sumados, pues tenemos los 15. Vale. Y como saturamos  como tal, pues sería solamente tener un máximo de 10. Vale. Después, las entregas de las actividades.  Se van a hacer en formato PDF. Cuando digo PDF es PDF. No word. Vale. Quedarse con eso. No se les va  a pedir el Jupyter, el notebook de MATLAB, el código en C y demás. Salvo que yo les diga,  podéis presentarlo de cara a si algo no está bien explicado en su documento, pues me veo el código  y podemos subir un poco la nota. Vale. Y si detecto plagios, pues la primera vez cero y la segunda  completamente suspenso de la asignatura. Tened cuidado con esto porque incluso detección de chat  GPT o el no detectable chat GPT lo tenemos muy bien controlado la mayoría de los profesores. Así  que por lo menos darles bien la suelta a vuestra imaginación. Vale. A ver, los dos puntos de asistencia  sincronada. Pues no, en principio no. Por favor, si no se considera la asistencia, no sé, lo que  está puesto ahí son las reglas de juego y una de las puedo cambiar. Y el turnitin sí, también lo vamos a  utilizar. Vale. Y el turnitin, recordar que el turnitin ya tiene control de plagios de este tipo.  Vale. Lo digo para que sean también conscientes de ello. Y después estamos ahí con problemas,  la última. Vale. Entonces, en la metodología, pues saben que tiene material disponible, el audiovisual,  apuntes, la presentación de los temas en los foros y también pueden profundizar con la bibliografía  y artículos científicos e incluso algunos vídeos que voy a ir pasándoles en las presentaciones. Vale.  En qué parte probamos el turnitin? El turnitin no le llega a ustedes, me llega a mí. Vale. Es que es  así, es una herramienta para el profesor, no para el alumno. Vale. Aunque hay algunas herramientas  tipo turnitin que son libres en la web, en las cuales lo pueden utilizar. Bueno, en presentación  general, pues tenemos un contenido teórico todas las semanas, más las presentaciones virtuales  todas las semanas. Hay test, que hay uno por tema que lo tiene que ir resolviendo. Hay dos  trabajos puntuales y un laboratorio de dos horas que tenemos. Y aparte vamos a tener, creo una  clase para resolver dudas previas al examen. Vale. Y nada más alguna duda que tengáis sobre este  punto, sobre la presentación de la clase antes de comenzar con el siguiente tema. Una asignatura en  Mac. A ver, ahora mismo ustedes están viendo un slide en PowerPoint, pero yo trabajo en Linux y Mac  no hay ningún problema. O sea, no. Lo único en el tema de a la hora de dar el examen, recordad, y eso  para todos, cuando deis el examen solo tener la documentación abierta, la que tenemos, navegadores,  el justo necesario para dar el examen y demás. Y si acaso la calculadora que da Windows, no habráis  ninguna otra web del navegador, ni WhatsApp online, ni Slash, ni otras cosas. ¿Por qué? Porque el sistema  de de corrección para ver si están copiando o no es muy amplio y al detectar otras páginas que no  sean las que se debería tener para el examen, pues las puede tomar como innecesarias y problemáticas.  Entonces, pedir en su contra. ¿Qué horarios del curso? Pues a ver, normalmente yo suelo coger esta  hora los viernes, pero dependiendo también del trabajo y algún problema que pueda tener, pues se  va a ir moviendo. O sea, no te puedo garantizar un horario fijo. Lo siento. ¿Javier qué dices? Buenos días.  Los laboratorios pueden ser diferidos o solo presenciales. Los laboratorios son presenciales,  pero vamos a hacer igual que ahora. Si no puedes acceder, pues lo podrás ver. Las actividades puedo  desarrollar en el lenguaje de programación que yo elija, es decir, en mi caso Python,  puedes usar Python. Si lo quieres hacer de mala, también, les he dicho, no tengo ningún problema  de poder pedirles que me presenten su código y analizar incluso nada. Si alguno quiere hacerme  el sistema operativo PyCoS ARIEN 653 de Avionica, también puede hacerle. O sea, no te no prenden.  La cuestión es que sepan implementar todo lo que vamos a aprender en el lenguaje de programación  en el cual se sientan más cómodos para seguir avanzando. Más temprano me sirve a mí por trabajo.  Bueno, a mí más temprano no me sirve mucho, pero bueno, de momento es lo que puedo decir. A lo mejor  lo puedo hacer más tarde también. No lo sé. Tal vez puedan dejarles la referencia de que se van a dejar el examen.  Será práctico, teórico. A ver, el examen, hay preguntas de, por ejemplo, cómo utilizar un filtro  morfológico o implementar este filtro o desde tu punto de vista cómo mejorarías esto. Aún  si dando siempre ejercicios que lo coloca el doctor Abdel, donde ustedes van a poder ver los  tipos de ejercicio tipo examen que solemos dar. La intención es poder organizar correctamente,  que significa el contenido bastante extenso y de recorrer. Pues ahí no te puedo decir mucho.  ¿Algún ejemplo de preguntas examen? Pues ya te las he dado. Para escoger un tipo de trabajo en  el sistema de unir, lo selecciona aleatoriamente o se dan oportunidad. Creo que lo dan aleatoriamente.  Los cálculos no son muy complicados, sino el enfoque más bien a la análisis. Exacto,  el análisis. Va a ser más el análisis. Te voy a pedir que me hagas un cálculo que puede ser  más sencillo o más complejo. Bueno, no tan complejo. A lo mejor, reemplazar los valores en  una ecuación determinada. Por eso les repito, siempre quedarse con las ecuaciones y a partir de ahí,  pues dar tu punto de vista de si mejora o empeora el filtro de la imagen y demás.  ¿Los algoritmos de actividades y proyectos son nivel avanzado por mi rama electromecánica? No,  no son nivel avanzado. Ten en cuenta que esto es un máster. Algo que tienen que tener en cuenta  es lo siguiente. A veces se confunde. Tú cuando estás en la universidad, el profesor está detrás  de ti. Entonces te dan toda la documentación, todo para que estudies y para que profundices lo  que quieras. Cuando haces el máster, te dan las pautas necesarias para que tú tengas, si quieres,  obligación o necesidad, si quieres profundizar en algo o ir avanzando en algo en una parte más  específica. Pero ya no es mi deber de obligarte a ti a que lo hagas. Tú llegas hasta el nivel que  tú o tu exigencia te lo permita. Así como el doctorado y es que tú mismo te marcas hasta tu  propia hipótesis. Entonces un máster tiene ese enfoque. No piensen que vamos a estar detrás de  ustedes para que profundicen algo que a lo mejor no lo van a utilizar en su trabajo o en sus proyectos,  pero les vale conocer por lo menos las pautas necesarias para que no les pide el toro a la hora  de implementar o que su equipo de trabajo implementa. Entonces nosotros damos sólo eso.  Se verán filtros digitales, la matemática asociada a señales, sistemas con convoluciones  claro sí va a haber eso. Pero recuerda lo vamos a hacer muy por encima de los vídeos explicativos  para que ustedes quieran profundizar un poco pero tampoco sin irnos más a detalle. La matemática no  me complica pero no es muy fuerte. Me tocaría estudiar de nuevo. ¿Sería recomendable que un  TFMS sea en Estaria? Eso lo vas a ver tú. Dependiendo del área a la cual te sientas más a gusto,  tu TFMS va a ir afocando en eso. Vale. Yo les digo, de los alumnos que he tenido en el coche más  de anterior, seis me han planteado hacer un TFMS conmigo de los ciento y pico. He aceptado  a los seis, más que todo porque eran temas que me gustaban, muy centrado a la filtrado de imagen.  Pero si a lo mejor te gusta algo de NLP o también series, pues tienes que enfocarlo a eso. Ten en  cuenta que el máster te va a abrir un poco el abanico de opciones en la cual te puedes entrar  la parte de guía y depende de ti a qué lado se esgar para especializarte y según tu trabajo,  tus expectativas. No necesariamente puede ser esta materia. Yo estoy en el rubro de producción de  materias primas. Me encantaría explorar aplicaciones para hojas de tabaco. Pues mira,  aquí puedes sacar bastantes cosas, sobre todo para el filtrado de imágenes que después te\n",
      "Intervalo 30-40 minutos:  va a servir para alimentar el procesamiento de imágenes de computer vision o de reines neuronales  aplicadas al computer vision. Vale, entonces vamos a comenzar con el tema uno y dos.  Sí, porque ya llevamos mucho tiempo y ya tenemos que ir avanzando.  Dame un segundo que se me ha quedado pillado el...  Yo soy ingeniero ambiental y pensé hacer el TFM enfocando en detectar claras con imágenes  en drones. Bueno, te diré que he trabajado con drones. Bueno, sí que he trabajado con drones.  Entonces te puedo dar algunas pautas. Eso sí, si vas a usar un drone y quieres implementar  algunas inferencias de IA, utiliza los ordenadores en bebidos de NVIDIA. Es lo mejor que puedes hacer.  Son sistemas COTS que te ahorras dinero y procesamiento.  ¿Algoritmos genéticos? Sí, puedes implementar algoritmos genéticos también.  No se van a ver porque eso depende, creo que de otra materia, pero vamos, si tienes alguna  duda con genética algoritmos yo te puedo ayudar porque he hecho la tesis aplicando eso.  La tesis doctoral. Cuando digo tesis, aquí se refiere a tesis doctoral.  Tengo el desarrollo de un modelo matemático usando método de zona mínima para estimar  defectos en planitud y sin certidumbre. Al ser un problema no línea y complejo,  pues todos usan IA o algoritmos para ese modelo. Utilizan también a lo mejor algoritmos  multiobjetivos genéticos también para realizar los YATSA. O sea que no descartaría más un  multiobjetivo genético para eso, más que un IA. En mi opinión.  Es bioinformática. ¿Qué pasa con bioinformática?  ¿Algún kit de NVIDIA recomendado que quieras? El último kit, el de la Chechson Orin. ¿Sabes  que son baratos? Bueno, baratos, cuestan 2000 pavos, pero están bien. Yo los he trabajado bastante.  Yo he trabajado con termográficas y me imagino que me haces un  referente para que pueda ser claro. Es que con eso vas sacando las características.  Ten en cuenta la termográfica, te puede dar calor y metida en blanco y negro en varios colores,  pero tu análisis mejor si lo haces en blanco y negro. Bueno, o sea escala de grises.  Soy microtónico. Tengo ya un poco de conocimiento de deep learning.  Estaba entrando a hacer ojos bien y tendría algún alcance. Pues sí, bueno, habría que ver  cómo sería el planeamiento, pero vamos. Yo trabajo en Banque y quiero utilizar  visión artificial para identificar imágenes de documentos transaccionales que se podría hacer  con visión artificial. Bueno, OCRs y demás. Sí, a ver, aquí te va a servir para tomar  características, encontrar líneas, curvas y demás. Hay alguna materia donde sacamos  características. Sí, un tema donde sacamos esas características que te van a venir bien.  ¿Algún modelo de NVIDIA puedes recomendar? Nos vendía el NVIDIA.  ¿Algún modelo de NVIDIA puedes recomendar? Nos vendía por el chat un modelo de la tarjeta  o procesador. Lo que te he dicho, la JetSonic Ring, es la última que han sacado el año pasado  y todavía sigue en vigente. Me iría por versiones más pequeñas donde tú puedas manejar un poco  más la circutería. Bueno, entramos con el tema 1 y 2 que nos queda una hora de tema y tenemos  que avanzar bastante. Vale.  Bueno, entonces vamos a ver los sistemas de percepción. Nos vamos a plantear siempre un  problema, como os he dicho en cada bloque. El problema planteado o la pregunta es cómo podemos  optimizar la integración de los mecanismos de capacitación de señales auditivas y visuales  de un ser humano en el diseño de sistemas de percepción computacional. Es decir, cómo nos  va a servir todo lo que sabemos del ser humano, la parte de pista y oídos, para implementarlo en un  ordenador. Cuando digo ordenador digo un computador. Para resolverlo, pues, vamos a ver estos temas 1 y  2. El tema 1 vamos a ver la introducción, vamos a ver los objetivos, el sistema auditivo humano,  la visión humana y algunas herramientas. En la introducción, pues, hay que recalcar la idea de  percibir. Percibir es recibir algo y encargarse de ellos. Es captar los sentidos, imágenes a través  de los ojos, el tacto o el tipo de texturas, a través del tacto de los dedos. Bueno, cuando digo  tacto en realidad es de todo el cuerpo, toda la piel o las sensaciones externas. Y también es la parte  de comprender o conocer algo. En este sentido, pues, los elementos necesarios para un sistema  de percepción sería un interfaz entre el mundo exterior, que sería la captación de estímulos o  la captura de información por parte de sensores específicos. Aquí hay que tener en cuenta muy  claro el hecho de sensores. Quédense en la cabeza. Por otro lado, el procesador, que va a ser el  tratamiento de información, que va a ser una caracterización y después un decisor que nos  va a servir para la interpretación o la toma de decisiones a estas expertas. Entonces, con todo esto,  construimos sistemas tomando como referencia al ser humano. Había uno, no me acuerdo ahora mismo  el nombre, parte que dijo que iba a ser un ojo biónico y demás, pues, está imitando un ojo.  Se está imitando un ojo humano. Una cámara es imitar un ojo. Un micrófono es imitar más o menos  un oído. Una altavoz es imitar la boca. Una altavoz, una bocina, un parlante con quien  llamarlo. Entonces, en este sentido, vamos a capturar sonidos, luces, es decir, imagen,  tacto, olfato y gusto para tener sensores. A día de hoy hay sensores sin gusto del olfato. Hay  una nariz electrónica que hay aquí en el laboratorio de investigación del Consejo  Superior de Investigaciones Científicas que es capaz de detectar el aroma que produce cuando  una persona está fumando un porro, es decir, un cigarrillo de marihuana para que vean el alcance  que hay. Ahora, los que me habéis hablado de imágenes de hojas de tabaco y demás, aparte  de una imagen RGB de toda la vida, también tenemos cámaras que pueden detectar infrarrojos  o multiespectrales y demás. Entonces, ¿cómo vamos a construir estos sensores o estas interfaces  del mundo exterior? Generalmente son sistemas artificiales que van a manejar señales eléctricas.  Cuando digo señales es porque todos son señales. No sé quién ha puesto la traducción automática,  yo no la he puesto. Si alguien la ha colocado, por favor quitarla.  Bueno, todos son señales. Ustedes saben que las distintas gamas de colores son señales,  lo que oímos son señales y lo que vemos son señales. Todos son señales. Entonces,  para esto necesitamos traductores que me conviertan esa señal original en una señal eléctrica que  sea capaz de ser procesada por nuestro sistema. Entonces, cámara, luz, tengo como resultado mi  imagen digital. El micrófono me va a capturar el sonido y después va a producir una señal.  El termómetro voy a capturar la variación de temperatura y voy a generar un valor o un  rango de valores. La presión lo mismo, la presión arterial. Y ahí podemos extendernos entre los  sensores que detectan las señales del corazón, que no dejan ser señales. Entonces tenemos  todas esas características. Hay que tener en cuenta también que tras esta captación de información  del exterior, esto se tiene que registrar y se tiene que procesar en distintas etapas. Es decir,  convertir de la parte analógica del mundo real en algo digital. Hacer un muestreo,  hacer una quantificación, hacer una eliminación de ruido, hacer una extracción de información,  sacar todo eso y tenerlo listo para procesarlo. Entonces tenemos que ser capaces. Incluso ahora  mismo deben estar escuchando un ruido. Hay un ruido presente, no sé si lo podéis detectar.  Hay un ruido presente en mi entorno. Estoy en mi casa, el ruido de mi hija que está coleteando y  demás. Entonces existe todo eso. Y para eso tenemos que ser capaces de poder detectar esos  ruidos y poderlos controlar. Por otro lado, el tratamiento de características de la información  nos ha permitido también describir el comportamiento y extraer estos atributos.  Ah, vale. Bueno, no se escucha nada, pero que sepáis que hay ruido aquí presente. Después,  finalmente vamos a emular este aprendizaje humano y vamos a optimizar la respuesta y las  decisiones ante estos estímulos. O sea, vamos a recopilar ejemplos y vamos a construir sistemas\n",
      "Intervalo 40-50 minutos:  expertos que nos van a ayudar a emular y ayudar a reducir todos estos ruidos o a interpretar  estas imágenes. Vale. Durante esta asignatura de percepción computacional barra visión artificial,  vamos a abordar distintas etapas en la construcción de sistemas artificiales,  es decir, no humanos. Nos vamos a centrar principalmente en audio y en imágenes,  más en imágenes que audio. El audio nos va a servir un poco para entender a lo mejor el ejemplo  simple que extrapolado en dos dimensiones es nuestra imagen. Y finalmente, pues construiremos  sistemas capaces de extraer la información e interpretar señales de entrada. Vale.  Con todo esto, pues algunos ejemplos que vamos a ir viendo es el diagnóstico médico,  la visión artificial, reconocimiento de voz y un mantenimiento proactivo y preventivo. Vale.  Ahora, por ejemplo, díganme qué es lo que ven en esta imagen. Díganme si alguien me puede decirme  dónde estamos o de dónde es esta imagen. París. Ahora dime por qué sabes que es París o por qué  saben que es París. Desde la notedad. Vale. Por la Torre Eiffel. La pregunta es la Torre Eiffel y la  Gárgola. Vale. Pero si, por ejemplo, hubiéramos visto solamente esta parte, vale, voy a activar  el puntero. Vale. Si solamente les hubiera mostrado esta parte del canal y esta zona de la ciudad,  ¿qué hubiera sido? No se hubiera podido identificar. Hubiéramos pasado de París,  hubiéramos pasado a Copenhague, Venecia no creo porque Venecia no se parece a nada,  pero hubiéramos pasado a otros sistemas muy distintos. Vale, otras ciudades distintas. Entonces,  el hecho de tener una imagen y poder observarla o poder tener la imagen nos va a ayudar a  identificar primero el lugar a través de algunas características, localizarlo después y finalmente  reconocerlo. Y todo eso utilizando el lugar, los objetos o personas que hayan eventos y reconocer  objetos e incluso ya darnos cuenta de lugares. A ver, muy difícil identificar porque faltaba  contexto. Lo que estamos hablando. Todo eso es gracias a que tengamos toda la información para  identificar, localizar y reconocerlo. Y con todo eso a aprender de una forma selectiva,  una fuente visual entre muchas. Tendríamos un grupo de candidatos y podríamos confirmar.  Exacto. Muy bien, Hernán, muy bien. Pero se dan cuenta que solamente con esta imagen ya nos  hemos entrado en algo de la materia. Muy bien, vamos bien. Ahora, las áreas de conocimiento sobre  las que nos vamos a apoyar. Biología, imágenes médicas geniales, teoría de señal, porque vamos  a tener que entender un poco las señales que vayan generándose. Estadística, probabilidades,  vamos a tener que tocarlo. Aprendizaje automático que tiene una materia también. Y la parte de  computación, ¿por qué? ¿Dónde lo voy a implementar? En un ordenador, en un computador. Ya sé que en  Latinoamérica se dice computador y yo suelto a veces el ordenador y les pido disculpas por ello,  ya mi entorno pues ya es más español y manejo más ese lenguaje. Pero, por favor,  entenderme cuando yo diga ordenador, me refiero a un computador. Vamos a ver el sistema auditivo  humano. ¿Qué es el sonido? Es una perturbación mecánica del medio elástico. Entonces,  el sonido se propaga en el aire y mueven las ondas, se mueven las ondas. Y en el agua uno lo ve  mejor porque cuando hace un movimiento en la piscina, en la playa, ve cómo las ondas son un  movimiento mecánico que transmite una fuente de mecánica, una fuente de energía. La perturbación  se va a ir propagando siempre en forma de ondas, muy en el ambiente e incluso en el agua. Y por  otro lado, el sonido resulta de la vibración de ida y vuelta de las partículas en el medio a  través de la cual se mueve dicha onda. No es una onda de presiones. Si nos vamos a ampliar la  frecuencia del sistema auditivo humano, llegamos al ultrasonido y ya no lo detectamos. Pero con  el ultrasonido ya podemos detectar muy claramente algunas imágenes. Porque por el choque de las  ondas recibo ese choque y ya puedo discernir y formar una imagen. Entonces, a través de una  imagen o de una señal en una dimensión es factible sacar imágenes en dos dimensiones.  ¿Vale? Señales, todas señales. Por eso en el vacío no hay sonido, no hay medio. Exacto,  no hay un medio en el cual se transmita. A ver, el oído. Es un órgano encargado de transmitir  sonidos hasta el cerebro. Eso lo sabemos. Tiene tres partes. Oído externo, el oído medio y el  oído interno. Los martíleos de un castillo. Donde se detecta y convierte las señales sonoras  e impulsos eléctricos capaces de ser conducidos a través del sistema nervioso. A ver, esto es el  funcionamiento del oído externo. ¿Cómo funciona el pabellón auditivo? Como una antena. El conducto  auditivo, como una guía de ondas. Y los tímpanos, pues, son las que me sirven para afinar ese sonido,  para poderla ya pasar a impulsos eléctricos que intentan meter.  Como los ecolocalizaciones de los murciélagos o también de las velugas. Las velugas también  tienen ecolocalización. ¿Cómo funciona el oído medio? Pues, en la cadena de huesos, del martillo  de yunque estribo, tiene una ventana oval y todo. O sea, esto es teoría un poco de cómo sería el  oído. ¿Cómo estaríamos amplificando el sonido en la ventana oval? ¿Y cómo las membranas que  brindan de forma opuesta a la ventana oval es la ventana redonda y la trompa de ostaquio? Pues,  es la parte del conducto que sirve para equilibrar las presiones a las dos caras del tímpano. Por  eso, cuando nosotros tenemos ese problema de laberintitis, que a veces estamos muy mareados  y demás, o cuando una persona bebe bastante, pues la trompa de ostaquio es la que se queda ahí en  desequilibrio y uno ya pierde conocimiento. Eso respecto al oído. La parte del oído interno,  pues la conexión con el cerebro, los impulsos eléctricos que he dicho, donde contiene la  cóclea como elemento principal. Y esto seguramente si hay alguno que decía que bioinformática y  demás o en general de biomedicina, saben que al día de hoy hay implementas simplemente,  hay un implante coclear, creo que está bien dicho, coclear, en el cual no es más que un chip que está  dentro del cerebro y se alimenta a través de una alimentación eléctrica que se va apagada en la  parte posterior de la oreja. Aquí en España hay bastante, en Europa. Entonces, sirve para emular  todo esto, para que se ubiquen hasta el oído humano y está siendo casi reemplazado. También  tiene el tema de la temperatura. Cuando hay diferencia de temperatura se pierde el quilo.  Exacto, también la temperatura influye pero no tanto como el otro. Un amigo la tiene sí y NeuroLink  que está de moda. Eso es, muy bien. ¿Cuál es tu nombre? ¿Qué te ve ahí? Muy bien, veo que estamos  hablando ahí el mismo yo. Características físicas del oído. Pues percibimos ondas de 20 Hz a 20 kHz.  Mayor sensibilidad a 4 kHz. Hay un umbral de sensación de cosquilidad a partir de 120 dBs,  que sería más o menos 10 logaritmos de la intensidad, parte de la intensidad de referencia.  Y la voz humana va entre los 300 y los 4 kHz. Y obviamente los niños, sobre todo los bebés,  tienen un ancho de bandes, es decir, un aspecto mayor de audición más que nosotros los que  somos mayores. Y a medida que vamos siendo más viejos, como en mi caso que ya tengo una edad,  pues mi frecuencia de auditiva se va reduciendo. Y los que escuchan reggaeton también pueden ir  reduciendo su ancho de banda auditiva. Incluso los que escuchan heavy metal. Pero bueno, eso es un  poco de crítica para que me entiendan cómo van las cosas. Aquí les he puesto un vídeo muy explicativo,  a mí me gusta mucho, desde el minuto 15 al minuto 72, por eso les decía que les voy a poner  siempre varios vídeos, donde se ve claramente todo esto que hemos hablado. Echales un vistazo  si queréis profundizar un poco. En vez de estar viendo vídeos de TikTok y demás, estos vídeos  son muy muy buenos, los que voy poniendo. Ante todo, vamos a llegar a la ley de Webflex.net.  Esta ley me da una respuesta en frecuencia de nuestro oído y nos dice que es logarítmica.  Es decir que cambios lineales en la magnitud del estímulo producen cambios logarítmicos en la  percepción del estímulo. Entonces la ecuación de intensidad es la que tenemos aquí, que la he  descrito antes. Quedarse siempre con las ecuaciones, por favor. Esto ya es lo que veo en acústica,  pues sí, es acústica. Entonces tenemos esto, quedarse siempre con las ecuaciones para el nivel  de intensidad. Después el nivel de intensidad de múltiples fuentes, pues me dice que cada fuente  tiene una intensidad de 60 decibeles, por ejemplo. Si no tenemos en cuenta la separación de las  fuentes, ¿cuál sería el nivel de intensidad combinada? Pues tenemos que la intensidad es igual a la\n",
      "Intervalo 50-60 minutos:  intensidad 1 más la intensidad 2, a dos fuentes que tenemos. Cada una tiene 60 decibeles. Entonces,  ya que las dos fuentes tienen el mismo nivel de 60 decibeles, pues para mí sería 10 logaritmo de  intensidad total, en este caso 2L, es decir, 2 por 60 partido entre la inicial.  Propiedad que estoy iniciando, propiedad de log...  Entonces teniendo en cuenta esto, 10 logaritmos de 2 más 60, el resultado sería 63.  Es el nivel que crece en tres decibeles cuando doblamos la intensidad del sonido.  Habría que investigar de convertir imágenes en sonido. Bueno, en este caso no, pero vamos,  yo lo que quiero es que se den cuenta de eso. Ahora, a partir de lo que estamos viendo,  pregunta para el foro. Voy a crear por cierto las distintas ramas del foro,  si, están 8 rayos españoles, están 8 rayos de FNRI. Ejercicios. Hemos visto aquí con dos  fuentes similares y despreciando la distancia. Ahora les pregunto, si una fuente A tiene un  nivel de intensidad de 10 decibeles y la otra tiene una fuente de intensidad de 20,  ¿qué sabemos sobre las intensidades, en vatios sobre metro cuadrado, de las fuentes A y B?  Es decir, ¿qué pasa con esas fuentes? Y después, ¿qué pasaría si en vez de tener dos fuentes,  tengo tres fuentes de sonido con las siguientes características 50, 60 y 70? ¿Cuál sería el  nivel de intensidad del sonido combinado? Son dos ejercicios que se los planteo,  llámenlo Tarea para la casa, desarrollarlos si podéis y comentármelo en el foro. Y Alitza,  esto no es nuevo para ti, entonces espero tu respuesta, sobre todo la tuya. El resto también,  espero el resto de respuesta. Vamos ahora con la parte de sistema de visión humana,  es lo que nos interesa, que no maten. La visión es el fenómeno resultante de la percepción de  colores en distinta forma de distancia de los objetos en un espacio. En pocas palabras,  es el resultado de la incidencia de la luz como una electromagnética sobre la retina del ojo,  pero de rebote del objeto. Nosotros en un cuarto oscuro no vemos nada, acá subimos los bortes,  y es porque el ojo se va a ir adaptando a la falta de luz y los conocidos bastoncitos y la pupila  y demás va a ir dilatándose para poder capturar mejor la imagen. Por otro lado, el color percibido  va a depender de la frecuencia o longitud de onda de la luz que incide sobre el objeto,  lo que he dicho hace un momento. Pero qué es lo que sucede aquí, y esto es algo muy importante,  por ejemplo, los que tengan gafas, les hago el siguiente experimento, porque para mí es más  sencillo con esto, tengo mis gafas relativamente limpias. Si por algún caso yo le ensucio con el  dedo y demás y me la pongo, solamente me va a molestar al principio. Después el ojo humano,  como es tan inteligente y el cerebro también, pues ese blurry, ese punto de suciedad que me daba  esa distorsión de imagen, va a hacer que mi propio cerebro interpole y pueda crear un filtro  internamente para que la imagen ya me llegue más limpia. Vale, entonces pasa eso.  Para que vean cómo es el ojo humano. A ver, la anatomía del ojo. La luz refleja en los  objetos que se proyectan sobre la materia fotosensible o la retina. Después va a pasar  a través de la lente que es el cristalino y la vamos a ir obteniendo esa imagen. Es como la  detección de los outliers, pero en el contexto más exacto o rensa. Ahí le ha dado. Ahora la zona  visible del cristalino o la pupila va a ir modificando su tamaño para enfocar de lejos o  de cerca. Vuelvo a dar el ejemplo. Los que tienen gafas, si os quitan un poco las gafas,  lo alejan y demás, van a ver cómo está ese enfoque a través de la gafa y a través de sus  pupilas. Y los que no, pues tenéis el móvil, ya sabéis, agarrar la cámara y hacerle el zoom y  van a ver cómo se hace ese enfoque. Después, en la visión humana. Seguimos. La retina no es más que  una membrana interior que tiene el ser humano y en ella se encuentran las células fotosensibles.  Vale, están los conos que son menos numerosos y pocos sensibles a la luz de una. Y también  están los bastones o bastoncitos que precisan menor cantidad de luz que los conos, pero para  su excitación normalmente en visión nocturna. Teniendo este conocimiento, cuando el ser humano  o los ingenieros o los físicos han encontrado de cómo discernir lo que ve el ojo o no, es cuando  se ha comenzado a construir los siguientes sensores para las cámaras. Para las cámaras  electrónicas, no mecánicas, antiguas de carretera. No, para las electrónicas. ¿Por qué? Porque,  repito, toda la parte electrónica trata de imitar el ojo humano. La parte mecánica también lo hizo  en su día, pero la electrónica es mucho más precisa. Otra característica física, pues en  este caso volvemos con la ley de Weber-Feldschild, donde indica que conforme a la intensidad de la  luz, mientras sea mayor, pues es más preciso la variación de las imágenes. Tiene una variación  más similar y va a tener una respuesta logarítmica. En este caso ya sería una respuesta en dos  dimensiones en contraste a la que hemos tenido antes del oído en una dimensión. Ahora,  pregunta en las características físicas. Vamos a ver muy por encima, esto es un concepto  muy por encima, de un filtrado espacial. El filtrado espacial nos va a dar que nuestro muestreo del  sistema de visión tiene que realzar el contraste entre las zonas de diferente intensidad. Decidme  ustedes, ¿cuántos tipos de gris negro hay aquí? A ver, contar. 10, vale, ¿qué nada más? 9,  ¿qué nada más? ¿qué nada menos? Vamos a contar. Aquí el 1, aquí el 2, aquí el 3, 4, 5, 6, 7,  8, 9. El que dijo 10, y ahí se me extraña que tú veas 8. En teoría deberías ver más colores.  ¿Qué es lo que sucede? Aquí podemos ver claramente la variación de colores de forma  un poco más correcta, con mayor detalle, porque no hay una difuminación o un degradado,  del color. Entonces teniendo en cuenta esto, pues si tengo un filtro paso alto, por ejemplo,  los colores A, B, C, D, los filtros paso alto tienen una frecuencia más elevada que los  colores C, D. Entonces si quisiera filtrar los C, D o el A, B, pues es tan sencillo como tener  un filtro o algo que solo se encargue de realzar eso. Entonces con eso tenemos las características  físicas. ¿Por qué? Porque una frecuencia más alta pues me da los colores más a la izquierda y  más bajas los colores más a la derecha. Aquí, decidme, ¿cuántos colores veis? En total,  en esta imagen, ¿cuántos colores veis? Dos. ¿Estamos seguros? Alguien da más. Segurísimo,  cuatro, tres, ¿quién da más? ¿Alguien ve seis? Pues tenemos dos colores. ¿O no? ¿Cuántos  colores tenemos? ¿Tres, dos o cuatro? A ver, el que dice dos. Juan Luis, que te veo ahí muy lanzado.  ¿Por qué dices dos? Suéltalo.  Puedes hablar, ¿eh?  Ok, estaba analizándolo por una sola imagen, pero asumo que son de los cuatro cuadrados,  ahí serían tres colores. ¿Tres colores? ¿Por qué tres? Porque los dos colores en medio están  dentro de los cuadrados, serían muy similares y los otros dos de los internos son diferentes.  Vale, en realidad este color y este color es el mínimo, solo que el contorno en el  cristal pues hace que se vean distintos. Son tres. Muy bien, Juan. Pues básicamente esto de  de tener las características físicas me va a servir para tener el contraste y para poder distinguir  los tipos de colores que voy a tener independientemente del entorno. En este caso dependiente,  dependiendo de lo que le rodea. Por eso es que puede que algunos digan cuatro colores,\n",
      "Intervalo 60-70 minutos:  pero como esta imagen incluso la han debido a ver muchas veces en TikTok, Instagram,  yo que sé, pues claro. Van, perdón, eras, eso. Como lo ven en varias estas redes sociales,  pues te dicen, ¿cuántos colores ves? Que no sé qué, pues, para que vean que el propio entorno  engañan y sirve para que nuestro ojo se caiga en ese juego, ¿vale? Pero vamos, son tres colores.  Seguimos, nos queda media hora y tenemos que ir avanzando. Estamos a la mitad.  Ahora, con todo esto, pues la percepción visual del ojo humano pues va a hacer un  filtrado en frecuencia para seleccionar la parte de la radiación luminosa correspondiente al  aspecto de luz visible que tenemos. ¿No? O sea que nos va llegando. Después vamos a realizar  una transformación logarítmica a la ley de Weber, ¿vale? Para analizar el estímulo percibido.  Después puedo hacer un filtrado espacial para hacer un realce de bordes o fronteras, ¿no? El  cambio brusco de contrastes o de colores, ¿vale? Y después puedo hacer un filtrado temporal para  hacer un muestreo de señal donde se van a reflejar las frecuencias críticas de fusión y la frecuencia  de rendición de movimiento, ¿no? Todo esto sería lo que sería la parte de análisis de percepción  visual, ¿vale? ¿Es el ojo o nuestra mente? Andrés, dímelo tú, ¿es el ojo o nuestra mente?  ¿Es eso el anterior, no? Sí, sí, sí, pero ¿es el ojo o nuestra mente? O sea,  el cerebro o nuestra mente. Según el documento, nuestro cerebro solamente interpete señales,  probablemente sí sea nuestro ojo el que den señales. Sería la mente, sería el cerebro.  Claro, es que ya te he dicho, el ojo viene a ser la cámara que ve interpretadas señales. El  ejemplo de las gafas es porque el ojo ve esa borrosidad que he puesto en la gafa y al principio  voy viendo ese error o esa borrosidad de la imagen. Pero el cerebro ya se encarga, ya lo he comentado,  se encarga de, por así decirlo, filtrarla o de hacer que ya no lo vea. ¿Por qué? Porque va a ir,  como te digo, va haciendo su cálculo de Pris y Corriendo internamente para que ya no lo vea y  todas las imágenes en las cuales aparezca esa borrosidad pues ya desaparezcan para mí y vea la  imagen. Claro. Lo decía porque en el documento había una sección donde decía que las,  por ejemplo, la retícula, la que está ahí, la que se ajusta para hacer enfoque, entonces el  ojo hace ese trabajo. Obviamente me imagino por alguna señal del cerebro que le dice que haga eso.  Claro. ¿Puedes manipular la percepción que tenemos de las cosas de manera biológica  directamente? Podría ser el cual cual cambie. Exacto. Lo que pasa es que ahí entre sistema  de adquisición de imagen, sistema de procesamiento, la cabeza, ya tienes el bucle cerrado,  porque es un sistema de control alazo cerrado, donde interactúan ambos, sabes. Pero si no fuese  cerrado ya tendrías la respuesta. Pero vamos, es así, correcto. Por eso les digo que estas  clases son para interactuar unos con otros, para que podamos sacarle mayor provecho. Bueno,  seguimos. ¿Eramientas que vamos a utilizar en esta materia? Pues Anaconda, los que quieran  programar con Anaconda o con Python a secas. Algunas librerías como Sikkimage, el OpenCV,  sí, y Matlab, que ya les he dicho, donde tenemos desde el Signal Processing, AudioToolbooks,  Wablet, los DCPs, Computer Vision Toolbox, el LIDAR, Mechanical Image, Vision Pro HDL y demás.  Vale, esas serían las herramientas que hemos tenido. ¿Libros? Para mí, la Biblia, para mí,  puedes usar Collab también. Collab es Python. Python, Anaconda, no hay problema con eso. Para  mí el libro de referencia como tal es el Digital Image Processing, de la Universidad de Tennessee,  Dr. Richard Woods y Steven Ellings. Esta es la referencia. Tenemos los libros de OpenCV 4 con  Python 3 y el Image Processing también con Python y también hay con Matlab. Vale, entonces, lo que queráis.  ¿Alguna duda hasta aquí antes de comenzar el siguiente tema?  ¿Los que somos desarrolladores podemos enfocar las actividades usando librerías con OpenCV?  Sí, puedes usarlo. No hay ningún problema. A mí lo que me interesa es tener un resultado final.  Las herramientas que utilices, eso ya va dependiendo de las capacidades que tengas.  ¿Se necesita mucho computador tipo GPU para el curso? No, no necesariamente mucho GPU,  pero sí en algunos ejemplos de proyectos que les voy a ir mencionando a lo largo del curso,  voy a mencionar siempre utilizar a lo mejor más GPU o más CPU, sobre todo por temas de coste  beneficio a la hora de la alimentación. ¿En el examen podemos usar Collab? No, en el examen  no se utiliza ningún lenguaje de programación, ninguna herramienta de desarrollo. Con Collab no  necesitas GPU. Hernán, tengo que decirte que para algunas cosas, por más que tengas Collab,  necesitas GPU en Collab, porque sí, lo necesitas. En el examen no vas a ver computador, es verdad,  en el examen no vamos a usar el ordenador. ¿Pese a aplicar Deep Learning, CCN y Transfer Learning  para un proyecto visual artificial? No, no vamos a llegar a ese grado. Vale, para los dos temas del  día de hoy, ¿hay nivel de fórmulas serias, fórmulas logarítmicas? No, bueno, la que he puesto  hace un momento, de momento es una. Vale, Elizabeth, de momento. Le repito, quedarse con las ecuaciones.  Bueno, seguimos con el siguiente tema porque nos piden todo, que ya son las 5 y 10 y esto termina  en 20 minutos, máximo. ¿Te recomiendo pagar mensualidad de Collab? Son unos 10 dólares  aproximadamente. No sé qué decirte, Gerardo, pero bueno, un portátil con GPU y demás,  a lo mejor te simplificas las cosas. No lo sé. El examen final es práctico en el sentido que lo  mejor tienes más que desarrollar alguna ecuación. Bueno, colocar los valores a una ecuación y por  lo menos desarrollar esa ecuación, reducir algunos, yo que sé, reducir la fórmula un poquito y  dejármelo en modo indicado. Si puedes calcular el valor final, por ejemplo, de la entropía,  pues perfecto. Pero si me lo dejas de tal forma para que yo sepa que lo has desarrollado de arriba  a abajo, pues a mí me vale. ¿Vale, Carolina? Así que tranquilo. Bueno, siguiente el tema.  Los elementos de un sistema de percepción, ¿vale? O de imagen. Vamos a ver la presentación de los  objetivos y los elementos esenciales, ¿no? Como si la captura de información o el procedimiento  de información y toma decisiones. Entonces, nuestro objetivo es aprender los elementos  esenciales para un sistema de percepción computacional o de análisis de imágenes o  computer vision como quieran. Entonces, tengo que ver qué se hace y cómo lo hace. Entonces,  ¿cuáles son los elementos esenciales o cuáles son los elementos que necesito para percibir cosas  del exterior? ¿Qué necesito, por ejemplo? Si ustedes están ahora mismo viéndome, pues están  escuchando, están oyendo. ¿Vale? Lo que sería el sentido del oído. Están viendo. Algunos estarán  tomando apuntes, estarán pensando, estarán anotando o memorizando. Yo qué sé, si estuviésemos en una  cata de vinos o de quesos o de comida, pues vamos a estar probando, vamos a estar tocando. Entonces,  tenemos una serie de elementos esenciales que necesitamos para percibirlos. De todos estos,  podemos agruparlos en tres. Uno sería oír, ver, probar, medir, que serían netamente  cosas mecánicas, si lo quieren ver así. Memorizar, razonar, pensar sería un poco más lógico.  Y cambiar a ser andar, oír y decidir, que sería físico. Entonces, con todo esto,  las primeras me sirven para capturar la información. ¿Vale? ¿Por qué capturo la información? Porque es  lo que estoy analizando en un raster. La siguiente, procesar. Estoy procesando, estoy memorizando,  estoy razonando. Y finalmente, la parte física de decidir, cambiar, hacer oír y andar, es mi toma de  decisiones. Con todo esto, captura de información, el procesamiento y la toma de decisiones y seguimos  en el bucle. ¿Vale? Estas son las tres cosas. Exacto, es como un entrenamiento de red neuronal.  Lo que van a ver en esta materia es lo que, por así decirlo, serían las tipas o la arquitectura de  una red neuronal, pero sin llevar al ámbito de la inteligencia artificial. Ustedes van a saber cómo es  que la imagen tiene que ser preprocesada, cómo tiene que ser capturada y qué decisión debe tomar.  Porque por tu vida, abstraer algunas características y dependiendo de eso, tomaré alguna decisión.\n",
      "Intervalo 70-80 minutos:  Puedo tener un clasificador, por ejemplo. No necesariamente es una red neuronal, es un sistema  abadeciano, además, la parte de la inteligencia artificial. Pero es algo físico que está en  nuestra mente y que puede hacer un ordenador antes de ser una red neuronal o tener una influencia.  En el día a día estamos haciendo un fine tuning, pues sí, en el día a día hacemos un fine tuning,  seguro. Tomamos las características para tomar decisiones. Exacto, muy bien Juan Luis. Entonces,  esta imagen la voy a ir pasando casi en todas las clases. Captura de información, preprocesamiento,  segmentación y filtrado, extracción de características y toma de decisiones. Todo esto me engloba  captura de información, procesamiento y toma de decisiones. Pero estos serían los pasos que vamos  a seguir a lo largo de toda la materia. Para la captura de información hay que tener en  cuenta que es el primer paso de todo sistema de inteligencia artificial. Porque necesito algo en  el cual obtener las cosas. Es el dispositivo clave, es el sensor. Dentro de la toma de decisiones  estaría el enfoque. Sí, estaría el enfoque. Volvemos al tema. Entonces, la captura de  información es los sensores. Toda la sensorica es eso. Un sensor es capaz de convertir el exterior  en información procesable y manejable. Después, todo sensor posee las siguientes características.  Mostro algunas, porque el vídeo de sensor es que tiene muchísimas más. Especificidad, precisión,  sensibilidad, consumo. Muy importante consumo para los que quieren hacer cosas de drones. Es decir,  en sistemas que no están alimentados constantemente. Tamaño muy importante también para gente que  hace drones y aviónica. ¿Por qué? Porque el tamaño sí importa. Es algo que algunos dicen que no,  pero también sí importa. Y por otro lado, que también influye ya en la evolución o en el hecho  de crear tu proyecto, es el precio. Porque no es lo mismo un sensor de 100 dólares, es un sensor de  5 dólares. Los antidrones también. Para los que utilizan en otros, es importante. La usabilidad.  ¿Por qué usabilidad? Cuando digo usabilidad, me voy a meter en temas de RAM, es decir,  de la vida útil. Si mi sensor me garantiza 100 horas de vuelo o 100 imágenes por segundo o 100  imágenes que va a capturar de forma idónea, no es lo mismo que uno que me cueste un euro,  que solo me garantice 10 y la vida útil es variedad. ¿Vale? Producto chino, dámelo así.  Entonces, todo eso influye en un sensor. Dependiendo del proyecto, por ejemplo,  si fuese proyectos militares, proyectos de aviónica, les digo eso porque es en lo que  estoy más centrado en el mismo, lo que más importa también es la vida útil. ¿Por qué?  Porque dependiendo de la vida útil me va a servir a que mi avión, mi dron, vaya más antes  a reparación o a mantenimiento que me siga generando dinero. Por ejemplo, si me meto en la  parte de precisión y sensibilidad y me voy al tema de médico, pues mi sensor de S.G. para  lectura del cerebro, mientras más preciso y más sensible tenga, me va a poder detectar mejor las  anomalías de un posible cáncer cerebral, que es de que la persona solamente se le venía la mente  de la imagen de yo que sé, una cara bonita y demás, y ha pegado un suspiro y no es más que  solamente eso. Entonces, todo eso hay que tenerlo muy presente. Hay vidas en juego, no se puede  comprar en el Aliexpress. Exacto, ese es el tema. Y la especificidad que les he dicho, pues cada sensor  se especializa en un determinado aspecto a medir. Por ejemplo, ¿usarían un termómetro para medir  presión? ¿Sí o no?  No, pero aunque no lo crean, puedo medir con un termómetro de presión por la diferencia de  temperatura. Se puede, se puede. Analizando el medio y demás se puede. Pero ¿qué es lo que pasa?  Entre que capturo la información del termómetro y haga todo mi cálculo matemático y demás,  pues un sensor barométrico, pero me lo hace mucho más rápido. Pierto tiempo,  pierdo dinero, por lo tanto, coste, es decir, dinerito. Y no es preciso. Exacto, se puede,  pero no es preciso. Muy bien, Juan Marcos, muy bien, muy bien. A eso me voy. Tenemos que ser  capaces de detectar qué sensor nos va a servir. Y para eso incluso, más adelante, van a saber qué  filtros va a ser más necesario para utilizar en qué tipo de imágenes y qué otros. De eso va la  materia, de poder tener todo a este. Seguimos, que nos queda poco tiempo. A ver cómo avanzamos.  Después, la especificidad. Cada sensor se especializa en un determinado aspecto para medir.  Lo que hemos dicho del termómetro, la precisión vendría a ser la capacidad de reducir el error  de una media. Por ejemplo, los sensores de lluvia me indican si llueve o no llueve,  pero es poco preciso. No me dice qué cantidad de lluvia. ¿Qué tipo de filtros van a usar?  ¿Qué se van a usar? Filtros de todo tipo. Paso banda, paso alto, canusiano, de media, de varianza,  filtros en frecuencia, temporales. Vamos a ir revisando todos. Los más importantes,  cuando digo todos, los más importantes. Después, la sensibilidad. Es la capacidad de adaptarse a  los cambios externos. Un semáforo de LED, por ejemplo, podrá ser detectado por los  parpadeos de los LEDs o no. Ahora, por ejemplo, un sensor que es poco sensible a variaciones de  temperatura, por ejemplo, no me va a servir para poder detectar o para poderlo implementar  en un encumador de horario un bebé. Entonces, ahí va la sensibilidad. O incluso esos sensores  que tenían en la época de la pandemia, que los colocaban cerca del cerebro, a la cabeza,  presionaban y veían si tenían o no tenían fiebre, eran poco sensibles. Eso solo se utilizaba  pues para tener un sondeo rápido. Pero después, el sondeo se iba en las UCIs, es decir, en las  UCIs, donde se medía a milímetro, por así decirlo, la temperatura de las personas que tenían la COVID.  Exacto, tenía poca resolución. A eso me voy con la sensibilidad.  Entonces, estos son los diferentes sensores que he podido capturar. Bueno, está creo que en sus  bolsos, en sus tretos y demás. Tenemos sensores de detección del corazón, de hierre para tocar, humedad,  todos. Y si me hubieran pillado sensores de arduino, si me hubieran pillado un mes atrás,  aquí les traía incluso un sensor de acelerómetro de inercia de movimiento de un avión. Es una cosa  pequeña, bueno, lo tengo que dejar en el laboratorio, pero me mide incluso las pequeñas  variaciones que puede tener un avión. Eso se utiliza no solo en la aviónica, también en los  satélites y los cohetes. Es más, el SpaceX a la hora de realizar sus sensores militares son más  eficientes así, más que todo por el grango de temperatura. SpaceX, cuando ha manejado el último,  se me da el nombre, el último cohete, este tocho que ha lanzado para que se hagan una idea. Tiene  un conjunto o un array de sensores inerciales de 200 sensores inerciales dentro, por eso es  tan preciso. Han conseguido la presión para aterrizar y demás. Entonces, es super heavy,  eso es. Muchas gracias, José. Entonces, para que vean que esos sensores de movimiento o de posición  de los inerciales sirven para todo. ¿Por qué digo un array? Porque un sensor me da la posición  absoluta, pero si tengo un juego ya puedo implementar algoritmos matemáticos y demás para detectar  a milímetro todo eso. ¿Por qué? Porque cada sensor también tiene, no es tan sensible, puede tener  error y demás. Y con un par de ellos ya reducirán ese error. Tiene que ser super preciso porque  aterrizan los cohetes tradicionales nada más caen. Pues eso, ahí las daban. Seguimos. Aquí,  por ejemplo, tenemos unos sensores. Por ejemplo, este es un sensor de humedad y este ahora mismo  no recuerdo bien, creo que también es de posición de los GPS. Una estación meteorológica. Entonces,  tenemos varios tipos de sensores que nos sirven para todo. Ahora, tras el uso del sensor somos  capaces de capturar esta información, ¿vale? Pero hay que tener presente que estamos capturando toda  la información necesaria. Es decir, nos hace falta todo lo que nos proporciona. ¿Sí o no?  Alguien me puede decir no. ¿Por qué no? A ver, Hernán, ya, te he pillado a ti. ¿Por qué no? ¿Quieres  escribirlo en el chat o habilitas el micrófono? ¿Por qué no? Porque necesitamos solo una parte de los  datos, por ejemplo, en sonidos. Necesitamos la frecuencia que escuchamos. El resto de las  frecuencias no nos van a servir para este reconocimiento. En el caso, por ejemplo, de\n",
      "Intervalo 80-90 minutos:  las imágenes. Necesitamos la longitud visible. El resto no. A menos que estemos trabajando con  con un color violeta o con un ferrojo. Exacto. Tenemos que escoger en el fondo cuál es la  información que vamos a utilizar de todas las disponibles. Eso es. Entonces, algo que no lo he  mencionado y a mí me gusta siempre sacar esa parte aquí es lo siguiente. Dependiendo del tipo de  proyecto que vayamos a encarar, ¿vale? O el tipo de problema que vayamos a resolver, vamos a tener  que usar un sensor u otro. Y aparte, toda la información que procesamos, vamos a tener que  transformarla. Vamos a tener que convertir esa parte analógica en discreta. Aquí vamos a utilizar  el conocido conversor analógico digital. Y de esa manera, también si nuestro sensor es muy amplio,  pues lo que tú decías, mejor con todo lo que he capturado, después de pasarlo a digital,  pues me quedo con un largo más pequeño. Aunque mi sensor me devuelva un tocho más grande,  pero ya me sirve para diseñarlo. A ver, se puede hacer un post-processado, un segundo micrófono,  como los teléfonos. Claro, el segundo micrófono es para quitarte el lujo. El sensor va a depender  de lo que se necesita estudiar. Exacto. A lo mejor, justamente me interesa lo que no escuchamos los  humanos. El ultrasonido, por ejemplo, que está por detrás. También dependiendo del caso del uso,  puede ser más importante la toma de camas. Sí, se puede plantear desde el principio la  incertidumbre y la misma adicción. Sí, todo correcto lo que está colocado. Para capturar  información, es comandable la redundancia también, pero no necesariamente la redundancia. Bueno,  entonces hemos dicho que habría que transformar mi señal analógica en digital. ¿Para esto qué  significa analógico? Pues que mi señal tiene infinitos valores. Es un rango infinito. Si  tratamos de encontrar los valores que tiene la voz humana, pues tiene infinitos. No tiene escalones  ¿Qué significa discreto? Cuando lo pasamos a la parte digital, pues que puede tener valores  finitos. Por ejemplo, una imagen tomada por una fotografía digital, pues tengo los píxeles.  Va a depender ya de mi conversor analógico digital que es cuantizable. Exacto. Muy bien,  Edwin. Vale, que no tiene escalones. ¿En qué consiste mi conversor analógico digital? Pues  consiste en asociar un valor determinado de un conjunto finito a una de finitos valores que  ha tenido. Entonces voy a cuantificar y voy a codificar. Aquí codificaría valores binarios,  pero por ejemplo, esta señal que es analógica, la estoy cuantificando. Eso es el analógico,  es continuo. Y aquí lo estoy volviendo digital. Estoy haciendo un muestreo. Muy bien, Juan Marcos.  Estoy haciendo esto en la captura de información. La imagen, pues miren, aquí tengo la monalisa.  Si yo voy a ver la monalisa, pues la veo perfecta. Pero antiguamente, no sé la edad que tenéis vosotros,  pero cuando sacaba las imágenes digitales antiguas que tenía a lo mejor un megapíxel o dos  megapíxeles de resolución, podía ver desde esto a cosas muy lujas. Un mejor ejemplo también,  aparte de la resolución hasta la cuantificación, pues los colores en los cuales voy a cuantificar.  Algo que sea full color, uno que tenga 256 colores y otro de 8.  Esta transformación tiene pérdida de información. Dependiendo del caso,  es problemático o no. Eso es. Va a depender un poco del caso, me va a servir o no. Por ejemplo,  si yo les hubiera mostrado solamente esta imagen, si hubieran dado cuenta que es la monalisa o no,  sean sinceros. La reconocemos igual, claro. Pero ¿por qué la reconocemos?  Por patrones. Exacto. Como ya conocemos más o menos la forma o el patrón de la monalisa,  por patrones reconocemos que es la monalisa. ¿Vale? Porque es conocimiento, pero también  conocimiento. Eso es. Muy bien, Luis. Bien. Un modelo de visión artificial necesita  la misma resolución humano para entender la imagen. No necesariamente. Aquí te puedo decir  que hay dos vertientes. Unos que dicen que sí, porque mientras más dato, mejor. Y yo te puedo  decir que no. Personalmente, en la práctica, cuando utilizas ilusión de sensores, llámese.  Utilizar una imagen RGB o imagen de una foto normal. Cuando fusionas estos dos tipos de sensores,  obtienes una imagen que para el ojo humano es como si estuvieras viendo nieve en la tele,  pero para una reneuronal entre nada es como si estuvieras viendo una imagen a través de  una captura de imágenes entre un banco de niebla. Es posible no tener necesariamente eso.  El tema es que la resolución es similar a lo que venden las cámaras de los móviles.  Los mayores, los mencapíces, los son mejores. Muy bien, Alexander. Muy bien. Necesitas una  resolución que se adecuada a tu programa. Muy bien, Hernán. También haces efectivo eso.  Y Andrés, los humanos tenemos un límite de percepción de resolución. Por ejemplo,  ¿vale la pena una pantalla de teléfono 8K? No vale la pena. Eso ya lo sabemos, Andrés. Bueno,  seguimos que se nos va el tiempo y tú, tenemos tres minutos. A ver cómo digamos.  En la captura de información, hemos dicho que se va a hacer un pre-processamiento adicional.  Se hace un pre-processamiento adicional. En toda la captura de datos que existen va a haber  errores en la medida, va a haber ruido y todas estas anomalías o todos estos errores, pues,  se ocupa de ruido, anomalías y errores en la captura. O el sensor es poco sensible o ha habido  destello o lo que sea. ¿Vale? La retina de estudios, como mejor podemos tener... Walter,  ¿es mejor perder información, captura? ¿Cómo? No es mejor perder información de una mejor  captura que nunca tenerla por un sensor de baja calidad. Bueno, no sé, esa pregunta ponla en el  foro, pero me parece interesante, Walter. No podemos debatir. Por ejemplo, en la captura de  información, aquí vemos la imagen de un... de un... de un tac, ¿vale? Y aquí vemos la imagen que  está borrosa, ¿vale? Para una persona entrenada, para un médico, esta imagen y esta imagen le da  igual. Para una red neuronal, mientras tenga el feedback, es decir, la retroalimentación del  experto, a esta imagen también le va a resultar igual. Pero si no tuviese esa retroalimentación,  esto estaría perdiendo resolución y estaría perdiendo. Para esta área, el uso del auto  encoder para reducir ruido es útil. Se va a emplear auto encoder. No, no vamos a emplear eso.  Por ejemplo, ¿alguien me puede decir qué ruido es el que se presenta en esta imagen?  Para alguno que ha visto ya este tipo de cosas.  ¿Ruido blanco? Salpimienta. Muy bien. Muy bien. Veo que están avanzando.  ¿Ruido blanco? Más que ruido blanco, salpimienta. ¿Por qué? Porque el punto blanco es puntón.  Entonces, la mayoría de los sensores, sobre todo los militares, produce este efecto de  salpimienta. ¿Por qué? Porque al ser tan sensibles y demás, son más propensos también a la captura  de estos destellos producidos por ráfagas de luz en choque con algunos objetos cercanos.  Eso por la experiencia se los cuento. Ahora, por ejemplo, de todas estas señales que pueden ver,  el más común es el concepto del ruido que está presente. Tenemos ruido que puede ser reflejos,  los brillos no deseados, salpimienta, por ejemplo, motas en la captura y sensores sucios. El sensor  sucio es lo que más produce ruido y es lo que menos uno se da cuenta. ¿Por qué? Porque al final  piensa que la imagen es así o está implícito en el lente, pero en realidad es el sensor, la parte  más baja. En imágenes satélites de radar también ocurre ese efecto. El de salpimienta sí, también  ocurre. Precisamente ocurre eso, sobre todo cuando hay rubia muy pequeña, muy finita.  Si nos vamos a la parte de análisis de ondas, la parte microondas se produce por eso. Uno de los  que deja esto ya después no siempre se les hace mantenimiento. Bueno, sí, también es verdad lo que  dice. Para eso se le mantiene bienito, cambia la cámara, es barato. Seguimos que ya no nos queda  nada. Por otro lado, el ruido no puede eliminarse completamente, siempre va a estar presente. Solo  podemos atenuarse su efecto. Allí es que donde hay ruido la información se ha perdido. En cualquier  caso se puede inferir qué podría hacerse para que el ruido no esté, o sea, hacer alguna estimación  y también una de las métricas más importantes, si esto es para Yai-Chao, como no me acuerdo cómo te llamas,  está el SNR, el Signal to Noise Ratio. Eso lo vas a ver mucho en tu carrera que estás haciendo ahora.  Finalmente, pues existe infinidad de mecanismos muy depurados capaces de  disminuir este efecto de ruido. Dentro del procesamiento de información, el objetivo de  este procesamiento es extraer información adecuada para las futuras tomas de decisiones. Porque les he\n",
      "Intervalo 90-100 minutos:  dicho, la imagen mientras más procesada esté para la entrada de computer vision es lo mejor.  Después, en un sistema biométrico, esto es para los que manejan el healthcare, el reconocimiento de  cara, por ejemplo, el procesamiento de información necesitaría detectar rasgos faciales, es decir,  distancia entre ojos, por ejemplo, aislar la cara del fondo, es decir, lo que tenemos ahora mismo,  que tengo el blue detrás, para poder detectar y centrarnos en los efectos, en los defectos que  tenga el rostro. Y tener en cuenta también los efectos que puedan ser causados por la iluminación  o por el blurry que se van a crear a través de, en este caso, lentes o algún ente cercano a la cara.  Entonces, los pasos más básicos que nos vamos a centrar para el procesamiento de información va  a ser uno, filtrar, segmentar y extraer características. Quedarse siempre con estas tres cosas.  Ahora, para el filtrado y suavizado la información, pues hay que tener en cuenta que principalmente  se debe a que existe mucha información que puede ser necesario o no. Sobre todo,  la textura y los bordes es lo que más información me va a dar. Por ejemplo,  aquí vemos, esta es parte del globo ocular. Si yo les digo, muéstrame, en este caso sería  una de las arterias principales del ojo, pues como sabemos un poco el reconocimiento de retinas,  por ejemplo, me sirve para extraer estos pequeños vasos que existen, vasos capilares y demás,  y una de las arterias principales. Pero si aquí yo utilizo un procesamiento, un filtrado,  un suavizado, pues ya lo tengo más claro. Ya me he quitado las pequeñas venitas o pequeños  vasos capilares que no me aportan nada y me quedo con lo que le puede suceder a esta arteria.  Un thresholding, pues por ejemplo, muy bien, veo que va a ir pisando fuerte.  Se resaltan los datos, analizan, exacto, se visualizan mejor. Por ejemplo, perdón por la hora,  voy a acelerar un poco, pero ya no se me ha pasado. En este caso, para documentar y detectar  regiones, nuestro primer paso va a ser detectar los datos en una imagen o patrones en un sonido.  Aquí, por ejemplo, yo directamente estoy no analizando ni los coches ni nada y solo me centro,  por ejemplo, en la carretera para una conducción autónoma. Exacto, solo me centro en la carretera.  Entonces ya sé dónde va la carretera, ya sé qué es lo que tiene y lo que no lo que esté presente  dentro de la carretera es un objeto. En este caso, aquí hay árboles que están metidos en  promedios y aquí hay un coche y aquí esto es una moto. Esto es el pixel with segmentation más o menos.  Bueno, el ejemplo ha sido sacado ahí, pero para que vean qué es lo que se quiere. En este caso,  ya para clasificar un poco y demás, pues ya sé qué color es la carretera, lo que no es carretera  también lo ha segmentado y los coches que van dentro de la carretera. Entonces ya tengo una  forma de procesar la imagen. Ahora, la extracción de características. Las características son el  concepto del patrón per se en realidad. Es la base de un a posterior de un algoritmo de aprendizaje.  Entonces me va a consistir en introducir los vectores o matrices de las componentes obtenidas  en una segmentación. Por ejemplo, una cantidad de objetos que tengo pues me va a servir para  obtener o extraer una característica de que estos son dados y a partir de aquí hacer un análisis y  sacar sus fiatures. Bueno, para poder verlo exacto. Muy bien, me sirve para eso. Detecto mis dados,  extraído las características y a partir de eso, pues hago las fiatures que tiene. Analizo las  fiatures. Lo mismo sería para el análisis de las matrículas de coche, patentes de coche, placas  de coche, como quieran llamarlo, ¿vale? Para analizar y poder detectar si hay números o hay letras.  ¿Cuántas contors son antes? ¿Son? ¿Es alguna zona azul con la L en este caso para marcarme  serse europea o desamericana? Por ejemplo. Por eso las placas van normalizando. Exacto. Ahora, si esto lo hago  a toda pastilla, es decir, a gran velocidad, me va a ayudar más la normalización que tiene, ¿sabes?  Y incluso me va a ayudar para poder detectar precisamente todo esto, ¿vale? Entonces tenemos que apoyarnos  en esta materia. Nos sirve para poder analizar todos estos problemas desde un enfoque un poco más  logístico y para implementarlo a la hora de meterlo antes a la reno.  Con todo esto pues ya hemos visto que vamos a ver en esta materia pues captura de información,  procesamiento, segmentación y filtrado, extracción de características y la toma de decisiones.  Nuestras referencias. Digital image processing, que ya se había dicho antes, y el hands-on si quieren  de Python. También está el de Matlab, está el de C, Octave, lo que queráis. Y básicamente eso.  No sé si tenéis alguna pregunta, alguna duda, algo que lo podamos resolver en un par de minutos.  Estas son las dos primeras materias, o sea que es más, ¿cómo les digo? Yo voy a tratar de enfocar  la materia lo más experimental posible para ustedes. Voy a profundizar lo justo por tiempo  y aparte les pondré los vídeos por si queréis profundizar. Pero lo que me interesa es que sepan  el concepto de lo que vamos a aprender para que puedan utilizarlo como herramienta para llevarlo  a una reno. Tengan en cuenta eso. Es recomendable analizar las fórmulas. Yo les diría quedarse con  las fórmulas, no memorizarlas. ¿Por qué? Porque el examen van a tener la documentación abierta.  Pero no me sirve una fórmula si no sé dónde la tengo que aplicar. Fórmula para esto, pues me sirve  para aplicar, para esto, esto, esto, mis tres cosas. Fórmula de la entropía, pues si quiero ver la  variación, el ruido presente, no sé qué, no sé cuánto. Tener en cuenta eso. Tratar de hacerse  siempre ese índice que yo siempre digo que para el examen, y créanme, les va a servir bastante,  donde tienen las cosas de cada tema y las fórmulas de cada tema.  Pues creo que sí. Creo que se encuentra en la parte de la biblioteca digital de la universidad.  A alguno de ellos creo que incluso lo puedes encontrar de formato libre, de forma legal,  dentro de internet también. Y hay varios artículos también donde pueden apoyarse.  Vale, que yo les voy a estar dando también esa información.  Y está. He leído varias veces el tema 1 en la selección de respuestas logarítmicas.  También he buscado informa, pero no queda claro el concepto. Respuesta logarítmica,  pues es respuesta en logaritmo. Y si no somos latinos, tenemos nuestras formas.  Ya, no sé. Que no me llamen doctor, que yo soy Javier. Esteban, no entiendo.  He buscado información, pero no me queda claro. Respuesta logarítmica. Es que es logaritmo.  Creo que te estoy entendiendo. Cuando digo respuesta logarítmica es porque tu resultado  va a ir en función a un análisis logaritmo. 10 logaritmos en base 2, base 10 de la proporción,  en este caso, de la intensidad de los altavoces. ¿Por qué te digo esto? Porque a veces el análisis  logarítmico me da un resultado más completo que uno normal. Eso lo vamos a ver en un tipo de  filtrado. En un tipo de análisis de imágenes. Y ahí te vas a dar cuenta porque va a resaltar  algunos valores que un análisis normal no te los muestra. Te va a resaltar alguna imagen de fondo  que no lo puedes ver. En un laboratorio lo vamos a ver. Lo vamos a analizar a detalle,  pero no te preocupes. Lo vamos a ir viendo. Estamos linearizando con logaritmos. El concepto  use in a different terms vale sí. Y el que explica esto de acústica. Muy bien, y alitza.  Los nombres son un poco complejos para mí, pero bueno, vamos que hay muchos nombres.  Describir comportamiento de la espalda humana, representar su novarismo, las formas.  ¿Este estudio de la materia está en base en entrenar una red neuronal para llegar a un Dali?  No necesariamente, no. Bueno, pues eso es todo. Si no tenéis ninguna duda más, voy parando esto.  Vamos a dejar de compartir. Y vamos a pararnos.\n"
     ]
    }
   ],
   "source": [
    "current_interval = 0\n",
    "accumulated_text = \"\"\n",
    "for segment in result[\"segments\"]:\n",
    "    start = convert_to_min_sec(int(segment[\"start\"]))\n",
    "    end = convert_to_min_sec(int(segment[\"end\"]))\n",
    "    text = segment[\"text\"]\n",
    "\n",
    "    # Verificar si el segmento sigue dentro del intervalo actual de 15 minutos.\n",
    "    if start // n == current_interval:\n",
    "        accumulated_text += \" \" + text\n",
    "    else:\n",
    "        # Imprimir el texto acumulado para el intervalo actual.\n",
    "        print(f\"Intervalo {current_interval*n}-{(current_interval+1)*n} minutos: {accumulated_text}\")\n",
    "\n",
    "        # Actualizar el intervalo y reiniciar el texto acumulado.\n",
    "        current_interval = start // n\n",
    "        accumulated_text = text\n",
    "\n",
    "# No olvidar imprimir el último intervalo acumulado fuera del ciclo.\n",
    "print(f\"Intervalo {current_interval*n}-{(current_interval+1)*n} minutos: {accumulated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2 = model.transcribe(\"/home/contrerasnetk/Documents/Classes/VisionArtificial/2.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intervalo 0-10 minutos:   grabar y otra vez compartir. Es que solo me sale eso. Ya inició la grabación. Ya inició.  Sí, me falta ahora compartirlas. Compartimos la pantalla. Podéis ver ahora, ¿no? Yo creo  que sí. Sí, ¿podéis ver la pantalla? Perfectamente, perfectamente. Sí, se ve así. Vale, comenzamos  la clase. Tema 3, ¿vale? De la materia visión artificial o percepción computacional. Estamos  dentro del bloque 1, que es percepción visual y auditiva, la parte de digitalización y cancelación  de anomalías. Vamos a ver hoy lo que sería la captura y digitalización de señales, ¿vale?  Antes que nada, hay que tener presente que lo que hemos hablado la clase anterior de los temas 1 y  2 era de utilizar los distintos tipos de sensores, cámaras, micrófono, sensores de proximidad,  ultrasonidos y demás, para pasar del mundo real al mundo digital. Es decir, que no del mundo  analógico, si lo quiero ver así, al mundo digital. Y con eso, con esa información, pues, poder  ya conseguir señales en una dimensión, en dos dimensiones que me van a servir para poder alimentar  mi sistema de percepción o de inteligencia artificial o de análisis o de filtros o lo que sea,  ¿vale? A modo de recordar eso de los temas anteriores. Con todo esto, ¿qué es lo que vamos a hacer hoy?  Vamos a plantearnos el siguiente problema o la siguiente pregunta, y es ¿cuántas muestras y  niveles de cuantificación o valores son necesarios para representar digitalmente una señal? Ya sea  una señal de voz, una señal de imagen electróptica o RGB que la conocéis, las cámaras de los móviles,  por ejemplo, de toda la vida, una señal de una imagen médica o una señal de imagen multiespectral  de los satélites militares o de los satélites climatológicos o de un radiotelescopio, etcétera.  Entonces vamos a ver cómo vamos a poder resolver esto o cómo vamos a contestar este problema.  Para esto vamos a seguir el siguiente índice, ¿vale? Voy a dar una breve introducción, muy breve. Vamos a ver  los objetivos del tema, el concepto de muestreo, vamos a hablar de la cuantificación y las aplicaciones, ¿vale?  Para la introducción, pues el imperio romano nos va a ayudar a entender un poco lo que es el muestreo,  por ejemplo, lo que ustedes veis en la parte izquierda no es más que una cerámica, el imperio romano  y así a cerámicas y si le dan cuenta, esta cerámica está roceada, tiene trozos muy pequeños, es verdad,  no son regulares, son irregulares y con estos pequeños trozos se ha podido generar la imagen  de un guerrero en este caso con su gorro o casco o su lanza, su vestimenta y demás y han podido sacar  la foto por así decirlo de una persona o representar a una persona, ¿vale? En este caso,  pues tenemos un número limitado de colones, si lo queremos ver en escala tradicional de un RGB,  pues podemos discernir aquí más o menos cuántos colores se ha utilizado, el fondo es uno,  el casco puede discernir que hay otros dos o tres más o hasta cuatro o cinco, algunos se repiten en  el cuerpo pero todos no, aquí tenemos seis, el borde siete o más o menos unos ocho colores, ¿vale?  Ahora cada tecela que son los trocitos pequeños del mosaico puede ser un píxel, si lo vemos de esta  forma es un píxel, entonces si el mayor de tecelas, perdón, si el número de tecelas es mayor,  ¿vale? Mi imagen va a tener mejor resolución y ¿qué pasa si tengo mejor resolución? Pues va a pesar  más, ¿no? O sea, imagínense, esto de formato digital o haciendo una extrapolación a un  formato digital sería las antiguas cámaras estas digitales de un megapíxel o medio megapíxel que  utilizaba, que se utilizaba en antaño, a lo mejor ustedes que son más jóvenes no han tenido el  gusto de conocerlo, pero antes las primeras cámaras digitales utilizaban un disquete de tres un medio  para sacar dos o tres fotos, pero la resolución era muy pobre, si acaso llegaba a yo que sé 800 por 600  o incluso menos, entonces esa resolución pues se traduce en los megas, ahora si me voy al ámbito de  telecomunicaciones transmitir esa información en 200k de imágenes, pues lo transmito yo que sé en  una conexión de radio punto a punto, pues a lo mejor lo sacó en dos segundos o en un segundo,  pero si incremento el número de tecelas, es decir, incremento el número de píxeles, incrementa el  número de megas, pues transmitir esa información ya va a tardar más y todo lo que transcurre en el  tiempo que necesita más o menos tiempo, pues eso es dinero, eso es coste, vale, entonces,  ¿para qué nos sirve esto? para identificar que cuando pase del mundo analógico al mundo digital  voy a tener limitaciones que se van a ver afectados en toda la cadena de traspaso de  información que voy a tener, incluso en la red neuronal, si la imagen es más grande, imagínense,  dos gigas por dos gigas, que más o menos es el tamaño de las imágenes satélite, imagínense  una red neuronal que se entrene con ese tamaño de imágenes, tener un tiempo extremadamente grande  para entrenar trozos por trozos de esa imagen, tenerlo en cuenta eso, luego vamos a ir deslizando  un poco mejor. Con todo esto, el objetivo de este tema, por un lado, es entender los conceptos de  muestreo y digitalización, es decir, si mi muestreo es mayor, pues voy a tener una digitalización más  pobre y si el muestreo es mayor, pues una digitalización mejor, que se parezca lo más ideal al mundo  analógico. El otro objetivo, pues, será reconocer la importancia de estos procesos en el procesamiento  de señal, valga la redundancia. Por ejemplo, lo que les decía las imágenes con menos megapíxeles,  una cámara esta de 5 megapíxeles, que creo que es del año 2004 o 2005, no estoy seguro, pues  su reducción es menor a tener una reflex al día de hoy que ya está por los 50 megapíxeles, que te  puede sacar a lujo de detalle, pues, una pintura, una fotografía, de lo que sea, de un paisaje. Y lo  mismo pasar con la parte de la música, ¿no? El audio normal de toda la vida, llámese vinilo, por  así decirlo, vale, pues tiene unas características muy idóneas y a medida que lo voy digitalizando,  pues, si bien se puede parecer lo más parecido posible a la parte real o analógica, pues en  algunos casos, pues, tiene algunos saltos que me pueden producir ruido o incluso que el oído  humano los pueda detectar. Aquí seguramente varios de ustedes me pueden decir el MNP3 a una  codificación de 96 Kbps, pues es tan pobre que se suena, que se escucha un sonido medio robotizado  versus lo que me puede sacar el vinilo o versus una codificación a 2900 Kbps, por ejemplo. Entonces,  pasa todo esto, ¿no? Para eso hay que tener en cuenta la importancia que tiene el procesamiento.  Y finalmente, pues, también tener claro que hay que identificar las aplicaciones prácticas del  muestro y la cuantificación en diferentes campos. Por ejemplo, lo que les decía de las imágenes  satelitales, una imagen satelital de un tamaño muy grande a la hora de procesarlo en una red  neuronal, pues me puede consumir varios minutos o varias horas en hacer crops e interpretarla,  versus si le voy bajando la calidad. Obviamente, al bajar la calidad, pues, voy a bajar un poco  también el hecho de poder tener una mejor resolución a la hora de identificar, por ejemplo,  objetos civiles o militares o identificar algunos parámetros que van a ser importantes para mi  trabajo, para mi proyecto. Sin embargo, hay que tener presente que a veces un sobremuestre,  es decir, una sobrecarga de datos en la etapa del procesamiento, pues me puede causar también  errores. Con todo esto, pues, se presenta, por ejemplo, un caso práctico. Vamos a presentar  acá un caso práctico que se va a ir hilando incluso en la parte de transmisión. El escenario,  una fábrica de sensores para drones o aviones no tripulados o los UAVs. ¿Qué vamos a hacer con esto?  Pues el volumen significativo de datos generados de estos sensores, por ejemplo, que se van a  generar entre cámaras, electro ópticas o infrarrojas o multiespectrales que se va a tener, pues, puede ser  grande. El LiDAR, que es un radar o el SAR, que es un radar de apertura sintética o cualquier otro  dispositivo, pues, van a recopilar constantemente datos del entorno que se está viviendo en el UAV  o en el drone, ¿vale? Y se van a tener que ir transmitiendo a tierra, ¿vale? Y toda esta  transmisión de datos, pues, van a recopilar información desde las imágenes, detectar obstáculos  en tiempo real, las señales de la trayectoria, que sería el UTM o ATM que se tiene, incluso la\n",
      "Intervalo 10-20 minutos:  climatología, ¿no? En la cual se están utilizando estos sensores, ¿no? O sea, hay que tener en  cuenta que toda la información que se vaya transmitiendo desde un objetivo móvil, llámese  si no quieren verlo como un drone, vean como un coche o un automóvil autónomo en la carretera,  todo eso se tiene que transmitir por la línea de datos, llámese por internet o una conexión de  radio. Eso es para que tengan en cuenta un poco el contexto. El desafío sería tener una eficiente  recopilación de datos y un eficiente procesamiento de estos mismos, ¿no? Como les he dicho, si tengo  mucha cantidad de datos y las señales son extremadamente grandes y demás, pues, cuando  vaya a generar las inferencias de las redes neuronales, pues, me puedo tardar segundos,  minutos y demás cuando dependiendo el tipo de caso en el cual quiero adentrarme, pues,  puede ser muy crítico. Si es conducción autónoma, si el coche se va a tardar, imagínense, 15 segundos  en identificar un peatón que está cruzando por el frente, pues, el coche lo va a arrollar, ¿no? Lo mismo  pasa en un drone militar. Si te va pasando por una zona de combate, pues, ya tiene 15 minutos por la  velocidad del drone, pues, imagínense, cuando lance el proyectil del misil, pues, no va a caer  donde se estimaba. O incluso, si se está haciendo una cirugía con soporte de inteligencia artificial  que se está llevando casi últimamente en Estados Unidos y aquí en Europa, pues, un movimiento malo  a distancia del, bueno, no a distancia, un movimiento tardío a causa de la guía de la inteligencia  artificial a la hora de hacer un corte, pues, puede producir un problema a la hora de tener una  cirugía, por ejemplo, en el corazón abierto o las que se utilizan las de cerebro también, ¿vale?  Entonces, todo eso hay que tener en cuenta, ¿no? Que partimos de un escenario en el cual vamos a proponer  nuestro problema, el volumen de los datos que vamos a tener y el desafío. Todo eso se traduce en la  transmisión de los datos que vamos a tener. Ahora, para esto hay que tener en cuenta lo siguiente.  El muestreo y la cuantificación de estos sensores son muy importantes, ya que dependiendo de su  capacidad para capturar y para representar de forma precisa esta información en un tiempo real,  me van a marcar las pautas de cómo se van a generar las imágenes, cómo se van a interpretar las  señales. Y por otro lado, hay que tener presente también la elección adecuada de la frecuencia de  muestreo, ¿vale? Porque si no hay una frecuencia de muestreo, ahora lo vamos a ver adecuada,  pues puedo tener muchos errores, ¿vale? Y por otro lado, la resolución a la hora de cuantificar  también es importante, ¿vale? Porque me va a garantizar la precisión de la información de  los sensores capturados. En pocas palabras, mi señala analógica estaría entrando a mi sistema,  que me va a convertir señales analógicos a digital. Hablamos la clase anterior, pasar del  mundo real o del mundo analógico al mundo digital. Pues el primer paso que va a tener es muestrearlo,  es decir, sacar muestras de esa imagen analógica o de esa señalada. Posteriormente, gracias a una  señal de tiempo, a un reloj, llámese reloj, voy a cuantificar esos dolores, cuantificar qué  significa, pues voy a dar valores, ¿sí? Porque el mundo analógico, como lo dijimos en la clase  pasada, tiene infinitos valores, pero ya cuando lo estoy cuantificando le voy a dar unos valores  finitos, ya no infinitos, ¿por qué? Porque estará dentro de un margen. Y finalmente, una vez obtenida  la señal cuantificada, pues voy a codificarla. ¿A qué me vale codificarla? Pues llevarlo al  mundo digital de 0 a 1, por ejemplo, para que sea algo binario, o que sea el mundo hexadecimal,  valores de 0 a f, 16 valores al mundo octal, de 0 a 8, etcétera. Entonces, para eso nos siguen.  Con todo esto, el concepto muestreo. ¿Para qué nos va a servir? Pues la señal de entrada va a ser  muestreada, es lo que les he dicho. Vamos a ir dándole distintos rozos a esa señal analógica, ¿vale?  Llámese una señal FDT que es analógica, la vamos a tratar de convertir en una IN, o una señal FDT,  perdón, a partir de una señal FDT, la vamos a convertir en una señal muestreada que sea en  función de I, ¿vale? Y después la vamos a digitalizar, la vamos a procesar digitalmente,  ya no sería una FDT, sería una FDI, F mayúscula, ¿vale? Donde vamos a tener ya nuestra señal  digital procesada y de ahí incluso podemos pasar nuevamente a la señal analógica, ¿vale? Es decir,  si esta es mi ruta para pasar del mundo analógico al mundo digital, el paso inverso sería pasar la  señal del mundo digital al mundo analógico, ¿no? Entonces a esta línea o a este trayecto se llama  un ADC, Analógico Digital, un convertidor, y el camino contrario un DAC, un Digital Tonalógico Converter,  ¿vale? Es importante tener este concepto más que todo para que se den cuenta que todo lo que viene  en el mundo real es analógico y todo lo que voy a trabajar yo es digital. Cuando lo quiero implementar,  otra vez en el mundo analógico, ya me sé, sacarlo en una en una bocina, altavoz, parlante,  lo que sea, pues tengo que hacer el camino inverso de esa discretización, ¿vale? O perdón,  de ese muestreo. Ahora, Luis, a ver que no he leído aquí previamente, me habéis escrito,  pero cuál es la recomendación para tener un óptimo procedimiento, ahora lo vamos a ver de  no te preocupes y el IZA de continua representación discreta, exacto, la T en función del tiempo,  es verdad Oscar, la T de aquí, esa en función del tiempo, todo lo que es T es tiempo, ¿vale?  Y Luis, es decir, se puede decir que las funciones transformadoras de la señal son  bio- univocas, ¿a qué te refieres con bio- univocas, Luis? Si quieres habilita la palabra,  el micrófono. Es decir, Puff, es que la, digamos, el dominio donde salgo de la señal analógica,  puedo reconstruir la función inversa que me regresa de manera univoca, o sea, sin error,  a la señal original, original, ¿no? A ver, no es, o sea, no es que te lleve a la señal original  sin errores, vas a tener errores, porque ten en cuenta que en el momento de discretizar y estás  perdiendo datos y el mejor ejemplo es este, y lo tenemos aquí. Mira, si este es el original,  no voy a llegar al original para nada, o sea, llegaré con un, a lo mejor con un 99% de  desactitud, pero ya tengo un error del 1%, entonces no sería la misma, ¿vale? Por más que haga un  proceso inverso con un conversor digital analógico, yo qué sé, de mil bits, no voy a llegar a eso,  esos valores siempre va a tener un error. O sea, lamentablemente no podemos llegar a un valor  analógico después de haberlo pasado, después de haberlo pasado de analógico a digital y volver  otra vez analógico, no podemos llegar al mismo valor, es, es imposible. En la literatura, varias  personas, varios libros dicen que sí, que llegamos, pero al día de hoy, que no se puede,  sí, nos aproximamos, sí, pero no lo llegamos, ¿vale? Después ya dicen, ya dicen en el número natural,  exacto, el cuantificador para que es el, ya vas a ver el cuantificador Melvin, no te preocupes,  vamos a llegar, vamos con orden, ¿vale? Y ahí está el ruido, así, exacto, debe haber un mínimo  de muestreo en el que se debe volver a reconstruir la escena original, exacto, y ese mínimo de  muestreo también no va a afectar. Se toma muestras de población total de datos, puedes regresar la  población de datos, sí, bueno, pasa mucho con las imágenes que sí toman, ¿vale? La entropía,  el próximo tema, y está en relación a esto, ¿vale? Pero no lo tocamos hoy, por eso dicen que  los transistores analógicos son mejores que los digitales, con los equipos de sonido en conciertos,  ahí te doy un punto Juan Marcos porque eso es algo muy verídico, ¿sabes? Y en los conciertos se  nota, senta bastante, es verdad, o sea, el mejor ejemplo es estar en un concierto que te puedas  mover tú alrededor, alrededor, o sea, en la media luna del concierto y ahí te vas a dar cuenta  cómo, dependiendo de la posición en la que estés, escuchas mejor o peor, pero eso es por el, por la  digitalización y la distancia de tiempo o el tiempo de vuelo del sonido que te llega al punto  donde tú escuchas. Hay una tesis doctoral en la Universidad de Munich, estoy seguro si era el 2008 o 2009,  donde habla de eso y utiliza toda la parte de paso analógico digital, digital analógico, donde se veía  todo eso, es chulísimo, si tenéis la oportunidad de buscarlo, era algo de, algo de estudio de\n",
      "Intervalo 20-30 minutos:  discretización analógica digitales en tiempo de vuelo, algo por ahí, no estoy seguro, cuando me  refiero a tiempo de vuelo es el tiempo que tarda en llegar la onda desde origen a destino, ¿vale?  Pero vamos, si lo encuentro, lo paso en el foro para que le echen un vistazo. Después, Juan Antonio,  es imposible que teníamos que poder leer señales a la velocidad de la, bueno, bueno, seguimos.  Para el concepto mostreo, por ejemplo, si quisiera hacer que el mosaico de la imagen cualquiera,  en este caso el del inicio, pueda tener un concepto de qué colores utilizar, cuántos debería utilizar,  o sea, qué cantidad de colores serían los idóneos y qué tamaño de pixel debería tener, o sea,  si quiero tener una imagen correcta de todo eso, debería haber algo que me indique que es, ¿vale?  Por otro lado, debería existir una fórmula que responda a las preguntas anteriores y en realidad la hay.  Esa pregunta está resuelta por el teoría de Nyquist, ¿vale? El teoría de Nyquist es, bueno,  Nyquist Shannon, que son estos dos hombres de aquí, normalmente se dice Nyquist,  es un estudio que lo hicieron muchísimos años atrás en los laboratorios Bell,  donde encontraron cuál sería la frecuencia mínima necesaria para realizar ese mostreo de las señales, ¿vale?  Quien dice señales dice una imagen, ¿vale? Para poder tener un valor eficiente, ¿vale?  Y el que dice es que, bien, el caso de los colores no sería 103, únicamente RGB,  o se refiere a la digitalización de los ocho colores, a la digitalización, me refería a la digitalización, ¿vale?  Entonces, para entender un poco más lo del teoría de Nyquist, vamos a dar el siguiente concepto, por ejemplo,  El día, si yo estudio mi día, o ustedes estudian su día, nos dicen, ah, mira, yo ayer, pues, he comido una manzana, ¿vale?  Si lo analizamos por ese lado, tenemos varias interrogantes, ha comido el desayuno, ha comido a media mañana,  la comida en el almuerzo, en la merienda o en la cena, no sabemos, ¿vale?  Entonces, digamos que la información ha sido digitalizada, por así decirlo, de forma muy pobre, porque es muy amplia, ¿vale?  Entonces, si yo digo, he comido una manzana el día de ayer, en el desayuno, ya sé que ha sido en un periodo de tiempo de 12 horas,  es decir, desde que me despierto hasta mediodía, 12 horas, y desde, bueno, las que sean.  Y después tengo el siguiente periodo de tiempo, desde mediodía hasta la noche, y finalmente en el periodo en el que estoy durmiendo, ¿vale?  Ahora, teniendo en cuenta que el día tiene 24 horas, si yo lo parto a la mitad, sé que si he comido la manzana, va a ser justamente en el periodo en el que estaba despierto, no en el que estaba durmiendo, ¿no?  Por ejemplo, entonces, Nyquist dijo que la frecuencia de muestreo que deberemos tener, debe ser al menos el doble de la frecuencia original.  Si el día es mi frecuencia original, mi periodo de muestreo va a ser la mitad, es decir, 12 horas, ¿vale?  Por lo tanto, la frecuencia del evento de un día será 24 horas. Su muestreo cada 12 horas.  Ahora, ¿se puede muestrear a más velocidad? Sí, se puede muestrear a más velocidad, ¿sí?  Por eso he hecho la división de muestrear cuando me he comido la manzana, en el desayuno, a media mañana, en el almuerzo o la comida,  a media tarde, la merienda, en la cena, hasta cinco veces, ¿vale? En este ejemplo.  Pero como mínimo necesitamos el doble, ¿vale? Dos veces la frecuencia original, ¿vale?  Con esto, pues, por ejemplo, para tener en cuenta la representación matemática, ¿qué dices, Maldi?  Cuando así pregrado, ¿cómo?  Cuando así pregrado, ¿cómo me he contado del muestreo de la etasima? ¿Cómo?  Bueno, si te explica, mejor, Maldi, escríbelo, ¿vale? Sigo con esto.  Ahora, la representación matemática de este proceso muestreo es el siguiente, ¿vale? Quedarse con la fórmula, ¿vale?  La función de x de n es igual a xn de ts del tiempo y demás, donde cada n pertenece a los números zeta y ts pertenece a los números reales, son racionales, ¿no?  Entonces, teniendo en cuenta que mi señal analógica continua en el periodo del tiempo es esta,  yo voy a muestrearla con estos valores, ¿vale?  Teniendo en cuenta que t es el periodo de muestreo y la frecuencia como tal es uno partido entre t, que sería mi frecuencia de muestreo, ¿vale?  Entonces, voy muestreando a cada intervalo de tiempo mi señal analógica.  Y en el número, puede ser negativo, sí, ¿vale?  Entonces, con todo esto, ¿qué sucede o qué problemas existe con el periodo de muestreo?  Por ejemplo, esta es mi señal de baja frecuencia, ¿vale? Y voy muestreando más o menos al doble, ¿vale?  Entonces, como estoy muestreando al doble la frecuencia de muestreo, pues tengo estos valores.  Ahora, si sigo manteniendo esta frecuencia de muestreo y mi señal de frecuencia es mucho más alta, obviamente estoy perdiendo datos, ¿no?  Aquí estoy creando esa pérdida de datos que se llama lehacen.  Si se dan cuenta, si esta es mi frecuencia de muestreo, ¿vale?  Y muestrear al doble de ésta sería los palotes rojos y demás, es correcto.  Pero aquí mi frecuencia ya ha cambiado, pero mi frecuencia de muestreo no, no la he cambiado.  Entonces, tengo pérdidas de señales o de información.  ¿Ese periodo de muestreo no genera error de sesgo?  Bueno, claro, pues aquí está generando el error, por eso es el lehacen.  Cuando hacía el pregrado, recuerdo una técnica de conversión analógica digital, delta sigma, donde el periodo de muestreo era bastante grande.  Ah, vale, vale, ahora te entiendo.  Es que mal, si me lo pones así, te entiendo mejor.  Sí, existe eso del delta sigma, ¿vale?  Eso normalmente se utiliza en la parte electrónica, que creo que eres electrónico o teleco o a secas, y tienes eso, ¿vale?  La pega de eso, perdón, el problema de eso, te genera muchos bits.  Si te va a servir para hacer un procesamiento rápido o capturar algunas muestras un poco aleatorias, perfecto.  Pero si vas a utilizar el 100% y vas a utilizarlo para aplicaciones que necesitan una respuesta casi media etapa,  ahí tienes un pequeño problema a la hora de utilizar esos datos, ¿vale?  O esos, o los valores de esa señal.  Sin embargo, si tu sistema que te va a hacer el cómputo de todo eso es extremadamente rápido, pues no vas a tener problema, ¿vale?  Pero sí, existe eso del delta sigma.  Ahora, este aliasing que ven aquí con la imagen pasa también en una imagen en dos dimensiones.  Si se acercan un poco, van a ver cómo la circunferencia que tengo aquí tiene como unas pequeñas sombras en ambos ejes, ¿vale?  A esta resolución de 64 por 732 píxeles.  Y también pasa lo mismo a 400 por 475.  O sea, eso es porque está mal hecho, ¿vale? Está mal muestreado, ¿sí?  ¿Dónde se pueden ver o dónde se pueden analizar este tipo de imágenes en el mundo real sin salir de casa?  Entonces se ponen a ver videos de YouTube, por ejemplo, de muchos años atrás.  Yo qué sé, personas que están pintando o personas en la playa, lo que sea.  Y van a ver como en su pantalla de 4K y demás que van a tener ese problema de muestreo.  Y van a ver como errores, como borrosidades entre medias y demás.  Y lo que está pasando, por ejemplo, aquí en Europa con el cambio de los canales digitales de un UHD que es de 1080 a 4K, está pasando lo mismo.  Hay mucha borrosidad entre medias y está pasando este aliasing a la hora de reproducir esas imágenes.  Pero bueno, con todo esto, el teorema de noche, lo que hablamos de IQuid Channel, ¿vale?  Nos sirve para resolver todo el contenido frecuencial de una señal que debe mostrarse a una tasa superior al doble de la frecuencia máxima, ¿vale?  De la señal presente.  Ahora, teniendo en cuenta la señal imitada en banda, es decir, que no tiene frecuencias más allá de los hertz o de la mesiomega, como quieren llamarlo,  se puede representar totalmente mediante las muestras si la tasa es mayor al doble de la frecuencia de muestreo, ¿vale?  Esta es la tasa de IQuid, ¿vale? Es dos veces la frecuencia de muestreo que hemos dicho.  Y esto les va a servir siempre para todos los problemas que vayamos dando a lo largo de la clase.  Porque cuando les diga tener en cuenta o indicar la frecuencia muestreo de una señal lógica de tal valor, no sé qué, pues ya saben que tiene que ser el doble, ¿vale?  ¿Por qué se los digo? Porque puede ser pregunta de examen, ¿vale?  Ya he dicho, omega es igual a 2 pf, sí. Muy bien.\n",
      "Intervalo 30-40 minutos:  Vale. Seguimos.  Ahora, para mostrarte una señal continua, no limitada en banda mediante el procesamiento,  pues se tiene que tener en cuenta que es necesario tener un pequeño filtrado, si lo quiero decir así, que me va a limitar esa frecuencia, ¿vale?  Esa frecuencia máxima. ¿Por qué? Porque la señal anológica es amplia, es muy grande.  Hay mucho ruido en el medio y no sé qué. Entonces tengo que aplicarle un pequeño muestreo, perdón, un pequeño filtrado antes de pasar al muestreo, ¿vale?  Ahora, ¿cómo puedo hacer, cómo sé qué filtro utilizar? Pues en este caso un filtro paso bajo.  Ahora me dirán que es un filtro, que es un filtro paso bajo.  Pues vamos a partir de lo que sería la convolución, ¿vale?  La convolución lo voy a dejar en términos muy sencillos, ¿vale? Porque, bueno, la mayoría de ustedes son ingenieros y lo han visto.  Multiplicación de dos funciones es una convolución. Nada más. Ya sea en la parte continua o la parte discreta, ¿sí?  El mejor ejemplo es ese, multiplicar dos funciones, ¿vale? La parte contínua o la parte digital que tenemos aquí, ¿sí?  Ahora, esto no lo he hecho yo, lo he sacado obviamente de internet que está puesto en todos los tutoriales y creo que les he puesto también un pequeño enlace de un vídeo  donde van a resolver el problema de la convolución, que no hay que tener miedo con esto.  Pero no es más que multiplicar dos funciones para obtener una tercera que me va a ayudar a resolver el problema o a crear el filtro.  Porque el filtro es mi señal normal o mi señal analógica multiplicado por esa función del filtro. Ya sea paso alto, paso bajo, paso banda, etc.  Para que se haga una idea un poco más rápida, pues sería multiplicar mi función por ese filtro en cada uno de los componentes.  Es decir, esto es una matriz de 3 por 3, esto es una matriz de 6 por 6, pues para el primer trozo se multiplica punto a punto,  voy recorriendo uno, hago la misma multiplicación, la misma multiplicación hasta terminar en la matriz en filas y continuo con columnas.  Y luego resuelvo. Aquí está una explicación un poco más somera de eso, donde voy multiplicando este 4 por el 6, pues voy haciendo lo mismo.  O sea, voy recorriendo cada uno del kernel de convolución por cada uno de las filas columnas de la matriz.  Bueno, no vamos a entrar más en detalles, ya les repito, creo que he puesto uno de los vídeos al final de la presentación, donde van a poder resolver esta duda de la convolución.  Pero lo importante es tener en cuenta que es solamente una multiplicación de funciones.  ¿Cómo se define la matriz de un filtro? Pues tú la tienes que crear, ¿vale?  Puedes tener un filtro que sea paso alto, por ejemplo, si te das cuenta, si te das cuenta aquí el filtro, todos son unos, es decir, abajo y justo en el pico hay un alto, ¿no?  Si te das cuenta, sería como una gausiana, ¿vale? Entonces ahí estás dejando pasar solamente los valores que están en el centro y hacia arriba, por ejemplo, que te hagas una idea.  Vamos, tenemos una clase exclusivamente donde vamos a hablar de filtros, ¿vale?  Pero básicamente va a depender un poco de eso.  ¿Señal comportador en transmisión de SRAD? Sí, eso. Muy bien, Juan Marco.  Te veo muy en el ajo, pero ya en la parte más técnica, digo más de ejecución.  ¿Los filtros es la parte analógica o se convierte en la parte digital? A ver, hay filtros analógicos y hay filtros digitales, ¿vale?  Hay filtros en cualquier lado. En este caso sería un filtro digital que me estaría, bueno, un filtro analógico en realidad, perdón, que me estaría reduciendo o cuantificando el valor de, perdón, no estaría sesgando el valor de la frecuencia de entrada, ¿vale?  Aquí te lo voy a poner.  Si esta es mi señal analógica, pues la voy a cortar, ¿vale? ¿Por qué? Porque hay mucho ruido a la entrada y no me sirve frecuencias menores ni mayores.  Por ejemplo, ya me hacía un micrófono que si es muy bueno tendría un ancho de banda, es decir, un rango de frecuencias en las cuales voy a capturar mejor el señal que en otras, ¿vale?  Vale, con todo esto, la convolución, ya les he dicho, multiplicación de dos funciones, ya está, no nos vayamos por las ramas, ¿vale?  Los filtros siempre se desgan, claro, pues filtrar significa filtrar que te filtre.  Tú, por ejemplo, cuando tienes una, yo qué sé, lo que te ha sobrado de aceite de la cocina, que te está refriendo patatas, si quieres filtrar para que no se queden los residuos, entonces colocas un filtro de esto de aceite, un papel lo que sea,  echas el aceite y la suciedad se queda arriba y pasa solamente el aceite. Entonces, estás sesgando datos, ¿vale? Estás sesgando ruido, ¿vale?  Ahora, esos datos que estás sesgando, volviendo al tema de inteligencia artificial, a lo mejor te estás quitando tu información, ¿vale?  Eso puede ser perjudicial, pero para eso hay una materia, hay un tema donde vamos a analizar con más detalle, ¿vale?  Pero sí, estoy quitando datos.  A ver, por ejemplo, si quiero hallar el muestro mínimo de una señal como sería la siguiente, ¿vale?  Coseno de 100 pi T más seno de 200 pi T más coseno de 500 pi T más pi cuartos más 7, ¿vale? Es una señal analógica, ¿vale?  ¿Cuál sería la frecuencia máxima de esta señal?  Bueno, aquí está la respuesta, es 500. ¿Por qué? Porque es omega T, ¿vale? Que sería 100 pi, sería omega.  Pues la frecuencia máxima 100, 200, 500. Pues 500 es la frecuencia máxima, ¿vale? La frecuencia radial sería 500.  Entonces, teniendo en cuenta eso, en hertz sería 250 hertz. Eso es, ¿vale?  Según Nyquist, ¿cuál sería la frecuencia mínima necesaria? El doble, de 250 sería 500.  Pero, ¿qué pasa si yo coloco 1000? Pues también funciona, ¿vale?  Si en el examen ustedes me dicen que la frecuencia de Nyquist mínima necesaria es 500, pues respuesta correcta.  Si me dicen 1000, respuesta correcta. Pero, ¿decirme por qué? Si me dicen 1000, porque la frecuencia mínima necesaria es 500, pero 1000 también es aceptable.  Pues viene la respuesta correcta. Pero, explicadme, ¿vale?  A ver, Javier, ¿qué dices? ¿Cómo podríamos llamar a cuando inferimos datos faltantes?  Esto sería opuesto al filtrado. Pues ahí estarías inventando detalles. ¿Vale?  Es que la verdad, si te faltan datos y no los tienes, pues ¿cómo los infieres?  Habría que haber... ahí hay formas, ¿vale? Hay modelos matemáticos que te ayudan a eso.  Bueno, seguimos porque miren, ya pasa mucho tiempo y estamos casi en la mitad.  Datos sintéticos, por eso inventarse datos.  Pero es que no serían sintéticos del todo. Sintéticos serían que tú los tengas completos.  O sea, que ni utilices datos o parte de los datos previos que hay para formar ese dato completo.  ¿Vale? Datos sintéticos serían un dato nuevo.  ¿Vale? Bueno, volvemos a la materia.  Entonces, este es el ejercicio.  Ahora, ¿qué pasa con el muestreo? Por ejemplo,  en la telefonía móvil o celular, la frecuencia de muestreo para la captura de voz me oscila entre 8 a 16 kHz.  ¿Vale? Ahora, para las aplicaciones de audio con mayor calidad, es decir, alta resolución, sonido de TES, no sé qué,  para la transmisión de música o la grabación de audio en alta fidelidad, la frecuencia de muestreo puede ser de 44 para arriba.  Entonces, ¿con esto qué quiero decir? Pues que la señal analógica que estoy muestreando va a tener mejor resolución.  ¿Vale? Se va a aparecer, puede hacer que mi muestreo se parezca lo más posible al mundo analógico.  Sería una extrapolación entre datos conocidos.  Se puede hacer una extrapolación volviendo al tema anterior.  ¿Vale? Bueno, seguimos que tenemos que terminar.  Con todo esto, teniendo en cuenta lo que sería muestreo, vamos a ver lo que sería la cuantificación.  La cuantificación es el proceso de discretizar la señal contina o analógica en el dominio del tiempo, ¿vale?  O del espacio en un conjunto finito de niveles discretos.  Es decir, si mi señal analógica tiene infinitos puntos, yo ya le voy a dar unos niveles en los cuales se va a cuantificar.  Por ejemplo, en una función xq de T, voy a cuantificarlo en q de x de P,  donde xq de T es la señal cuantificada, q así en cada uno de los valores de la muestrea, en el valor discreto.  Y bueno, se multiplicaría por ahí que se vendría a hacer el error.  Ahora, para cuantificar, si se dan cuenta que en esta señal analógica, es la señal azul,  entre valores de 1,1 o 1,5, como quieran verlo, pues aquí tengo un bit.  0 y 1 o 0 menos 1 es un bit, ¿vale? Porque es o uno o el otro.  Estoy cuantificando.\n",
      "Intervalo 40-50 minutos:  Si tengo dos bits, pues de 0 al valor máximo de la señal analógica que tengo, pues tengo dos escalones.  Puede ser de 0 a 0,75, de 0 a 0,75 a 1,5.  A los dos lados, positivo y negativo.  Entonces, tengo esa cuantificación.  Lo mismo cuando tuviese tres bits o cuatro bits.  La de cuatro bits se parece lo más parecido a la señal analógica.  ¿Vale?  Que dice Rubén, parece a una función objetivo.  Parece a una función objetivo.  No te comprendo las preguntas, Rubén.  Parece a una función objetivo.  O te falta una coma o no entiendo tu pregunta.  Pero bueno.  La parte de cuantificación sería eso.  Con el número de bits tratar de igualar a esa señal analógica.  Si nos vamos al mundo de las imágenes, sería eso.  Tener la mayor cantidad de bits para tener los colores RGB que habíamos visto en una imagen anterior.  Para cuantificar.  Esto es muy importante porque a lo mejor se les podría apretar en el examen.  Lo digo así por sacar un ejemplo.  Primero, voy a definir el rango de valores posibles de mi señal x de t.  Y el rango máximo y el rango mínimo.  Y saco la diferencia.  Entonces, ya tengo el rango en el que me debería mover.  Después, voy a calcular el paso de cuantificación.  Es decir, mi delta de cuantificación.  Es decir, el rango entre n.  Donde n va a ser el número de niveles de cuantificación.  N igual a 2.  O sea, el número de bits.  2 a la n bits.  Y después voy a aplicar esta cuantificación a cada una de las muestras de la señal analógica.  Perdón, de la señal discreta.  Y tenemos esta ecuación.  Donde el round, en este caso viene a ser mi función de redondeo.  Para el examen hay que tener las fórmulas.  Se las resuelve.  A ver, yo siempre digo que se guarden todas las fórmulas.  ¿Por qué?  Porque en el examen se las puedo pedir.  Y que me reemplacen con los valores.  Por ejemplo.  Y bien que lo has dicho, José.  Esto es un ejemplo.  ¿Qué puede ser?  ¿Vale?  Teniendo en cuenta la frecuencia de esta de Nyquist.  Pero en el examen no se deja sacar nada.  Ahora hablamos del examen un poco adelantado.  Para la frecuencia de Nyquist, les puedo decir que me indiquen cuál sería.  Y cuál sería.  O que me resuelvan la frecuencia de Nyquist.  Si me frecuencia máxima, el de 10.  Entonces ustedes me dicen.  Pues f es igual.  F es el mayor o igual a 2 por 10.  Igual a tanto.  Sería 20.  ¿Vale?  Entonces.  Por eso les digo que se quedan con las ecuaciones.  ¿Vale?  Porque en algún momento en el examen lo pueden utilizar.  Ahora, en el examen pueden utilizar los apuntes.  Y los apuntes tienen las ecuaciones.  Entonces quedarse con ellos.  ¿Sí?  Seguimos con el tema de cuantificación.  Por ejemplo.  Y lo que habíamos dicho antes.  Si tengo un bit.  Es decir, una imagen blanco y negro.  Pues tengo este tipo de resultado de la imagen.  Si tengo tres bits.  Pues ya mi imagen ha mejorado.  ¿No?  Su cuantificación es mejor.  Con cinco bits ya ni te cuento.  O sea se ve bien.  Y con ocho aquí ya mejora.  Ahora simplemente.  256 bits.  Pues mira.  Eso ya sería el no va más.  Ahora.  Por ejemplo.  Si a un hombre le dices que describa un color.  Por ejemplo.  De una prenda.  Y a una mujer le dices que te describe el mismo color.  Pues claramente el hombre va a ver menos bits que la mujer.  La mujer está más evolucionada.  Para ver una gama de colores más amplia que el hombre.  Lo saco siempre en calidad de broma.  Pero es que es la verdad.  Y yo con mi esposa.  Por ejemplo.  Yo le digo.  Este es un color rojo.  Y ella me dice.  No es un rojo.  Por ejemplo.  Un rojo pastel.  O un rojo.  Fusión.  Un rojo.  Lo.  Yo que sé.  O un rojo Ferrari.  Porque hay distintos tonos de rojo.  Que para mí.  Quien hace el mismo.  Pero para ella ya.  Tiene una capacidad de discernir.  Es decir.  Tiene más bits de resolución.  En su cerebro.  Para discernir mejor los colores.  Vale.  Entonces.  Es por eso que la cuantificación es importante.  Vale.  Para poder.  Tener unos valores.  Lo más idóneos a la realidad.  Al mundo analógico.  Y tal.  Pero teniendo en cuenta que nunca vamos a llegar a esa precisión.  Vale.  Eso.  Tenganlo presente.  Vale.  Ahora.  ¿Qué pasa en el mundo de la tecnología?  Pues.  Si nos vamos un poco.  A la parte.  Que utilizamos de transmisión.  Vale.  Por ejemplo.  La tecnología de cuatro.  No la de cinco.  La de cuatro.  Los bits de cuantificación.  Para la parte de voz.  Solen tener un rango de ocho a dieciséis bits.  Vale.  Por muestra.  Sin embargo.  Para señales de audio.  De alta calidad.  Para grabaciones de estudio.  Tienes de dieciséis.  A veinticuatro.  Y si nos vamos.  Un poco incluso.  A la parte de.  De captura de sonidos.  De.  De sonidos.  De.  De.  De.  De.  De.  De.  De.  De.  De.  De.  De.  De.  De.  De.  De.  De.  De.  De.  De a de.  De.  De.  De.  De.  De.  De.  De.  De.  De.  De.  De.  De.  De.  De.  De.  De.  De.  De.  De.  De.  hacer toda esa transformación un poco automática,  dependiendo el acelerno en el que esté.  Hablando un poco en estructura del teléfono móvil,  no de tecnología móvil, ¿vale?  O de celular.  En resumen, la cuantificación es un paso esencial en el  proceso del desarrollo del sistema de inteligencia  artificial, que implica convertir datos de múltiples  formatos a una forma que los modelos de inteligencia  artificial puedan procesar y analizar eficazmente.  Más que procesar y analizar, que tu sistema donde vayas a  generar las inferencias, pueda analizar.  ¿Por qué?  Si utilizas un cluster HPC de cómputo que tienes,  imagínate, infinidad de procesadores y demás para  inferir las cosas, puedes meterte imágenes que pesen  gigas.  Pero si tú lo utilizas en tu portátil que tienes una Nvidia  GTX 400, por ejemplo, no puedes meter imágenes que  pesen gigas.  Entonces te ves desgato.  Va a depender un poco de la infraestructura en la cual vas  a implementar tu sistema de inteligencia artificial, ¿vale?  Felipe, los bits para todas las muestras que estamos trabajando  en una sola señal deben ser los mismos para todas las muestras,  o pueden variar.  Por ejemplo, para una canción muestra de 6 bits,  indica que todas las muestras son de 16.  O puede haber otras menores.  Puede haber otras menores.  Porque va a depender un poco del proyecto que quieras hacer.  Sabes, va a depender mucho de eso.  Forge el número de bits para una cuantificación en el  abrinda el escenario del problema a resolver.  O debe ser objeto de cálculo.  A ver, se puede hacer por cálculo, ¿vale?  ¿Por qué?  Si tuvieses el ejemplo que dijimos aquí, ¿vale?  Aquí tenemos el N de la cuantificación, ¿vale?  Que sería 2 el número de bits.  Si quiero utilizar un canal RGB, que estábíamos hablando de 8  bits más o menos, ¿vale?  Sería 2 a la 8, ¿vale?  Porque hay 8 niveles, ¿vale?  O sea, sería 2 a la 8.  Ahora, si a esto le multiplicas, porque deberías multiplicarlo,  el tipo de variable, llámalo flotante o doble flotante,  yo qué sé, o long, yo qué sé, estarías multiplicando por eso,  ¿vale?  ¿Debemos poner el rango de bits para cuantificación?  Sí, es lo que estaba explicando ahora, ¿vale?  Entonces, los bits están presentes, ¿vale?  Va a dependerte un poco del tipo de variable, el tipo de los bits  que vas a necesitar y demás.  En este caso, 2 a la 3 sería los bits, ¿vale?  Vale, y seguimos, ¿sí?  Que yo siempre me paso de los 45 minutos, disculpame,  pero es que a veces el tema y las preguntas, pues, lo piden,  ¿vale?  Esto ya lo hemos visto.  Ahora, por ejemplo, la tasa de bits, en este caso,  viene a ser la clave del procesamiento de las señales,  ¿vale?  Y esta tasa de bits se define, o el bit rate, ¿vale?  Se define como la cantidad de bits transmitidos o procesados  por unidad de tiempo, generalmente por segundo,  los VPS, ¿vale?  Entonces, la tasa de bits se puede calcular como el producto  de la tasa de muestreo por los bits de cuantificación, ¿vale?  Porque de ese puesto, esta imagen tan bonita de 4G, 3G,  5G, Wi-Fi, Edge y demás cosas, es porque la tasa de bits por  segundo al día de hoy es tan alta que va creciendo que da  miedo, ¿vale?  Entonces, hay que tener en cuenta que aparte de la  transmisión, hay que tener presente que va a depender  también de la calidad de señal, los requisitos de  almacenamiento y la eficiencia de transmisión.  Tenemos un mundo grande por completo, ¿vale?  Respecto a la actividad 1, ¿se va a abrir el foro de primera  semana de mayo?  ¿Abrir foro de primera semana de mayo?  Bueno, no entiendo.  Pero bueno.  Respecto a la actividad 1, ¿se va a abrir foro de primera\n",
      "Intervalo 50-60 minutos:  semana de mayo?  Un foro, dices.  Bueno, puedes crear un hilo.  Si a eso te refieres, tengo que abrir el hilo hoy.  Los foros hoy, nuevamente, que me han dado la autorización.  Bueno, la autorización.  Me han borrado el foro que tenía y lo volveré a crear.  Ahora, aplicaciones para el muestreo, ¿vale?  Aplicaciones van desde, bueno, para muestreo y cuantificación,  ¿va?  Van desde la transmisión de audio y de video en tiempo real en  línea, ¿vale?  Les he puesto la transmisión de un drone.  ¿Por qué?  Porque el día de hoy, las imágenes que se están  transmitiendo, por ejemplo, un drone que no tiene la capacidad  de cómputo en el aire, tiene que transmitir todas esas imágenes  para que la inferencia que esté en la nube las vaya  realizando.  Entonces, si no tengo una conexión 5G donde la imagen vaya  a buena medida o 4G donde la calidad de imagen sea buena,  voy a perder datos y, por lo tanto,  mi inferencia va a obtener imágenes de muy mala calidad,  no va a haber buenas detecciones y demás.  Ahora, por otro lado, si el drone no se ve bien,  este es un drone que tiene capacidad de cómputo porque  tiene un ordenador en bebido, Jetson Orin,  el cual procesa directamente en ese ordenador todas estas  inferencias, ¿vale?  Para detección de objetos y demás en vuelo.  Entonces, ahí ya no me haría falta eso, ¿sí?  Pero si tuviese que utilizar la red de telefonía móvil,  sí necesito todo eso, ¿vale?  Por otro lado, los diseños de sistemas de control y  automatización.  Necesito que la transmisión de datos para los sistemas de  control sepan manejarme todo ese cúmulo de datos, ¿vale?  Por otro lado, capturar los datos en investigaciones  científicas.  Esto ya es para la gente que le gusta más la parte del  research, donde comienza a analizar todos los datos a toda  pastilla, pero si el canuto de información que me está  llegando la información, llámese datos de criptografía y  demás, son pobres, pues tengo un cuello de botella donde no voy  a poder generar o no voy a poder analizar todo eso de buena  manera.  Carlos que dice, ¿el Edge Computing sería mejor para  procesar grandes cantidades de datos y imágenes?  Por ejemplo, analizar tráfico de la gente en un estadio.  Pues sí, si tienes un Edge Computing de un DH, pues sí  tendrías todo eso.  Pero tienes que tener en cuenta también que por más que sea un  DH, necesitas que tu canuto de las capturas de imágenes hasta  el Edge donde estés procesando, tengan el mismo ancho de banda  de procesamiento.  Será interno.  Pero si todos van por cada, pues genial.  Pero si tienes alguno que vaya por el aire,  llámese Wi-Fi interna o Wi-Fi interna 5G y demás,  pues tienes que soportar eso.  No me vale un Wi-Fi de 2,4G, o sea, de ancho de banda,  porque podías llegar a lo mismo.  O cámaras que te admitan solamente por esa frecuencia.  Después, ¿qué impacto tengo con la tecnología?  Pues que voy a tener una mejora de la calidad de transmisión de  música, voy a tener mejor.  Lo que pasa hoy en día, Netflix, se transmite a tiempo real.  Pero si mi Wi-Fi o mi conexión de casa es mala,  no voy a poder ver un película 5G.  Los sistemas de calidad y de control,  tres cuartos de los mismos, no voy a tener un buen  procesamiento a la hora de discernir,  a la hora de detectar errores, por ejemplo,  en una cadena de producción.  O incluso, en la parte de transmisión,  en tiempo real de imágenes, por ejemplo,  lo que se está haciendo ahora mismo en Gaza,  en los hospitales de campaña, de naciones unidas,  bueno, de Naciones Unidas, no está metido ahí.  Los hospitales de campaña están comenzando a transmitir  imágenes de vídeo, de operaciones,  para tener soporte en Europa.  Entonces, si mi ancho de banda es malísimo o mis cámaras son  malas, no puedo sacar una resolución buena de mucha  cuantificación, pues no puedo ver ni dar esa ayuda a las  personas.  En conclusión, pues el muestreo y la cuantificación,  hemos visto que son procesos fundamentales en el  procesamiento de señal.  Y el procesamiento de señal viene a ser como el paso  premio al data curation de mi sistema de inteligencia  artificial.  Entonces, por lo tanto, tiene un impacto significativo en todas  las aplicaciones.  Hemos dicho transmisión de vídeo,  sistema de control y la parte de procesamiento digital.  Ahora, con todo esto, resolviendo la pregunta que  hice al principio es, ¿cuántas muestras y niveles de  cuantificación serían necesarias para representar  digitalmente una señal?  Pues ya las conocemos.  Vamos a calcular SN, con 2 por el número de bits.  Se va a calcular SN.  Por otro lado, ya sabemos cómo podríamos muestrearlo.  Entonces, ya sabemos muestrear, ya sabemos cuantificar,  pues ya podemos, más o menos, bueno, ya podemos, no más o  menos, que ya podemos representar una imagen  digitalizada.  Los valores que va a tener.  A ver, Felipe, ¿he sugerido utilizar la compresión antes de  transmitir?  En este caso, compresión se considera procesamiento.  Comprimir una señal, en pocas palabras, es filtrar,  si lo quieres ver así.  Entonces, estaría puesto.  Sería un preprocesamiento.  Sí, llámalo procesamiento, preprocesamiento,  estaría metido ahí.  ¿Vale?  Pues nada, ya sabemos resolver esta pregunta.  Les he puesto unos videoírios que les van a ayudar mucho en  este tema.  ¿Vale?  Y sobre todo, este concepto de muestreo que lo presenta aquí,  este de la Universidad de Valencia,  muy majete del hombre.  Este es un video largo, si más no equivoco.  Darles su tiempo, verlo el fin de semana y van a ver cómo se  van a distraer bastante y les va a gustar más este mundillo.  Ahora me diréis, ¿por qué es importante saber todo esto si lo  que yo voy a hacer es un, yo qué sé,  un arreglo neuronal de detección de, yo qué sé,  de huevos de pascua?  Pues esto les va a servir para que ustedes sepan qué cámara  elegir, qué tipo de equipo después de la cámara utilizar a  la hora de poder obtener esas imágenes,  almacenarlas y procesarlas en la red neuronal.  Por eso es importante.  Lo que estamos viendo en esta materia es analizar toda esa  cadena que me va a dar la red neuronal al final.  Desde cómo capturar las imágenes,  que ya hemos visto la anterior semana,  que es todos los sensores, esto de cómo debería muestrear y  cuantificar mis señales, es decir,  cómo debería hacer ese procesamiento después de obtener  las imágenes, cuántos bits me va a servir,  si mi proyecto de detección de huevos de pascua pues tiene que  ser de color blanco y negro o tengo que utilizar el RGB a gran  escala o me vale con 8 bits de codificación,  estamos viendo todo ese paso.  Y ahora, ¿qué nos prepara el siguiente tema o la semana que  viene?  Vamos a preguntarnos cómo identificar alteraciones  significativas en la distribución de energía de una  semana.  Para eso vamos a tener que identificar estas  alteraciones, ¿vale?  Donde es importante considerar los conceptos del ruido.  El ruido está presente en todo.  Tener en cuenta lo que es la entropía que a mí me lo  he preguntado, pero no voy a decir nada hasta la próxima  clase.  Y sobre todo, las otras medidas de complejidad de la señal,  que tiene una señal.  Y por otro lado, pues la distribución de una energía de  una señal nos va a proporcionar información valiosa sobre su  calidad, qué tan buena es, qué es lo que tiene y las  características, ¿vale?  Porque no es lo mismo.  Una señal analógica muy buena a una con ruido.  En pocas palabras, cuando vas en un concierto y quieres hablar  con tu pareja, pues no es lo mismo que estés tú en una  habitación hablando tranquilamente y demás a que  estés pegando gritos porque entre el concierto y la persona  que está a tu lado, pues no te deja escuchar, ¿vale?  Con todo esto, dudas, consultas.  Para calcular la limitación del proyecto, bueno, sí.  Las instrucciones de la actividad 1 no aparecen acorde a  lo comentado en la clase.  Sí, se van a modificar.  Eso es porque les he dicho, hay dos aulas,  una de percepción computacional y otra de visión,  de computer vision, ¿vale?  Se van a unificar, ¿vale?  Pero seguir a lo que se ha hecho en la presentación, ¿vale?  ¿Alguna otra duda más?  En el examen se puede usar el lector de PDF de tarro,  que es un PDF, una Adobe.  ¿Vale?  Pero eso ya lo vamos a ver luego.  Si no tenemos dudas, yo creo que ya lo podemos dejar ahí,  paro de compartir.  Vale, Carla, recordármelo en el foro, ¿vale?  Ahí, míramelo, has pasado.  Ahí, espérate.  Vale, lo coloco en descargas.  Muchas gracias, Carla.  Vale, pues nada, vamos a probar la grabación.\n"
     ]
    }
   ],
   "source": [
    "current_interval = 0\n",
    "accumulated_text = \"\"\n",
    "\n",
    "for segment in result2[\"segments\"]:\n",
    "    start = convert_to_min_sec(int(segment[\"start\"]))\n",
    "    end = convert_to_min_sec(int(segment[\"end\"]))\n",
    "    text = segment[\"text\"]\n",
    "\n",
    "    # Verificar si el segmento sigue dentro del intervalo actual de 15 minutos.\n",
    "    if start // n == current_interval:\n",
    "        accumulated_text += \" \" + text\n",
    "    else:\n",
    "        # Imprimir el texto acumulado para el intervalo actual.\n",
    "        print(f\"Intervalo {current_interval*n}-{(current_interval+1)*n} minutos: {accumulated_text}\")\n",
    "\n",
    "        # Actualizar el intervalo y reiniciar el texto acumulado.\n",
    "        current_interval = start // n\n",
    "        accumulated_text = text\n",
    "\n",
    "# No olvidar imprimir el último intervalo acumulado fuera del ciclo.\n",
    "print(f\"Intervalo {current_interval*n}-{(current_interval+1)*n} minutos: {accumulated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "result3 = model.transcribe(\"/home/contrerasnetk/Documents/Classes/VisionArtificial/3.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intervalo 0-10 minutos:   vamos a botar las cururas  y compartir  en la presentación  vamos a compartir la pantalla 2  No podéis ver?  Espera un rato que tengo el chat aquí.  Sí, perfecto.  Genial.  A la persona que me mandó el documento con la opción para quitar lo que está apareciendo  todavía, el Closet Caption y demás, que sepan que no ha funcionado.  Vale.  De todas formas se lo he comentado al servicio técnico.  Se ha puesto en contacto conmigo esta mañana, pero yo he estado reunido toda la mañana,  así que no he podido arreglarlo, o sea, que disculparme.  En cualquier caso, espero que para la próxima semana esté ya solucionado.  ¿Vale?  Podéis ver, ¿verdad?  Sí.  Bueno, pues vamos a comenzar el tema 4, que pertenece al bloque 1 todavía, ¿vale?  De la materia de visión artificial o percepción computacional.  Y el tema es referente a puentes y tipos de ruido.  Vale.  Cualquier consulta, duda, ya sabéis, me podéis interrumpir.  Si queréis usar el micrófono, pues usamos el micrófono.  Y si no, pues escriban en el chat.  Saludos desde Lima, dice Antonio.  ¿Qué tal?  Bueno, yo también tengo trabajo híbrido, pero vamos, cuando uno está reunido,  está en casa o en la oficina, pues está arriba.  ¿Vale?  ¿Qué tal, Yalitza?  Este tema seguramente te va a gustar a ti porque tiene mucho de ruido.  Como tú sé que estás ahí en el tema de acústica, pues si quieres interrumpirme  en algún momento y dar algunas pinceladas de tu conocimiento, pues bienvenido, o sea, ¿vale?  Pues vamos, vamos para allá.  Vamos a colocar el puntero.  Laser.  Genial.  A ver, para recordar lo de la semana pasada, ¿vale?  Hemos hablado del muestreo, la cuantificación.  Hemos visto cómo a través de una señal analógica o del mundo real, pues lo hemos pasado a una  señal discreta, discretizada.  Entonces hemos muestreado esta señal en distintos valores.  Lo hemos cuantificado en distintos valores para obtener una discretización de la misma, ¿vale?  Hemos visto también que la frecuencia óptima de muestreo tiene que ser mayor o igual a dos veces  la frecuencia máxima de la misma, ¿vale?  Eso es lo más importante, lo que nos sirve, ya que es la base de la transformación analógica o digital  para poderlo utilizar en lo que sería la parte de inteligencia artificial o cualquier sistema de análisis digital, ¿vale?  Con todo esto, pues la pregunta que nos vamos a hacer el día de hoy y lo vamos a ir resolviendo es  ¿cómo hay de identificar aquellas alteraciones significativas en la distribución de energía de una señal?  Cuando nos referimos a la distribución de energía es a la intensidad que tenga esa señal, a la potencia.  Ahora mismo, si me estáis escuchando a mí, es posiblemente que escuchen algún ruido que se está retiendo, ¿vale?  Que no es más que el ruido presente en el mundo real donde estamos todos, ¿vale?  Entonces, ese ruido siempre va a estar presente en el mundo real, ¿no?  En todos los sistemas que tengamos siempre hay ruido, en todo lado siempre hay secuelas, ¿vale?  Entonces, para abordar esto vamos a hablar un poco de la introducción, vamos a hablar después de los objetivos,  de los tipos de ruido, los procesos estocásticos y la entropía, ¿vale?  Entonces, como introducción, en el ámbito de la información, pues el ruido es toda señal no deseada  y de naturaleza aleatoria que modifica la intensidad de esta señal original.  Por ejemplo, estamos en un concierto, estamos escuchando la música tranquilamente, pero claro,  hay ruido del rebote de la señal o del rebote de los altavoces o cuando estoy hablando con mi pareja  en medio de la calle, pues los coches hacen ruido, un niño llorando, lo que sea,  cuando uno está en un hospital que dice, por favor, manténganse lesion, que igual hay ruido,  hay ambulancias y demás, ¿no?  O sea, el ruido viene del mundo real y es de forma aleatoria.  La definición del ruido, pues lo podemos decir como una señal SDT, que es igual a una función FDT  más el ruido de T, ¿no? El ruido que pertenece o el que genera el ruido, perdón,  en nuestra adquisición de señales, ¿no?  Entonces, con todo esto, debemos hacer una caracterización del ruido.  Tenemos que presentar el ruido con una señal determinista,  donde podemos conocer su valor exacto para cualquier instante de tiempo T, ¿vale?  Y viene definido por la ecuación, SDT del ruido es ola seno de 2pi FDT, por ejemplo.  Antonio, lo he comentado hasta el principio de la clase, es que no encuentro,  o sea, si alguien lo puede hacer, bienvenido sea, no he tenido tiempo de que la gente de IT  pueda solucionar el problema.  Es más, ahora si te das cuenta, esa transcripción en inglés que te distrae es ruido presente  en una visualización, ¿vale?  Y por otro lado, para una señal aleatoria, pues incluye un término de adenturadeza aleatoria,  que sería en este caso estocástica, ¿no?  No es posible obtener con exactitud su valor en un determinado de tiempo T,  y por lo tanto, pues la caracterización del término de adenturadeza aleatoria  permite estimar un valor de señal T con un error determinado, ¿no?  Tenemos ahí las ecuaciones, por favor, quédense siempre con las ecuaciones, como les digo.  Y bueno, eso sería un poco la introducción del tema, ¿vale?  Con todo esto, el objetivo va a ser definir el ruido en el contexto de los sistemas  para el tratamiento de la información.  Vamos a tener que conocer los distintos tipos de ruido existentes que pueden afectar  a nuestras señales y modelar matemáticamente el ruido, ¿vale?  Darme un segundo, por favor, ¿vale?  Darme un segundo, que creo...  Darme un segundo, por favor, que se me ha metido un ruido entre medias.  Tienes más que...  Vale.  No.  Ahora sí, disculpadme.  Tenía un pequeño problema con la pantalla que se me ha desaparecido.  Eso de manejar tres pantallas, pues es complicado, ¿vale?  Entonces, los objetivos va a ser definir el ruido en el contexto de sistemas  para el tratamiento de información.  Por otro lado, conocer diferentes tipos de ruido existentes  que pueden afectar a estas señales, ¿no?  Y después hacer un modelado matemático del mismo  y comprender el concepto de entropía  y su relación con la naturaleza aleatoria o estocástica de las señales, ¿vale?  Vamos a ver los distintos tipos de ruido que existen hoy en día.  Por ejemplo, el ruido en percepción computacional o computer vision  se puede definir como una variabilidad no deseada  que afecta los datos adquiridos por un sistema de percepción, ¿vale?  Por los sensores actuadores.  Si vemos la imagen, espero que varios de ustedes recuerden  cómo el ruido de un switch de una cafetera  producía aquellos artefactos o aquello...  O aquel...  Esa señal de basura que Sheldon Cooper en The Big Bang Theory  le produjo un dolor de cabeza a la hora de exponer sus resultados.  Entonces, se puede decir que el ruido presente, en este caso,  ha sido proveniente de un entorno ambiental, ¿vale?  Es decir, como la iluminación cambiante, el ruido acústico o electrónico.  Y sensorial, un ruido inherente en el propio dispositivo de percepción.  Es decir, micrófonos o cámaras. En este caso, el switch. ¿Vale?  Entonces, teniendo más o menos ese concepto,  tenemos estos dos tipos de ruido ahora mismo, ¿vale?  También está el ruido externo al sistema.  ¿Por qué digo externo al sistema?  Porque nuestro sistema...  Imagínense, vamos con la parte de arriba, ¿vale?  Vamos con... quedémonos con estas dos cosas.  Si yo estoy sacando una foto, ¿vale?  En un paisaje o a las estrellas, digamos,  una fotografía en la noche, a mi pareja, a mi hija, un cantante y demás,  pues voy a tener la influencia de varios artefactos  o varios otros actuadores o emisores de ruido  que van a influir en mí.  Ya sea la luz, ya sea los destellos a través del rebote de la luz en un cristal,  la luz de las estrellas, la luna, etc.  Cuando quiero obtener imágenes del cielo, por ejemplo,  con telescopios y demás, tengo la influencia de algunas estrellas  que están alrededor del foco donde quiero sacar alguna imagen.  Sin embargo, a día de hoy, con tanta basura espacial que tenemos,  sobre todo la cantidad actual de satélites, starlight,\n",
      "Intervalo 10-20 minutos:  nuestro amigo el ingenuoso, el dueño de los coches Tesla,  que se me dio el nombre,  esos satélites, a la hora de sacar las fotografías y demás,  emiten reflejos de la luz del sol y demás  y de otros elementos, causando una nubosidad  o una mala adquisición de información  al momento de capturar las imágenes.  Entonces, tenemos en ese caso ruidos externos que nos van a influir.  Elon Musk, muchas gracias a todos ustedes.  Me van los nombres, no puedo contener tanto.  A ver, Jorge, ¿qué dices?  Con relación al concepto de ruido,  puedo inferir que la interferencia de señales es ruido por aleatoriedad  en el momento en que se presenta.  Claro, es que hay ruido aleatorio.  Eso es lo que quiero decir viendo.  ¿Cuál sería el ruido más presente que tienen en las imágenes?  Uf, en las imágenes, Carlos, hay ruidos de tomo.  A mí me gusta, porque es el que más aparece para mí,  el ruido salpimienta.  Y ahora vamos a ir viendo de ahora en adelante esos ejemplos.  Tal vez podríamos filtrar el ruido de Starlink.  No puedes filtrar ese ruido, Hernán,  y eso es importante que lo sepáis.  ¿Por qué?  Porque al estar filtrando ese ruido solamente del reflejo,  no digamos del radiotelescopio, solamente imagen óptica,  al quitar ese reflejo, lo que estás haciendo es oscurecer  el fondo de la imagen donde quieres tener.  Igualmente.  Entonces, solamente por óptica, cuando oscureces,  y es muy complicado obtener una señal original.  Ahora, si utilizamos radiotelescopios,  pues lamentablemente es como si estuvieras emitiendo hacia un punto,  y ese ruido, que es la presencia de los satélites,  hacen que rebote la señal.  Entonces, tus ondas de emisión o de recepción van a chocar con eso.  Por lo tanto, el ruido no te lo puedes quitar en estos satélites.  O sea, podemos hacer mil y un bilguerías para poder quitarlo,  pero es muy complicado.  Es por eso que la mayoría de los astrólogos se quejan mucho de estos satélites  porque están colocando muchísimo más ruido de lo que se debería.  Otro ejemplo es el proceso de imágenes de teletección.  Bueno, el ruido desplega...  Bueno, a ver, el SAR es un radar de apertura sintética,  y para este contexto no vendría mucho al caso,  pero bueno, puede ser.  Y el salpimienta que lo he comentado.  Vale, bueno, seguimos, ¿sí?  Podrán interrumpir y podemos ir debatiendo, ¿vale?  Nada como un vigil bien apuntado para solucionarnos.  Yo diría mejor que su vida útil se reduzca, por lo menos a la mitad.  ¿Vale?  Seguimos, tipo de ruido.  También tenemos un ruido interno al sistema, ¿vale?  Ruido generado por dispositivos o la circutería o la electrónica que está dentro.  Por ejemplo, varios de ustedes,  a veces cuando sacan una foto con los móviles,  se han dado cuenta ya...  Yo qué sé, cuando su móvil tiene más de 10 años, por ejemplo,  pues por el uso de la circutería, por los sistemas COTS que tiene de cámara,  los sensores y demás, pues va generando un desgaste.  Y el genero de desgaste va teniendo ese ruido.  La vibración, por ejemplo, o el calentamiento de las baterías  hace que se genere un ruido térmico también, ¿vale?  Y produce una aleatoriedad a la hora de adquirir las imágenes o las señales,  que eso arruina las cámaras, ¿vale?  Y por otro lado, también existe, por ejemplo, el parpadeo o el flicker,  que es un origen no está claro,  y se caracteriza por una caída en potencia en función de la fervencia.  ¿Vale?  Edison.  Y las imágenes obtenidas por equipos de misiones ionizantes.  Eso es otra cosa.  A eso se debe el movimiento de iones y demás.  Ahí se produce un tipo de error,  pero eso es más que todo por la vibración externa del...  En este caso sería interno, ¿vale?  Vibraciones internas producido por estos iones  a la hora de capturar las imágenes, ¿vale?  Es un poco de lo que más o menos recuerdo  en esta parte de iones antes que se ha visto en el doctorado.  Y la publicidad humana.  Claro que es ruido.  Si lo vemos por ese contexto, es ruido, pero es un ruido que gusta.  ¿Vale?  Es que en un mundo ideal, Ricardo,  sin ruido no existiría.  Es mi punto de vista. ¿Por qué?  Porque ese ruido viene a ser algo aleatorio,  que no sabemos cómo controlarlo, pero está presente.  Pero no sirve, ¿sabes?  Para que te hagas una idea.  Vamos a hablar de ruido blanco dentro de un momento.  Es lo que tranquiliza a los bebés.  Y lo digo con experiencia.  Mi hija se dormía escuchando ruido blanco.  O sea que para mí ha sido lo mejor.  Bueno, seguimos, ¿vale?  Hablamos del ruido térmico, ¿vale?  El ruido térmico es uno de los más comunes  producidos por sistemas de radiofrecuencia,  que yo les he dicho, por la parte de calor y demás.  Se generan internamente los componentes  en los sensores.  Es por el uso, ¿vale?  También se lo conoce como el ruido de Johnson on Nyquist, ¿vale?  Y se modela por la ecuación.  Quedarse con la ecuación, ¿vale?  Describiéndola por S.P.N. en la potencia de ruido,  K sub V constante, Bosman,  la temperatura K y V el ancho de banda, ¿vale?  Quedarse con las ecuaciones.  También tenemos a nuestro querido ruido blanco, ¿vale?  Es un proceso aleatorio,  es una distribución de probabilidad con media cero  y una varianza finita, ¿vale?  Este ruido está presente en todos los sistemas electrónicos,  ¿vale? Y también en el ruido ambiental,  el ruido blanco en las televisiones.  Si hay alguno de ustedes que tiene más de 40 años como yo,  pues se le va a acordar que  cuando encendía la televisión no aparecería directamente  Netflix o YouTube o lo que sea.  Había que sintonizar los canales y si el canal estaba caído  había un ruido en la pantalla de nieve y demás,  ese ruido blanco estaba ahí presente en cada una de las casas, ¿vale?  Buenas, Jorge, ¿qué tal?  También hay que mencionar lo que sería  el carácter úbico del ruido gausiano,  hay que tenerlo presente, ¿vale?  Este ruido neto generalmente es observado,  consiste en suma de pequeñas contribuciones aleatorias  independientes que vienen de varias sumas,  o sea, varios ruidos van contribuyendo a eso, ¿sí?  Estas variables aleatorias independientes pues tienen  una media y una varianza finitas, es decir, que es supermedible,  y es el terapéutico del límite central que define  la suma de las gausianas, ¿vale?  Por ejemplo, aquí tenemos más o menos un poco lo que estoy hablando,  distintas fuentes que van a tanto el ruido  que se va a sumar, y sumando todas las fuentes de ruido  generamos un gausiano.  Los tubos radios católicos y las teléfonas antiguas.  Juan, dicen que la lluvia de nieve de la tele  eran ruidos estelares.  Si fuese así, como tú dices,  debería haber una casi sincronicidad  o una repetición, bueno, no debería ser tan aperiódico,  por lo que tú dices, Juan, sobre todo,  en los momentos que hubiesen las tormentas solares y demás,  deberíamos tener, o se debería haber visualizado  mayor ruido blanco, o esa lluvia, esa nieve en los televisores,  que no es así, y al día de hoy sé que en Latinoamérica  hay todavía televisores que pueden capturar todo,  o que generan ese ruido blanco, ¿vale?  Y Alid, si queréis, hago la prueba, bueno,  luego después, y Alid, te pediría, cuando termine, me lo pases,  y los últimos cinco minutos hacemos la prueba de quitarlo, ¿vale?  Porfa, que si no, nos...  cambiamos de tema muy rápido.  También existe el ruido rosa, o el flicker noise,  a mí me gusta más llamarlo rosa, que es uno partido por R, ¿vale?  Es un proceso aleatorio que se caracteriza por la densidad espectral  inversamente proporcional a la frecuencia, es el uno partido por R,  que digo, se llama rosa porque ocurre  en varios sistemas físicos y biológicos.  ¿Por qué biológicos? Porque el ser humano también emite  ese tipo de ruido, ¿vale?  Emite esos ruidos.  Cuando se van capturando imágenes electrocardiográficas  o del enzafalograma, está presente ese ruido.  La actividad neuronal es la que produce eso,  el movimiento de las células, de los glóbulos,  rojos, blancos y demás, es el que producen.  Incluso hay estudios de health care  donde se analizaba  incluso la digestión humana  y se capturaba un tipo de ruido,  no era más que el ruido de rosa,  que era por el proceso de actuación de todos los ácidos del estómago  a la hora que se estaban dirigiendo las cosas  después de una comida en un ser humano, ¿vale?  Eh, José...  Sí, José, a ver...  Siempre digo, no puedo abarcar todo el tema 4 presente.  Trato de quedarme con lo más central,  lo más importante.  Y también suelo colocar algunos vídeos de ayuda  donde podemos ver todo esto,  donde pueden ustedes ampliar sus conocimientos.  De todas formas, José, recuerda,  un Tfm no es como la universidad,  un máster no es como la universidad,  que te lo tengo que dar todo comidito para que tú lo estudies de pie.\n",
      "Intervalo 20-30 minutos:  Se tiene que hacer eso, pero ustedes tienen  que ampliar también un poco vuestros conocimientos.  Por eso también hay una biblioteca,  hay una cantidad de bibliografía que se les da.  Entra en el examen de estudio.  A ver, normalmente lo que solemos hacer nosotros  es colocar lo que vamos explicando.  Y si queremos que haya algo más presente, pues también entra.  Ten en cuenta que para el examen vas a meter tanto esa documentación  como lo que hacemos en clase, las actividades de clase.  Puedes meter toda esa documentación al examen, ¿vale?  Eh, pues sí, se dé material.  ¿Hola, Ketik, cómo estás?  ¿Sí? ¿Quién habla?  ¿Quién habla?  ¿Alguien preguntaba algo?  No, profesor, a alguien se le abrió el micrófono por accidente.  Ah, vale, vale, vale.  Que justo tengo ruido lumínico en la otra pantalla  y de las tres, pues justo no he visto a quién se le ha activado.  Pero bueno, ruido humano, muy bien.  Antes de seguir con el ruido, hice otro ejemplo de ruido  en teledetección óptica de las variaciones atmosféricas.  También se introduce en ruido cuando las imágenes son captadas  por el sensor perfecto, Jairo, mini punto para ti.  También tenemos distorsión de rail, perfecto, las nubes  y generan ruido óptico en el sensor óptico.  Sí, pero tienes que tener en cuenta que si también  a ese ruido óptico le colocas la lente  para la adquisición y demás, si fuese óptico,  más que de sensor electrónico, por el teorema de Nyquist,  no Nyquist, Nyquist, también tienes un error  que ahora mismo no me acuerdo, pero es que se define  más que todo por el arco que vas a tener en la lente.  No recuerdo muy bien, pero los tiros van por ahí.  Lo digo porque todo eso se ve en el doctorado.  La resonancia de fondo se considera ruido  a tres grados Kelvin.  Ahí sí que me has pillado, Diego, no lo sé, no lo sé.  No sé, mi conocimiento no es tan amplio.  Pero bueno, puedes investigarlo y lo ponemos en el foro.  Bueno, esto sería más o menos una relación  de noise frequency  para definir el ruido rosa, el ruido termal  y demás cosas.  Ahora, lo que decíamos, en la parte  de ruidos, de ruidos de imágenes,  estamos hablando de lo que yo dije de la luz  y de la luz que se ve en el foro.  En el foro, en el foro,  en el foro, en el foro,  estamos hablando de lo que yo dije del ruido salpimienta  que lo tenemos en algún lado, por aquí.  Aquí tenemos el salpimienta, este es el que me gusta.  Que está presente  por las pequeñas presencias  como el ruido blanco dentro de una imagen.  Entonces, son estas características  que van a ir influyendo más o menos  en una imagen.  Ahora, mediréis, pero claro, podemos quitar el ruido,  es más, podemos utilizar filtros,  variaciones, interpolaciones y más.  Pero teniendo en cuenta todo esto,  igualmente el ruido, lo único que va a hacer  es atenuarse un poco y va a estar  todavía presente pero en el fondo.  Disculpe, profesor, me entro...  La diferencia entre el salpimienta y un gausiano  es la distribución.  Digamos que el gausiano es un poco más casi...  Lo quiero ver así, casi centrado.  O sea, el salpimienta  es más aleatorio que el gausiano.  La figura 4 es como si tuviera  radiación. ¿Por qué radiación?  Ahí ya no lo he pegado.  Bueno, seguimos.  Impacto del ruido en el sistema.  ¿En qué nos afecta el ruido?  Necesitamos tener una medida  o algo que nos sirva para ver  esa relación del ruido con la señal que tenemos.  Ahí surge la relación señal al ruido o el SNR.  Ya dice, tú aquí eres la...  Si quieres interrumpir, puedes hacerlo.  Esta SNR nos va a indicar  la forma cuantitativa  de la calidad de la imagen.  Se calcula como el coeficiente entre el potencial  del señal PS y la estimada del ruido PN.  Y se miden de siberios.  Esta relación señal al ruido es muy importante  sobre todo en la parte de imágenes médicas.  A la hora de hacer detecciones, ya sea con imágenes  con equipos de ultrasonido o imágenes PET o TAC  y demás, esta SNR nos va a servir  para quitar el ruido del fondo  por presencia de los sensores, etc.  Y nos va a sacar a la luz cualquier  agrupación  que no sea normal de células del cuerpo.  Para que se vea una idea,  la detección de cáncer con equipos de ultrasonido  se hace mejorando la relación señal al ruido  de la imagen o de los sensores.  El ruido por ser aleatorio  puede ser usado para decryptar información.  Eso es una parte de siberio que prefiero no tocar.  Lo siento Ale, pero tampoco me lo sé.  Pero no estaría desenfocado.  Lo que pasa es que tendrías que tener esa aleatoriedad  para volver a desencriptar.  Y si no la tienes bien medida,  es más complicado.  Volviendo al impacto del ruido,  hemos dicho que es el logaritmo  del PS de la señal sobre la del ruido.  A partir de eso,  tenemos más o menos una pirámide o un reloj de arena  de las relaciones que tendríamos desde 0 decibelios  hasta 100 decibelios.  El equipo de contomografía  produce armónicos por sus componentes electrónicos.  Por ende, en ocasiones aparecen artefactos.  Esto puede ser considerado un ruido.  Cuando hablamos de artefactos, hablamos de interrupciones,  hablamos de ruidos. Se podría decir que es lo mismo.  Se podría decir. No estoy diciendo que lo sea.  Seguimos con el ruido.  A una señal A, por ejemplo,  que está aquí todo mostrado,  que la he mostrado bien,  la estoy colocando a sus valores de forma cualitativa,  cualitativa, como me quieran llamarlo, que es analógica.  Después de hacer esta discretización y demás,  remeto un ruido a mi sistema. Por lo que sea, entra el ruido.  Entonces, dependiendo  de cómo analizo el SNR,  puedo obtener, con un SNR  mayor, me va a servir para poder identificar  la forma original que tenía  mi señal de entrada.  Mientras menos sea la relación SNR,  más ruido voy a tener. Por eso es lo que les decía  que estos valores,  o sea, que en la parte de imagen  me viene mejor saber esta relación SNR.  Mientras más alta sea el ruido.  Edwin, por tercera vez lo estoy mencionando.  No sabemos quitarlo, lo vamos a hacer  en los últimos cinco minutos.  Y es un ejemplo fehaciente de que estás viendo  un ruido presente en la pantalla.  Vamos con la parte  de procesos estocásticos.  Hay que tener en cuenta que se van a  emplear en escenarios reales, donde el valor de la señal  incluye la contribución de un término aleatorio  o de ruido. Por lo tanto, estas señales  serán aleatorias.  La medición de una señal no siempre proporcionará  el mismo resultado debido a su naturaleza  del ruido.  Por lo tanto, nuestras señales serán modeladas como procesos estocásticos.  No tendrá un proceso  exacto sino estocástico.  La relación de la que habíamos definido antes  es esta.  Hay que tener en cuenta  las reglas que se asignan a cada resultado.  La SFP, por ejemplo, de una función  depende de un determinado número de variables  como el espacio muestral, el conjunto de sucesos  y la ley de probabilidad.  Lo que sería la similitud con variables aleatorias  es que, por ejemplo, el hecho de lanzar un dado  y que vaya colocando un determinado  valor a una señal original.  Voy a ir colocando el ruido.  Siguiendo con esta parte  de procesos estocásticos, hay que tener en cuenta  que es una sucesión de las variables aleatorias que se van a ir creando.  Por ejemplo, la señal  XDTA con los parámetros libres  de este proceso, donde  es el resultado concreto de un experimento aleatorio,  es decir, de las primeras variables libres  y la segunda fija, y la función determinista del tiempo.  Con todo esto,  si hacemos variable en el tiempo,  tenemos que nuestra señal aleatoria,  nuestro ruido,  va a tener más o menos una respuesta  a estas características.  Este proceso estocástico estaría presente.  O sea, sería generado por el ruido.  ¿Cómo podemos caracterizar\n",
      "Intervalo 30-40 minutos:  estadísticamente?  En función de su distribución  y en función de la densidad de probabilidad.  Equaciones que están ahí,  ecuaciones que las podemos implementar.  En la carrera no nos vamos a detener mucho tiempo.  Pero lo que sí me interesa  es ver esta parte de estacionariedad  o estacionalidad, como quieran llamarlo.  Algunos con R y otros con L.  En el sentido estricto, la función de densidad de probabilidad  se caracteriza porque es un proceso que no varía en el tiempo.  Es decir, está estacionada.  Y en un sentido amplio, los momentos estadísticos,  la variancia, la covariancia, la moda, etc.  se caracterizan porque el proceso  tampoco va a variar en el tiempo.  Entonces, ¿qué sucede?  Por ejemplo, en esta señal,  imagínense que es una señal  de la voz humana cuando estoy hablando por teléfono,  se van a producir una serie de valores  o de presencia de ruido  estadísticamente medibles  por la singularidad  que tienen los dispositivos electrónicos  a la hora de discretizar la voz analógica  en digital y después transformarla  para transmitirla por el aire.  Llámese telefonía móvil normal  o por datos por Internet.  Entonces, nos quedaremos con esta parte de estacionariedad.  Esto es más que todo por tener un conocimiento del tema.  No digo que esto sea una teoría pura  que tenemos que aprender en el examen.  No, es para tener en cuenta que dentro de los procesos estocásticos,  pues a la hora de transmitir  datos, ya sea imágenes o ya sea audio,  siempre va a producir unos valores  muy deterministas a la hora  de poder hacer cálculos.  Así como hemos dicho que el SNR,  la relación señal-ruido, es muy importante,  pues a la hora de transmitir los datos,  el hecho de tener un vouch rate  o una tasa de transmisión más limpia  con menor tasa de ruido, teniendo en cuenta  paridad par o impar  para transmitir los videos de datos,  pues es ese checksum  que me va a producir la transmisión,  pues va a estar influido por estos procesos estocásticos.  Sé que suena un poco complicado,  pero quedarse solamente con ese concepto.  No les digo que sea muy importante,  pero cuando vayan a analizar en algún proyecto  de transmisión de imágenes o de audio  a velocidades altas,  hay que tener en cuenta que siempre hay que tener procesos de checksum,  no es más que procesos estocásticos  para tener un control de que no haya habido variaciones  en la transmisión, pues están presentes.  Solamente por eso.  Ahora, vamos a entrar a la entropía.  Transmisión-recepción, José.  Vamos a entrar a la entropía.  La clase pasada creo que lo preguntaron, vamos a revisarlo.  Es algo bonito, pero como yo siempre digo,  es importante en algunos de los casos  sobre todo para analizar imágenes  y demás cosas.  Bueno, no imágenes, señales.  ¿Qué dice señales? Dice audio o video.  ¿Cómo la definimos? Es la cantidad de información  en promedio de una fuente aleatoria.  Cuanto más probable es el suceso,  menor cantidad de información se aporta.  La necesidad de cuantificar la cantidad de información  en un mensaje y entender cómo se transmite de manera eficiente  a través de un canal de comunicación  se produciría en lo que sería la entropía de Shannon,  de HDX, como un concepto utilizado  para medir la incertidumbre o la aleatoriedad  de este suceso. Y se define  como la reforma negativa o la minimización  de la sumatoria de X  para todo punto P  o para todo proceso OOP,  por el logaritmo en base 2 de PX.  Esta ecuación es muy importante y siempre les  recomiendo a todos que se la queden.  ¿Por qué? Porque normalmente esto nos va a ayudar,  como hemos dicho hace un momento, nos va a ayudar a poder medir  la cantidad de información que va a aportar,  que vamos a poder aportar.  ¿Sí? ¿Quién estaba hablando? ¿Edwin?  Creo que te he visto del refilón.  ¿No?  Interrumpirme cuando queráis.  Por ejemplo...  Perdón.  El ejemplo de Bernoulli, por ejemplo,  para una variable aleatoria discreta X,  es decir, en cada uno de los puntos que voy recorriendo esta  variable aleatoria,  con N posibles valores,  la entropía máxima se logra cuando el punto máximo  tiene las características de Shannon.  Está definido por la ecuación, es la misma que hemos puesto anteriormente,  y lo único que se ha ido haciendo es quitar la sumatoria  y...  ¿Cómo lo digo? Estirar o componerla,  componer la sumatoria y sacar la función indicada.  La entropía en imágenes SAR también es una métrica  que usamos principalmente en análisis polimétrico SAR.  Perdón.  Poliarimétrico SAR, para entender la complejidad  y la diversidad de los mecanismos de dispersión de una variable  aleatoria zebrada.  Exacto. Y cuando utilizas el SAR  y te lo llevas al SAR, que es ultrasonido,  el resultado es lo mismo.  Entonces, no solamente para imágenes SAR, sino para imágenes numéricas.  También utilizas las métricas.  Sobre todo para poder discernir distintas texturas,  si no me equivoco.  En la parte de Helker te lo digo.  A lo mejor tú nos puedes dar algún matiz  en la parte de imagen AR.  Si quieres hablar, ahora es momento.  ¿Jairo, quieres decir algo?  No.  Bueno.  Pues sí, que sepa que lo que puso Jairo  es completamente cierto. Pero también Helker  en la parte médica se lo hace por la parte SAR.  Bueno, no pasa nada.  Si quieres, lo puedes mencionar en el foro. Te invitaría que lo pongas.  Y si puedes colocar algún ejemplo, te lo agradecería.  Es muy importante para que tus compañeros también lo tengan.  Y cuando tengas ese ejemplo, yo pongo la versión en Helker.  Porque también se utiliza bastante en SAR.  ¿Vale?  Seguimos.  Con todo esto, pues podemos ver la ganancia de la información  y la codificación. ¿Por qué? Porque la entropía también está relacionada  con una ganancia de información. Es decir,  con una disminución. Es decir, que una disminución en la entropía  implicará ganar información. ¿Se acuerdan lo que dijimos  de las relaciones señales de ruido? Pues más o menos  van relacionados los tiros. ¿Vale?  En aplicaciones con árboles de decisión, por ejemplo,  la entropía se puede utilizar para medir la eficacia de la división.  ¿Vale? ¿Hacia qué lado voy a tirar?  Y en teoría de codificación, pues la entropía es más baja.  O cuando la entropía es baja, eso implica que la codificación  es más eficiente. Para el que me dedico la parte de  encriptación y demás, la entropía estaría metida a la hora de hacer esa  encriptación. ¿Vale?  Por otro lado, en la parte de espectral,  de entropía espectral  de una señal, se puede definir como una medida  de la distribución de su potencia espectral. ¿Vale?  Y también es útil en la extracción de características  en el área como la parte  diagnóstico médico. Es, por ejemplo, para detectar fallas  en el corazón humano, en los latidos,  o en el reconocimiento de los... ¿Vale?  Todo esto en la parte de Helcet.  La parte de la entropía espectral, pues, imagínense,  para una señal XDR que ya la hemos visto,  con una espectra potencia es W,  y teniendo en cuenta la transformada discreta de Fourier,  pues su valor sería de PW,  de la frecuencia de S sobre la sumatoria  de todo ese sup I. ¿Vale?  Entonces, la parte espectral de la entropía  estaría relacionada por esta ocasión. ¿Vale?  Ahora, seguramente me van a decir,  y voy a utilizar la entropía en el examen,  cuál entraría en el examen, ahora,  me diréis, pero si no puedo sacar  logaritmos de base 2, ¿por qué no puedo usar calculadoras?  Pues también no se van a usar calculadoras,  pero si ustedes me describen la ecuación de la entropía  y van reemplazando cada uno de los valores  por lo que está en el enunciado y me lo colocan  de forma indicada, pues para mí es una respuesta correcta.  ¿Vale? ¿Por qué pasar eso a una calculadora?  Es más, es muy sencillo, pero entender cómo voy a utilizar  los valores en la ecuación, pues eso es lo que nos interesa a nosotros.  A ver, y Hiro dice...  Hiro dice, un ejemplo de aplicación podría ser  el monitoreo de cambios ambientales en la entropía.  Puede utilizarse para monitorear cambios de tiempo. Bueno, sí,  es lo mismo que decía, pero llevada a parte de Jerker.  Bueno, con todo esto, quedarse con las ecuaciones  y este vídeo que les voy a pasar,\n",
      "Intervalo 40-50 minutos:  que lo colocaré hoy o mañana como máximo  en el portal,  echarles un vistazo, ¿vale? Es un vídeo muy  regulativo, a mí me gusta, ¿vale?  Si me mantengo equivocada, es de 35 minutos.  Habla muy bien sobre la entropía en la parte de información,  en la parte de transmisión y en la parte de imagen, ¿vale?  Es muy interesante que lo veáis.  ¿Qué tipo de entropía podemos tener  también? Tenemos la entropía lineal,  definida por su... por su ecuación,  la de Reini, definida por su ecuación  y por la de Stalitz.  Todos estos tipos de entropías se pueden utilizar  dependiendo del contexto que vayamos a usar o dependiendo del área.  Nosotros nos vamos a quedar con la principal,  no nos vamos a ir por las rongas. Sólo que lo sepáis  que hay todos estos otros tipos.  Vale, vamos.  Tenemos la entropía... Para calcular la entropía de una señal,  señal como proceso estocástico,  va a ser una sucesión de variables aleatorias.  Si la señal es de longitud de N,  se caracterizará por la función de densidad  y probabilidad conjunta de las señales variables aleatorias.  Y teniendo en cuenta  en señales o en series temporales,  tendrá esta ecuación.  ¿Qué dice Sartre-Sander?  La entropía, así como la aurística, sirve para medir  o determinar los patrones anómenos en la nación de información.  Claro, es lo que estamos llamando.  La entropía va a ser eso que nos va a ayudar a medir.  Vale.  Si vamos a interpretar un poco  la parte de la entropía en señales,  dependiendo del grado de regularidad de las señales,  podemos saber que a un menor nivel  de señal a ruido,  vamos a tener una entropía más baja.  Y si la relación del señal a ruido es mayor,  voy a tener más entropía.  Y por lo tanto,  en el sentido de cantidad de información,  más imprescindible, más información.  Lo que estabas comentando.  Y bueno,  la entropía también refleja  la señal, la ocurrencia de los eventos.  Por ejemplo,  estas son unas imágenes...  Uy, se me ha ido ahora mismo.  Se me ha ido ahora mismo.  Se me ha ido ahora mismo.  Eran de saturación.  Saturación en sangre.  Pero se me ha ido ahora mismo.  Entonces, un sujeto sano,  imagínense, si es saturación en sangre,  veamos el problema del COVID.  ¿Cómo puedo interpretar que un sujeto está sano  y no tiene COVID? Pues su saturación en sangre,  los valores que va a ir teniendo,  son próximos al 100%, vale.  Entonces, su variación va a ser más baja.  Por ejemplo, los pulmones sanos,  pulmones que no están encharcados,  vale. Sin embargo,  si el enfermo de COVID lo paso  por un análisis de la saturación en sangre  teniendo en cuenta que tiene ya saturación  en los pulmones, perdón, ambos pulmones están  encharcados, ni siquiera es uno, porque es bilateral,  son los dos pulmones, pues va a tener una mayor  saturación, y es una forma muy aleatoria,  muy cansada. Por eso es que se decía en el COVID  que si lo colocabas de costado respiraba a lo mejor.  Sí, porque tapabas uno y respiraba  con el pulmón menos afectado, y lo cambiabas  de puesto y de lado, y se intercambiaban  las cosas. Entonces, con eso se consiguió poder,  por ejemplo, salvar a las personas solamente cambiándoles  y haciendo que los pulmones trabajen de mejor forma.  ¿Cómo se dieron cuenta? Con la saturación del oxígeno en sangre.  Entonces, para eso la entropía  nos viene como a nivel de...  Esto me gustaría que se lo creen, porque esto  les puede venir hasta en las prácticas.  ¿Cómo sacamos la entropía? Pues he dado  una señal temporal de N muestras, de U hasta N,  de U sub 1 hasta U sub N,  pues en una forma de secuencia  de vectores, pues voy analizando, colocando  los valores de U hasta N,  utilizando...  cada una de las muestras,  después lo paso a los números reales, es decir,  cambiar valores, y después voy a calcular  la entropía siguiendo la ecuación que tenemos.  Este algoritmo de entrada, que lo veis aquí  bien bonito, no es más que la respuesta  a la ecuación que hemos puesto antes,  pero es una forma, digamos,  lógica a la hora de programarlo  en código.  Los que usan NAD Lab utilizan  aproximadamente entropi y mi señal.  En Python, ahora mismo  no me acuerdo la función, pero si alguno se acuerda,  que lo ponga y le damos un minipunto.  Si tengo un código de NAD Lab,  o en C, donde no tengo que utilizar  un pncv ni nada, esto me va a servir  para colocar o para calcular mi entropía.  Con esto, ¿qué quiero decirles?  Hay formas, esto sería más o menos un pseudo de código,  hay formas para poder calcular la entropía  dependiendo del proyecto que hagamos,  y dependiendo del tipo de tecnología  y nada más. De momento,  ¿alguna duda, consulta?  ¿Nada? Bueno.  Pues para la próxima clase, un poco viendo  lo que nos espera el próximo viernes,  vamos a plantear ya la pregunta, la repasaremos  la semana que viene, es ¿cómo voy a poder diseñar  un sistema de detección y cancelación de anomalías  utilizando el aprendizaje no automático?  Lo que estamos viendo ahora mismo es aprendizaje no automático.  Lo único hecho ha sido detectar el ruido,  saber cómo se presenta el ruido,  cómo poder más o menos detectarlo con la relación  señalada al ruido, ¿vale? O cómo poder  encarar cosas con él, y también cómo poder  interactuar con la entropía, qué es lo que me va a dar  la entropía, ¿vale? Entonces, la próxima clase  que vamos a ver es esa detección y esa cancelación,  pero de forma no supervisada automática, o sea,  con fórmulas, si lo queremos ver así, ¿vale? ¿Cómo se hace eso?  Confíltenos.  A ver.  A ver, a ver que, uy, han escrito bastante, madre mía.  La entropía, así como la heurística, bueno, sí,  lo que me está todo muy claro, ¿vale, Juan?  Hernán, ahí te he visto poniendo tu código en Python.  Sí, data y bases 2, ¿vale, perfecto?  Una duda, ahora que recuerdo, se comentó en clases  semanas anteriores que se iban a crear foros de consulta,  están creados los foros, ¿sí?  Solamente pasa una cosa, y no estoy seguro,  y ahora que están todos ustedes presentes,  avisadme. Por cuestiones del  master, esta clase, por eso en el primer,  la primera claspa, el primer slide,  siempre coloco, percensión computacional y visión  artificial, ¿vale? Es porque los alumnos  de percensión computacional, algunos que están en esa  materia, pues, estoy colocando el contenido en ambos lados.  En principio está puesto el foro en ambos sitios.  Confirmarme si están los foros creados.  Yo creo que sí, porque he estado respondiendo algunas  preguntas, ¿vale? Están puestos ya  los foros, ¿vale?  José Manuel Rodríguez,  escríbeme de qué, si eres de percensión computacional  o de visión artificial, ¿vale? Y lo reviso.  Xavier, que no soy Xavier, soy Javier con J, Carlos Iván.  ¿Puedes darnos algún ejemplo de cómo aplicar  un ejercicio práctico la disminución de ruido  en una señal o la entropía?  A ver.  Más que disminución de ruido me va a servir para poder  saber si lo que está aquí puesto.  Vale. Por ejemplo,  esto es una relación muy buena.  Si el cálculo de mi entropía es mayor,  es porque tengo una mayor relación, una mayor  relación, una es en relación  señal-ruido, ¿vale? Entonces tengo más entropía.  Por lo tanto, mi señal que dijimos que estaba  siendo digitalizada y estaba siendo interpretada por ruido,  pues puede tener unas mejores características, ¿vale?  El ejemplo claro, en un micrófono o  mira, el mejor ejemplo, un ecógrafo.  Al sacar una imagen de ecografía, si mi relación  señal-ruido es elevada, pues voy a tener una mayor entropía.  ¿Y cómo calculo eso? Pues primero me lo da el equipo de ecografía  y por otro lado puedo sacar la relación de ruido de fondo  a partir de la escala de grises que tiene la imagen.  Después Hernán, vale, perfecto.  Has hecho un copy paste, pero está bien.  Felipe, la calidad de información está en cierta  manera relacionada con la entropía o  bueno, sí, la entropía me daría también la calidad de información.  Todo sea por ese minipunto.  Muy bien, Hernán.  Muchas gracias. A mí no me hace la percepción, José.  Lo reviso ahora, ¿vale? Después\n",
      "Intervalo 50-60 minutos:  de terminar la clase lo reviso.  Estamos por el visible, ¿vale?  Lo reviso. En percepción lo voy a revisar. ¿En qué carrera?  Es percepción computacional. Es del máster,  solo que antes, el anterior cuatrimestre, en el pera anterior  se llamaba percepción computacional y ha cambiado de nombre.  Nada más, ¿vale? Como dicen en mi pueblo,  la misma chula con otra poliera.  Solo queda responder a usted para reforzar.  Sí, vale, perfecto. Pues si no tenéis más dudas,  preguntas, como decía Sheldon Cooper,  nada.  Pues lo dejamos ahí.  Muchísimas gracias.  Nos vemos la próxima clase y  voy a revisar ahora lo del foro en el  OPPOpera en percepción, ¿vale?  Pues nada, muchísimas gracias a todos.  Vamos a parar esto, deja de compartir.  A ver si se me da bien.  Dejamos de compartir.  Y paramos.\n"
     ]
    }
   ],
   "source": [
    "current_interval = 0\n",
    "accumulated_text = \"\"\n",
    "\n",
    "for segment in result3[\"segments\"]:\n",
    "    start = convert_to_min_sec(int(segment[\"start\"]))\n",
    "    end = convert_to_min_sec(int(segment[\"end\"]))\n",
    "    text = segment[\"text\"]\n",
    "\n",
    "    # Verificar si el segmento sigue dentro del intervalo actual de 15 minutos.\n",
    "    if start // n == current_interval:\n",
    "        accumulated_text += \" \" + text\n",
    "    else:\n",
    "        # Imprimir el texto acumulado para el intervalo actual.\n",
    "        print(f\"Intervalo {current_interval*n}-{(current_interval+1)*n} minutos: {accumulated_text}\")\n",
    "\n",
    "        # Actualizar el intervalo y reiniciar el texto acumulado.\n",
    "        current_interval = start // n\n",
    "        accumulated_text = text\n",
    "\n",
    "# No olvidar imprimir el último intervalo acumulado fuera del ciclo.\n",
    "print(f\"Intervalo {current_interval*n}-{(current_interval+1)*n} minutos: {accumulated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "result4 = model.transcribe(\"/home/contrerasnetk/Documents/Classes/VisionArtificial/4.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intervalo 0-10 minutos:   compartir pantalla y sigue saliendo  yo no sé qué es lo que pasa pero vamos  no puedo solucionar el problema y no entiendo por qué he hecho todo lo que  me han dicho  y no funciona  pero ya no se ve no creeme  lo estaba viendo justo cuando estoy antes de compartir y ya me sale  qué tal feliz viernes con todos  los raros es que les digo  es que no están activadas  un segundo  no  voy a ver si el chat gpt tenía la solución a nuestros problemas  a ver a lo ahora  para  no  vale pues  ahora yo creo que ya no salen verdad  no sigue saliendo podéis verlas sí sí se ven  a ver  y ahora no es que van a seguir saliendo ahora sale en español sí  a ver  ahí pon la pantalla que estoy compartiendo no vale darme un par de  minutos vamos a ver si si puedo quitarlo  porque me decían que ya no era el zoom que el sumo estaba desconectado pero  posiblemente la presentación  cuando estoy activando la ahora a ver para solventarlo le puedes hacer el sumo  en 100% y ahí tú lo vas modificando y se pierde a ver  ahora esta otra opción si yo creo que ya está no sí ya no sé ya está  pues es un problema del office 365 con la configuración que tenía mal y miren  que lo había repasado y después también había revisado lo que me dijeron del  sumo el sumo si también te da la traducción pero bueno  bueno muchísimas gracias por la espera par de minutos  vale ya sabéis cualquier pregunta me podéis hacer podéis utilizar el  micrófono o escribirme por el chat lo que queráis  vamos a comenzar con este tema 5 que la detección y cancelación de anomalías  de la materia de visión artificial o percepción computación a  que no ha puesto el puntero láser ahora sí  a ver si se acuerdan en la clase anterior habíamos visto esos ruidos esas  anomalías que existían que aparecían vale cómo no las podía  clasificar por así decirlo e incluso como gracias a la entropía pues podía  saber si a mayor entropía pues la señal tenía más cambios o menor  entropía la señal tenía menos cambios por ejemplo en el caso de lo que vemos  aquí unas señales de encefalograma en las cuales es a menor entropía pues hay  menos cambios entonces el paciente es sano y un paciente que tiene algún  disease alguna enfermedad pues tiene una mayor entropía porque tiene una  mayor cantidad de cambios no entonces teniendo en cuenta lo que habíamos  visto antes nos vamos a plantear la siguiente pregunta o el siguiente  problema que viene a ser cómo podría diseñar yo un sistema de detección y  cancelación de anomalías usando aprendizaje no supervisado varios de  ustedes me diréis vale utiliza una reina uraná y no sé qué decir no sé cómo  vale pero eso es supervisado es una reina uraná entonces tengo que ver cómo  podría hacerlo utilizando una red que que no sea supervisada vale disculpadme  pero ando un poco con la alergia así que me oiréis a lo mejor un poco ronco  también entonces para abordar este problema  qué es lo que vamos a hacer vamos a ver un poco la introducción los objetivos  los tipos de anomalías que hay los métodos de identificación y la  eliminación de las mismas como introducción pues  si ustedes ven la imagen pueden decir ahí claramente  qué anomalía existe en medio de un rebaño de vejas blancas tenemos una  veja negra la anomalía también llamada olier pues es un patrón inusual que no  se ajusta al comportamiento esperado no es es algo poco probable si el origen de  esto pues ruido es algo inestable a la hora de hacer la captura pues algo que  directamente está variar variando en el sistema  por ejemplo aquí tenemos una señal de audio vale una persona cantando por  ejemplo donde tenemos claramente unos picos que uno me diréis bueno si está  cantado pues pueden ser los gallos que tiene pero también puede ser aquellos  pequeños acoples que existe en el micrófono y generan esos pitidos a la  hora de la captura del señal no exactamente si bellos se dan muy bien  entonces existen esas anomalías están presentes es el ruido  por ejemplo este este imagen alguien me puede decir qué tipo de ruido es que  siempre lo digo que es el que más me gusta este tipo de ruido aparece mucho  ahí está muy bien el sal pimiento este ruido aparece mucho sobre todo en las  cámaras antiguas vale porque es una anomalía que está presente por el  desgaste de los chips de los sensores en la parte electrónica en la parte  digital pues es muy poco probable que salga este sal pimiento precisamente por  la textura de la digitalización de la imagen pero en la parte digital pues sí  con bastante frecuencia la sal pimiento yo tengo una duda dígame  viendo esta imagen como de sal pimiento disculpe que más adelante yo he  escuchado sobre técnicas que le llaman data a un mentención donde modifican de  cierta manera la imagen y a veces he visto imágenes parecidas a este tema de  sal pimienta que aún así sirven para entrenar  entonces es bueno o es malo en cuenta ten en cuenta que el dato aumentación como  su palabra en inglés lo dice el lado literal que aumentando los datos si  tengo pocos datos para alimentar a mi red neuronal pues puedo colocarle ruido  vale para que la imagen sea completamente distinta a la hora de  aprendizaje de pasarle por el aprendizaje vale incluso puedo hacer un  dato aumentación de en vez de que la imagen se vea nítida pues se vea como si  hubiera sacado la imagen en plena lluvia es decir con un ruido que parezca una  lluvia o que parezca un difuminado por una presencia de nubes es decir que  tenga niebla vale entonces para eso sirve el dato aumentación y la sal  pimienta es el ruido que voy a ir colocando en este caso con este tipo de  imagen para aumentar mi imagen y mirar este tipo de imágenes y sea  completamente distinta a la hora de que la red neuronal aprenda  voy a colocar ese punto diferenciador que hace que dos imágenes no sean iguales  para el ojo humano obviamente si coloco esta imagen y una sin el ruido de sal  pimienta va a ser la misma sólo que una va a tener ruido pero eso es porque  nosotros ya lo hemos aprendido pero una red neuronal que tiene que identificar  por ejemplo si es una rosa o es una margarita el hecho de que tenga el  ruido de la sal pimienta pues le va a servir para poder discernir también  entre una rosa y una margarita voy a aumentar el número de editas  dice luis se puede usar técnicas de interpolación que rellenen los píxeles  con ruidos basados en valores cercanos ae luis te estás adelantando pero bien  bien eso es lo que me gusta de usted este grupo participa mucho y tiene las  cosas claras muy bien luis minipunto para ti  bueno seguimos vale los escenarios pues  estos escenarios de la somalía de las anomalías pues está el preprocesado de  señal donde voy a identificar y eliminar estos elementos es purios vale estos  estos errores que hay entonces estos patrones inusuales que están presentes  en todo el mundo o sea en toda nuestra naturaleza nuestro environment que  tenemos actualmente pues están presentes en una red por movimientos  inusuales del sistema vale en el diagnóstico médico a la hora de detectar  las imágenes pues están presentes por la misma electrónica el movimiento de la  persona yo que sé la presencia de algún metal  imagínese una la protés y de cadera pues puede producir a lo mejor algún  robot en alguna señal en algún momento dado y demás incluso en el momento de\n",
      "Intervalo 10-20 minutos:  las de hacer transacciones bancarias si cualquier ruido procedente en la  transmisión de datos puede producir todas estas anomalías no o incluso la  fuga de clientes en una empresa de telecomunicaciones es un ruido es algo  anómalo no o sea  porque es algo no esperado  con todo esto vamos a ver los objetivos pero antes que dice yo la red neuronal  puede estar en condición de emitir el ruido o separarlo para hacer  identificación si tú le entrenas a que lo filtré y después te identifique pues  te va a hacer las dos cosas si no pues solamente identificar lo  tenga o no tenga ruido  los objetivos como hemos dicho vamos a definir la anomalía en el contexto de  el tratamiento de información lo que nos interesa por otro lado conocer las  diferentes tipos de anomalías vale estudiar posibles métodos de  identificación y finalmente comprender y aplicar estos métodos específicos para  la eliminación de estas anomalías vale es decir utilizar filtros  vale que desde cuando la palabra filtros que es muy importante  tipos de anomalías pues existen distintos desde el rango de la señal  que está afectando la dependencia o no de los valores de la señal la  naturaleza en la cual nos está viniendo como puede ser un ruido algo generado por  el hombre es decir incluso el ruido de un motor que el propio ser humano lo está  produciendo pues genera ese ruido o incluso cualquier otro tipo de métodos  que se puedan tener una identificación un poco más clara todo depende del  entrenamiento de la red neuronal claro a ver hemos dicho que aquí no vamos a  ver las redes neuronales vale entrando la pregunta de alex todo depende de la  reina o no al claro del tipo de entrenamiento si yo quiero entrenar un  y una red neuronal que me filte la imagen si tiene ruido pues primero tendré  que tener una reina o no que detecte el ruido y la filtr  y después que detecte las cosas no los puedo colocar en cadena puedo crear una  que me haga las dos cosas y demás eso va a depender de del proyecto que yo  quiero usar normalmente esto entre reparentes la industria  para hacer proyectos y demás es decir lejos de limas de no me voy a  dedicar a hacer toda una red neuronal desde las bases utilizaría modelos  preexistentes preentendados y juntarlos como si fuese como si fuesen las piezas  de un rastis de un ropa de cabeza así demás y generar el resultado que yo  quiero que puedo tardar más o menos en todo el entrenamiento y demás eso va  a depender un poco ya del tiempo o del budget el tiempo y presupuesto que tenga  el proyecto pero lo ideal es ya utilizar modelos preentendados  para una red neuronal un auto encoder serviría claro un auto encoder te  serviría bueno seguimos con esto vale entonces  estos serían los tipos de anomalías teníamos una anomalía puntual vale que  son comparables con valores atípicos los auliers vale los como vemos aquí en  una señal que tengo los en noidad pues si bien tengo el ruido por todo el lado  pequeñas anomalías pues tengo algún pico de señal que me genere ese aulier  vale y por otro lado los valores atípicos pueden ser puntos que se  esperan que estén presentes o no en el conjunto de datos lo que serían los  errores aleatorios inevitables que podríamos asemejarlos a estos errores  que están caracterizando la forma de mi señal por su noidad vale y por otro  lado pues el valor de la señal se aleja excesivamente del resto de los valores  de una señal normal por ejemplo a la hora de tener valores en la detección  por ejemplo la detención de una persona pues sé que hay unos rangos en los cuales  una persona sana o estable se encuentra y aquellos valores que están alejados  pues vendrían a ser esos valores atípicos  entonces tendría valores en este caso sería unas anomalías que serían  puntuales por otro lado la contextual vale lo que me indica es que va a tener  una anomalía en un contexto determinado es decir en unas características  determinadas vale pero no sin este contexto es decir si no están dentro de  estado de relación pues no funciona o no estaría  detectándose quien dice contexto dice a ver señal para que se hagan poco la idea  vale por ejemplo aquí podemos ver la temperatura presente a lo largo de un  año si de marzo a marzo si lo quiero decir así aquí en europa es decir la  parte norte del planeta donde los meses de verano que más o menos junio hasta  septiembre son los más altos vale sin embargo podemos ver que la temperatura  por ejemplo presente principios del año comparadas con mitad del año  pues tiene unos valores idénticos podemos decir que te unes igual que te  dos vale se puede decir que es una anomalía pero como ocurre en un contexto  diferente pues no se consideraría no se consideraría como tal  vale ahora si fuese en el contexto propiamente de los meses de verano pues  sí ahí sí se consideraría al hablar de a ver qué dice seduin al hablar de  señales en este caso continuas sus frecuencias y valores picos no logras  ser la correspondencia en la imagen son matrices de anterior de nm por tres como  no te entiendo  y si quieres hablar habilita el micrófono te invito porque no no no  entiendo tu pregunta  y  buenos días qué tal el incómodo a saber cuéntanos si es que yo veo que los  ejemplos son con señales continuas pero estamos en visión computacional de  imágenes entonces no veo si las imágenes de realidad son matrices en  esas matrices cómo se encuentran las anomalías vamos a llegar ahí porque  estoy porque está viendo señales en el tiempo y en una dimensión porque  primero antes de correr tienes que caminar entonces antes de dos  dimensiones veamos una dimensión vale no te preocupes y por otro lado las  anomalías presentes por ejemplo  aquí lo que tú estás viendo son ruidos que están presentes en una matriz de n  por n por 3 por rgb si lo quieres ver así vale entonces si logramos almacenar  toda esa información o analizar perdón esa información vamos a tener por  ejemplo el color rojo en esta zona pues de la imagen n por m  el valor de 3 la tercera dimensión pues tendrá unos colores rojos imagínate 50  que son aceptables pero esos puntos sal pimienta pues tendrá un valor que ya no  es aceptable si es un valor de 10 el sal pimienta tendrá un valor de 90 por  ejemplo porque sal pimienta es negro blanco entonces tenemos esas anomalías  vale entonces lo vas detectando con eso si los ejemplos que pongo aquí en una  dimensión es para que tengan presentes cómo sería en una dimensión en dos  dimensiones es extrapolar si es extrapolar lo que por ejemplo ahora una  anomalía colectiva vale es una colección de instancias de datos  relacionados que es anómalo respecto a un conjunto de datos vale en este caso  estamos viendo una señal sg vale donde la falta de captura de datos en un  periodo de tiempo vale hace que sea un anormal que se presenta una anomalía  vale si lo quieres ver así estoy capturando vídeo y en mitad del la mitad  de los vídeos sabes que un vídeo no es más que una sucesión de fotogramas  llama lo 25 frames por segundo pues de los 25 frames 3 me aparecen en negro y  cuando estoy transmitiendo el vídeo pues lo que dure esos tres frames pues voy a  tener como un pequeño flash entre medias ya sea negro o sea blanco que me va a  producir esa anomalía a la hora de analizarlo o pasarlo a una red neuronal o analizarlo  estadísticamente pasándolo por filtros pues voy a detectar eso en el con cuando  haga el análisis del conjunto vale  con todo esto los tipos de anomalías pues tenemos el ruido que puede tener un  patrón fijo aleatorio de bandas vale tener las anomalías por artefactos de  captura que pueden ser internos ya sea aliasing que lo hemos visto la anterior  semana o hace dos semanas si me equivoco, soft de fact o de monitoring o una baja  resolución de mi sistema o de mi sensor un desenfoque decidía problemas en la  óptica distorsiones y avergüecciones que pueden ser producidas ya sea por  efectos luminosos o por efectos mecánicos vale externos obviamente en mi  sistema o a mi cámara si lo quiero ver así y también la parte de artefactos  externos como son movimiento del color y la sobre o su exposición vale lo que  tendría ser el hizo si lo quiero ver así si vemos el ruido  pues tenemos un ruido que tenga un patrón fijo aquí si podéis ver pues  hay una variación de colores claramente más o menos apreciable de ruido el  aleatorio que es completamente ya un movimiento sin usual vale y el de bandas\n",
      "Intervalo 20-30 minutos:  vale como podemos ver que es como una concentración llaman los líneas que se  va presentando en una imagen en la parte de artefactos de captura o los  luego lo que sería el ruido interno las maneras internas pues tenemos lo que  sería el aliás y el efecto de la línea así y los artefactos de moirey vale  como me doy cuenta pues si podéis apreciar bien esta imagen que tiene una  una principio para una resolución menor y aparte por efectos de distorsión y  demás pues podemos ver que no se aprecia claramente las ventanas y  presenta este tipo de comportamiento ahora la captura vale después en la  parte de los artefactos también internos pues tenemos la baja resolución  si el hecho de que nuestra resolución sea muy pobre hace que tenga mayores  ruidos aquí podemos ver una imagen de 4x4 de la imagen está del aplie me de  rogues de los 70 no me acuerdo ahora mismo el año pues la resolución al ser  muy baja va a tener más anomalías que cuando tiene mayor  mayor resolución igualmente aquí se puede apreciar un lado tenemos una  resolución más elevada y el otro lado más baja y por lo tanto pues existe más  anomalías  en la parte de ese enfoque tres cuartos de lo mismo es más ahora mismo ustedes  me están viendo el rostro de una forma clara pero  tengo un desenfoque en la espalda vale eso es una anomalía pero claro si utiliza  por así decirlo este filtro inverso por así decirlo o esta rena uran al  inversa que es capturar a la persona y desenfocar la parte de atrás  precisamente para que no se vea todo el contexto que tengo a mis espaldas que yo  quiero que se oculte  en la parte de distorsión pues o haber recién pues por culpa del de las lentes  vale o del tipo de señal que está entrando yo sé que el sensor se  estropeado o que tengamos una lente ya un poco  chunga no sea que está deformada y finalmente la parte de los factos  externos pues el movimiento cuando ustedes no sé cuántos de ustedes han  tenido una cámara digital antes antes de tener la del móvil había la opción  donde apareció una personita como en una como corriendo que era que servía para  sacar varias fotografías varios frames por segundo varias tiradas para que el  desenfoque por movimiento pues no se vea  influido no sé a que no tengas ese error vale y después la del color pues  influye dependiendo la gama de colores que voy a este vale y finalmente es los  artefactos externos pues sobre el la sobre o su exposición de la imagen pues  se que tenga mayor en este caso cantidad de blancos y demás que puedan afectar la  la imagen marcos y es para la velocidad de captura exacto ahí las da  vale  con todo esto vamos a ver ahora qué métodos de identificación tenemos  los métodos de identificación pues disponemos de ejemplos de anomalías por  ejemplo la parte del diagnóstico médico sí un experto va a reflejar los tramos  de las señales que sucede un comportamiento anómalo el sg lo que  vimos anteriormente va viendo cómo va variando el ritmo cardíaco vale o las  derivadas pqs y hay un trabajo donde no hay entonces el médico pues tenemos ya  la detección lo que sería el fraude en la parte de transacciones bancarias que  por ejemplo que hayan muchas transacciones bancarias a la misma hora de  eso puede ser propenso a un fraude es algo que que no es que no es habitual es  algo anómalo valiente entonces teniendo estos ejemplos  pues podemos detectar ahora si no tenemos ejemplos disponibles como cómo  puedo cómo puedo saber cómo puede identificar pues tener que revisar o  analizar las señales temporales una base de datos donde se ha almacenado  estas señales temporales en este caso serían al no tener ejemplos pues  inferimos en rangos que puedan generar la anomalía o que den la anomalía  vale con todo esto para la identificación pues primero  tenemos que tener presente uno si podemos ver las anomalías visualizando  los datos sin procesar 2 poder ver estas anomalías en otros datos transformados  de decir en otro dominio estamos viendo ahora mismo todo el dominio en el tiempo  si lo pasamos a la frecuencia y ya es otro dominio entonces es que pasan con  estas anomalías y finalmente tener en cuenta si podemos separar  estadísticamente estas características normales y anómalas  porque digo lo del dominio del tiempo porque vamos a ver el análisis en  dominio del tiempo y en la frecuencia en todo en toda la materia no en esta  clase vale a veces hacer algunas cosas en el dominio del tiempo es más costoso  que hacerlo en el dominio de la frecuencia  y por otro lado y esto de cara al futuro cuando hacemos un análisis en el dominio  de frecuencia implementar eso en un chip en una re neuronal para ejecutarlo en un  ordenador es mucho más rápido que hacerlo en el dominio de aquí  eso para que lo tengan presente  ahora vamos a ver qué métodos supervisados existirían  para detectar anomalías las que están etiquetadas o el grontrum  no esas ya ya la sabemos son son las más clásicas  después entrenar un clasificador utilizando machin learning el reconocimiento de patrones  eso ya lo han debido a ver por lo menos el reconocimiento patrones lo han tenido que haber  visto en la carrera donde hay anomalías y lo que no es anomalías y por otro lado pues lo que  sería el desequilibrio muy significativo entre ambas clases no lo que sería diferente de la  probabilidad a priori en las necesidades de equilibrar previamente el entrenamiento y  finalmente pues hay que tener en cuenta que pocas aplicaciones de las que se conozcan las  anomalías pues están etiquetadas vale por ejemplo existen muchas aplicaciones vale donde no se  conocen de antemano las anomalías y que pueden ocurrir espontáneamente incluso durante la fase  de test vale en una reina uronal entonces hay que tener presente que eso puede estar presente  incluso en los momentos de entrenamiento si ahora como serían los métodos semi supervisados pues  son anomalías que no están etiquetadas puede haber una aproximación supervisada de decir  entrenamiento con un clasificador tipo clase 1 vale también estas anomalías que van a la salida  del clasificador pues reflejan una baja probabilidad de pertenencia a esa clase y comúnmente se utiliza  en un svm un suelo uf se me dio el nombre del svm su por vector machine vale con un núcleo  de audición que puede realizar este procedimiento vale todo esto es un poco de lo que era análisis  de patrones vale jorge los métodos de identificación en el número al 3 bueno no sea número al 3  filtro de mediana para un data se detectar los anomalías aplicados metodologías de distancia  aquí en el vector suporte ya bueno jorge luego me me lo dice quiero terminar esta parte vale  y ahora los métodos no supervisados son aquellos que no disponemos de muestras anómalas  etiquetadas vale por lo tanto entonces no entrenamos ningún modelo a partir de estos  ejemplos compuestos vale por otro lado se infiere la existencia de anomalías a partir de las estructuras  de datos y hay técnicas de clasper y por ejemplo o estimación en función de la densidad de  probabilidad tipo histogramas el camis método de parser y demás que me van a ayudar a hacer  vale y teniendo en cuenta que estas anomalías van a estar muy lejos de los centroides o de las  agrupaciones que voy a ir teniendo porque van a tener una baja probabilidad al ser observadas  entonces lo que vamos a lo que vamos a ir viendo es esto que vamos a ver esto ahora me diréis me  va a explicar el camis método de parser y histogramas y demás vamos a explicar los  los principales desde mi punto de vista porque porque por un lado hay tanto en la literatura  que lo pueden implementar de un método a un método de ya sea en python ya sea mandat ya sea  no sabe hacerse más más en opec uve o pencele etcétera entonces no vale la pena ver todo el  abanico que existe nos entramos en dos o tres o en uno yo que sé pero la cuestión es que que  tengan la idea clara vale tiene que tener presente que el máster siempre les va a mostrar las  herramientas y yo siempre les voy a dar o voy a procurar darles los ejemplos de estos problemas  en los proyectos vigenes porque la teoría me puedo leer yo todos los libros que quiera puedo  ver todos los vídeos que quiera pero para mí lo que más vale personalmente es los ejemplos dentro  de los proyectos que me vale  con todo lo que hemos hablado si tenemos los modelos aquí para lo que sería un sistema  supervisado se me supervisado y no supervisado pues tenemos mis datos de entrada o mis test de\n",
      "Intervalo 30-40 minutos:  entrada algunos estarán etiquetados otros no tendrán etiquetas mismo de los de entrenamiento  y mis resultados vale si todo es supervisado pues mi probabilidad o me acura así de tener la mayor  cantidad de andó de detectar anomalías de las que no pues es muy fija es muy justa vale en el  semi pero semi supervisado más o menos pero sin embargo en lo que no está supervisado pues me da  unos márgenes de errores vale entonces hay que tener presente eso  esta imagen me gusta ponerla para que se den cuenta un poco de cómo sería este tipo de  clasificación y los métodos que hay por ejemplo por vecindad pues tendríamos el las  k n n s elifore es métodos de clas te rin que tenemos en los modelos estadísticos tenemos  los paramétricos y no paramétricos donde estarían los histogramas el kernel y las mezclas  graucianas vale esto es un poco de teoría a lo que hemos estado hablando a pasándola a una imagen  vale  cómo eliminamos estas anomalías pues vamos a implementar filtros alguien dijo que se  podría utilizar filtros por proximidad vale entonces sí vamos a hacer eso que es un filtro  vale el filtro de la mediana es un filtro no lineal y se puede aplicar en una dimensión o en  dos porque quiero ponerlo en una dimensión para que me entienda vale al que decía esto es  imágenes y es más pero para que me entiendas si este es mi señal y tengo estos dos puntos de  anomalía al far al pasarlo por una mediana los valores van a subir un poco pero la forma más o  menos se va a mantener si entonces este tipo de filtro de mediana es el más empleado para eliminar  estos artefactos estos ruidos impulsivos vale se aplica utilizando medianas de n muestras es  decir si quiero capturar de 6 muestras una ventana de 6 pues 2 4 6 y de aquí sacó más o menos un  promediado entonces tengo más o menos unos valores que debería tener los otros seis lo mismo y así  sucesivamente pues voy quitando los picos ahora si luego en dos dimensiones en vez de coger los 6  primeros cojo una ventana de 3 por 3 y me voy moviendo de 3 por 3 a lo largo del es un ejemplo  también aparte del filtro de la mediana está el filtro de la media es sacar un valor estadístico  la media de unas si tengo una señal cuadrada con estas características ya me sé transferencia de  información por una red de cerne por ejemplo o de o de imágenes que puede ir por aquí claramente  una imagen que está codificada y a esta se le presenta un ruido pues lo que yo hago es con  filtro de mediana pues capturar más o menos la mediana de aquellos valores entre ruido que  o del ruido que tengo o capturar una ventana de una media si se dan cuenta la que más se asemeja es  la media y en dos dimensiones pasa exactamente lo mismo por ejemplo y esto pregunta para el foro  si podéis contestar vale ya les he dicho los que vayan contestando en el foro a las preguntas que  hago pues tendrán su 0.0001 punto en el momento de que revisen por ejemplo tengo una imagen con  un ruido de speckler una imagen gausiana y una salpimienta vale en qué tipo o perdón si en qué  tipo de ruido aplicaría yo este ruido de media en cual de estos tres en cuál sería el que me de  mejores resultados vale responderme en el foro sobre el filtro de mediana tengo una duda en  los tres del tema 5 la pregunta 9 y 10 mientras no me preguntes o mientras no me digas cuál es la  pregunta 9 y 10 jose pues no te puedo decir porque no sé las preguntas no me las sé de memoria vale  entonces si me la pregunta pues tratamos de resolverla con todos salpimienta gausiana  salpimienta a ver porque dicen gausiana y porque dicen salpimienta a ver un pequeño debate rapidillo  no me salen las preguntas del foro aunque tengo visibilidad de las nuevas entradas no las preguntas  cuando digo pregunta para el foro es para que me respondan a la pregunta que estoy haciendo  jose tú dices a la respuesta del tema 5 de en qué ruido se aplicaría del filtro de mediana la respuesta  es esta  vale salpimienta porque salpimienta a ver dígame por qué salpimienta salpimienta es esta la tercera  porque salpimienta y por qué no aquí y por qué no aquí yo estoy por la de gaussian porque como  es una mediana si ves entre los tres gráficos tenemos la opción de seleccionar entre tres opciones  ese es la media en un punto medio y se aplicaría hasta por lógica la más  y  sigue sigue el otro a ver yo diría que la salpimienta porque como es el punto medio  si llevamos eso y lo vemos cómo transformamos esos datos y tratamos de buscar entonces entre  los puntos adyacentes el central se va a parecer más a lo que se suponía que tenía que haber  dentro del ruido entonces por eso creo que más salpimienta en la otra en la en la gauziana y en  la exactamente como hay muchísimo más ruido si buscamos entonces la mediana se va a colocar  más oscura eso es lo que yo entiendo me gustaría que los dos los que han hablado ya ni creo que la  gimnasia no estoy seguro van a ver ahora mismo un código que se ha puesto en matlab traten de  implementar vale pero me gusta cómo lo han defendido porque si no hago la prueba les puedo  decir vale los dos me parecen bien pero como yo ya he hecho las pruebas y demás pues sé que una  respuesta vale pero me gustaría que los que lo prueben vale eso se trata también de que vayan  implementando un poco parece dicho no vamos a codificar la el filtro porque y está implementado  y esto implementado en python lo usaremos la teoría ya la sabemos vale a la hora de resolver en el  examen pues me lo escriben utilizando tal función pues sería así y está pero les invito a que lo  hagan y que lo coloquen en el foro porque lo que han dicho no es nada descabellado y es correcto  ambas pero una de ellas tiene más peso y es cuando lo implementen y se van a dar cuenta  vale a ver qué dice jose vale gracias se aplica un filtro de mediana de longitud 5 al punto  marcado en negrita la siguiente serie en valor de salida vale entiendo que la mediana no es entera  colocame colocame esa pregunta con pie no por trozos por favor vale y lo resolvemos al final  sí pero a lo mejor cuando terminamos la clase te vas a dar cuenta del resultado  o se o la persona que me ha hecho la pregunta creo que sí jose manuel vale vamos que no sé  qué hora es y van a tirar las frecuencias bueno seguimos vale entonces filtro de mediana con el  ruido sal pimienta vale eso sería ahora si tengo este tipo de ruido o perdón este tipo  de imagen original original que está obviamente llámese es una fotografía de hace 20 años de  mis padres no sé qué quiero mejorar cómo podría filtrarla pues aquí está el código de cómo  podría filtrar obviamente he generado un poco de ruido sal pimienta colocando las funciones y  después cómo he hecho el filtrado entonces más la en python lo tenéis si alguien puede compartirlo  en python bienvenido o sea vale yo voy colocando en más la porque manejo un poco más larga que  python pero manejo ambas en alguna otra clase pues les pondré en python vale pero esto es lo que  podrían utilizar que jose que dice cómo sería la mejor forma de poder comentar trabajo de una  empresa pues escribe a tu tutora y se pone en contacto conmigo aunque ya tengo muchos  alumnos de tfm este cuatrimestre vamos no doy abasto vale seguimos ahora esta es una  comparación entre un poco en lo que sería un filtro de mediana y un filtro de haple o  ample o como quieran llamar vale ambos ambas funciones se pueden implementar con esta función  de madad y creo que las mismas son para python vale no estoy seguro por eso hay que tener en  cuenta que estos filtros son muy similares vale sólo que uno las respuestas digamos que son más  más acotadas el filtro de mediana me da como que una pérdida si lo quieren ver así de la energía  o de la señal o del filtro en cambio el de haple no me da tanta pérdida si me mantiene un poco más  de ruido sí pero no tengo esa pérdida de señales esos orlayos no se no se pierden no tenúan tanto\n",
      "Intervalo 40-50 minutos:  vale para qué sirve esto pues a veces como estamos viendo el hecho de atenuar que es lo  que hace el filtro pues estoy recortando información y al recortar información y  a que está más información a la hora de analizar una imagen pues puede ser muy traumático por  ejemplo esto les ha debido pasar a ustedes porque es el mejor ejemplo que puedo sacar  cuántos de ustedes hombres mujeres se han sacado una foto y han puesto la opción de  de que quiero verme más joven para que si las patas de gallo se estilicen mejor y no  parezca que tenga rugas y demás el que me diga que no miente todo yo el primero vale entonces  veo mi piel más más bueno y vale mayormente se hace un filtro de mediana en eso sí entonces por  ejemplo si hacen el filtro de mediana y ustedes en vez de colocar el rostro colocan la mano que no  esté tan cerrada vale cerca el filtro de mediana va a ser como si tuviesen la mano cerrada o si  tuviesen pequeñas deformidades en los dedos y eso aparece vale entonces eso es un error grave  porque ese suavizado esa mediana que se hace al al imagen me está afectando pero estoy perdiendo  los contornos de los dedos vale indirectamente estoy perdiendo eso para eso el filtro de papel  es bueno porque me va a hacer ese suavizado pero me va a mantener un poco los bordes en este caso  vale no me quitará las patas de gallo al 100% pero mantiene los bordes ahora teniendo el contexto  de esta imagen este filtro de los móviles que les he dicho cuando nos vamos a detección y esto  a partir de un proyecto de inteligencia artificial que se hizo tres años atrás no un año antes de  la guerra de ucrania vale se hizo un análisis en campo de batalla para detección de trincheras en  bosques y se puso en con y se puso en en revisión por así decirlo filtros de papel y un filtro a  partir de la papel con inteligencia artificial para que pueda discernir precisamente las trincheras  que estaban casi ocultas en en el bosque vale en la jungla y cuál fue la sorpresa que por más que  la red neuronal y es que haya estado entre nadie demás visualmente para el ojo humano el filtro  de papel era idóneo más que la red neuronal para mostrar en pantalla y poder discernir esas trincheras  entonces teniendo en cuenta eso de papel por más de que no sea supervisado tiene su peso en alguna  de las aplicaciones y no en muchas sobre todo en defensa vale eso que lo tengan presente  ándrea existe el filtro de moda así existe el filtro de moda no lo vamos a ver pero existe  se puede hacer en audio creo que se utiliza más que van a estar un poco confundido entonces los  filtros de mediana se utilizan cuando la foto es muy vieja a ver puedes utilizar filtros de  de mediana o de media vale vas a tener algunos tipos el mejor ejemplo está aquí vale porque  el coloco en una dimensión para que veas es que si lo ves esto en dos dimensiones no vas a apreciar  tanto vale porque porque el filtro de la mediana me va a parecer lo más cercano posible a la  realidad porque debería ser la foto pero la de media estaría un poco más suavizada vale y tú  vas a decir pues mira se ve mejor la de media porque lamentablemente porque si me pongo las  gafas para una persona con gafas un filtro de media es mejor que una mediana de cara a ver  una fotografía vale entonces va a depender un poco bueno seguimos lo que decíamos el filtro de  lo que utiliza es una mediana móvil vale que se aplica como una ventana de tamaño específica  sobre un conjunto de datos vale en vez de todo una móvil que se va moviendo una media móvil después  va a haber una diferencia absoluta donde se va a calcular la diferencia absoluta entre cada  valores de ese conjunto o de esa mediana calculada del paso anterior y finalmente pues voy a tener  una tolerancia vale estos tres pasos se los aconsejaría que les diría que lo sepan por  si entre sea el examen vale si acaso pero tener en cuenta estos estos pasitos que hacen ahora  para los filtros en dos dimensiones que otros filtros también tenemos tenemos la geométrica  para sacar una geométrica un filtro de media geométrica vale pero voy a tender a perder  menos detalles de la imagen es decir bortes y lo que hemos dicho antes las armónicas también me  van a reducir el ruido de sal pimienta o el ruido gausiano vale después hay filtros de máxima  filtros de mínimo donde voy a incrementar y detectar puntos más claros o más oscuros y  reducir algún tipo también de ruido que está presente vale ahora para las técnicas estadísticas  vale hay aproximaciones de función de densidad probabilidad eso ya lo sabemos vale eso es  análisis de patrones vale donde las anomalías van a tener valores muy poco probables y a partir  de la identificación voy a poder hacer una eliminación de estas muestras anómalas o  sustituirlas por un valor estimado o imputado que yo saque por ejemplo una mediana vale entonces  una de las técnicas más conocidas el histograma vale que va a servir para discretizar la de para  tener una discretización de la imagen y a partir de ahí hacer un ajuste vale como ser el de frieman  diacónis vale entonces otro tipo de estimación o otro tipo de técnica estadística pues la  paramétrica donde se asume que la tiene la forma de decir de función de densidad probabilidad por  ejemplo la normal la normal de una matriz vale por otro lado se pueden utilizar parámetros de  funciones para si te gusta una media una varianza una covarianza vale esto es estadística entre  varianza covarianza para poder mejorar la detección en imágenes de ultrasonido por ejemplo por qué les  digo lo de covarianza porque es un método que se ha utilizado mucho en las ecografías de dos  dimensiones a partir del año 2010 a 2014 para detecciones de tumores de cáncer de mamas con  ecografías vale varios de ustedes me van a decir pero el mamógrafo no es un equipo de ecografía  exacto no lo es pero ha habido una línea una tendencia en esos años para poder evitar el dolor  a las mujeres y algunos hombres porque también hay cáncer de mamas en los hombres vale utilizando  estos equipos de presión de los mamógrafos entonces hay estudios hay líneas de investigación  que han estudiado utilizando imágenes de ultrasonido y la mejor forma de eliminar las anomalías y  detectar anomalías de la imagen era con la aplicando filtros de covarianza vale para estudiarlas ahora  me puede decir sí pero ahora podemos utilizar una red neuronal y demás claro puedes utilizar una  re neuronal pero antes no podías pasar a un chip esa re neuronal el día de hoy sí puedes hacer  pero antes llevaba ese procesamiento y miren no estamos ni 20 años atrás estoy hablando hasta  2015 más o menos o 2013 creo que era pero donde la re neuronal ya ya pegó su estirón y demás con  las con el para el computing de las tarjetas en vídeo pero no ha sido hasta estos últimos 5 10 no  5 7 años que apego al estilo vale pero entonces si así de esa y las desventajas pues la función  de esa probabilidad pues de la variable puede significadamente tener una distancia muy gradual  respecto a la imagen original a las agrupaciones wall clustering que tenga a ver qué dice aquí  y pose la pregunta 9  y creo que ya la respondió tú edwin no ahora la respondemos que ya vamos a terminar vale nos  queda poco otra forma de la eliminación de anomalías por forma estadística pues la ventana  de parse vale es una estimación de la función de densidad de probabilidad donde estas ventanas son  útiles en situaciones donde la forma de la distribución no es conocida de antemano no  sabemos la distribución que tiene es muy aleatoria esto por ejemplo para proyectos en  astrofísica se utiliza bastante y se sigue utilizando porque porque si hay algún astrofísico  que me diga si me equivoco pero el telescopio web este que aparte de esta programado en java no sé  qué parte del sistema del cuero que tiene utiliza ventanas de parse para que mediante la función  de probabilidad poder sacar todas estas eliminar todas estas anomalías de forma estadística  vale ahora me diréis pero porque no utilizan una reina oral repito el red son web por ejemplo el  satélite o el telescopio es que ha sido diseñado hace años hasta que se pruebe hasta que se lance  pues ha tardado su tiempo para que se hagan una idea todo toda la tecnología que los próximos  yo creo que cinco años se va a ir lanzando al espacio que vamos a llegar otra vez a la luna y  demás y tiene internamente todo el desarrollo de redes neuronales puestas en chips entonces  ya tiene toda esa tecnología que al día de hoy está siendo ya un boom ya se está colocando en\n",
      "Intervalo 50-60 minutos:  chips antes no se podía implementar eso porque era muy grande para hacer el cómputo y para los  cálculos y para la fabricación a día de hoy es sencillo pero unos 10 años atrás era casi  impensable vale a día de hoy ya se puede hacer todo entonces toda la tecnología que viene de aquí  adelante pues ya está comenzando a tener eso para entonces tengan presente que a lo mejor de aquí a  cuatro años pues digamos pero para que utilizar una red neuronal si puedo utilizar yo que sea una  red de súper neuronal está inventando un hombre no donde la evolución sea más rápida y sea un  autopredesaje en tiempos reales y que no esté con esta que no esté en marcado en un chip tal cual  a mano no sé si metiéramos que la tecnología avanza y todas estas cosas que estamos viendo ahora  pues ha sido el core de antes que sirven al día de hoy porque es lo que estamos viendo de hoy pero  lo que viene el mañana se va a implementar de lo con lo que están aprendiendo incluso vale  el ejemplo de la ventana de parsen pues esta es nuestra ecuación vale la función de parsen y  tenemos aquí la bueno estamos diseñando un poco lo que sería cada uno de los parámetros vale donde  cae el núcleo que tiene estas características de la densidad del espacio h el ancho de banda de la  ventana en el número de puntos de datos que voy a tener vale y f pues la estimación de la densidad  probabilidad en un punto esto es una dimensión si me dicen y profesor como en dos dimensiones  sencillo doble sumatoria de i hasta de igual a 1 hasta él pues la otra sumatoria de i o de j1  hasta m vale entonces con eso ya tendríamos la ventana de parsen ya les he explicado ventana  de parsen se utiliza más que todo en astrofísica porque es para capturar imágenes que son  completamente variables las funciones estadísticas que hemos visto normalmente se utilizan en  imágenes en captura de imágenes en tiempo real vale dos dimensiones aquí en la tierra es lo que  vamos a ver así donde tenemos las anomalías más o menos detectadas tienen en cuenta los histogramas  y demás vale y los filtros en este caso analógicos pues que están presentes que ya lo he mencionado  que hay por ejemplo el teléfono móvil pues tienen medianas geométricas o medianas o de mediana o de  media vale  y con eso estaríamos hablando todo lo que sería las anomalías vale entonces respondiendo a nuestra  pregunta de edición cómo podría diseñar algo que no sea supervisado para detectar y eliminar  estas anomalías me aplica un filtro ya sea de forma estadística ya sea de forma aritmética y  ya lo tenemos vale entonces ya sabí perdón  entonces ya podemos responder a nuestra ahora qué es lo que vamos a ver la próxima clase pues vamos  a ver cómo abordar el desafío de mejorar la visibilidad de detalles en la imagen  aquí me refiero visibilidad de imágenes bordes fugosidades a lo mejor una imagen está muy  oscurecida y les digo pues cómo podría detectar mejor la imagen de que se ve al fondo vale o que  está sobre expuesta que hay muchos blancos más o menos eso es lo que vamos a ver la próxima clase  función de ciudad probabilidad fdp vale a ver ahora preguntas  y si no respondemos aquí al amigo que lo hemos dejado  hasta el momento no porque a dos se utilizaron muy sensuales  pues muchas gracias a ver jose vale la pregunta nueva está si se aplica un filtro de mediana de  longitud 5 al punto marcado en el grita de la siguiente serie cuál ser cuál será el valor de  la salida 2 tan tan tan tan tan tan vale si aplico el filtro mediano a longitud 5 aquí  cuál será el valor de salida dime cuál es la mediana de esto  cuál es la mediana de eso pose esto es estadística sácame la mediana  y  o sea  necesito que me digas y cuál es el valor de la mediana  que se quitaba el valor del medio no a ver por eso le las preguntas por eso te  he dicho córcame la pregunta o sea de esto sea de este truso cuál es la mediana la saca si  esta es la respuesta si  mediana es el que más se repite ya lisa o es la moda  es la moda la que más se repite exacto  pues si no hay más dudas no sé si no hay por eso es que está metida la silla la silla de qué  según chargpt el tren a ver vamos a vamos a parar un poco esto si no hay más preguntas pues ya está  la actividad así es verdad jose he visto tu pregunta no me ha dado tiempo de resolverlo  he regresado y de viaje y he estado hasta arriba tranquilo vale pero lo voy a ir resolviendo vamos  a dejar de compartir y vamos a parar la grabación\n"
     ]
    }
   ],
   "source": [
    "current_interval = 0\n",
    "accumulated_text = \"\"\n",
    "\n",
    "for segment in result4[\"segments\"]:\n",
    "    start = convert_to_min_sec(int(segment[\"start\"]))\n",
    "    end = convert_to_min_sec(int(segment[\"end\"]))\n",
    "    text = segment[\"text\"]\n",
    "\n",
    "    # Verificar si el segmento sigue dentro del intervalo actual de 15 minutos.\n",
    "    if start // n == current_interval:\n",
    "        accumulated_text += \" \" + text\n",
    "    else:\n",
    "        # Imprimir el texto acumulado para el intervalo actual.\n",
    "        print(f\"Intervalo {current_interval*n}-{(current_interval+1)*n} minutos: {accumulated_text}\")\n",
    "\n",
    "        # Actualizar el intervalo y reiniciar el texto acumulado.\n",
    "        current_interval = start // n\n",
    "        accumulated_text = text\n",
    "\n",
    "# No olvidar imprimir el último intervalo acumulado fuera del ciclo.\n",
    "print(f\"Intervalo {current_interval*n}-{(current_interval+1)*n} minutos: {accumulated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "result5 = model.transcribe(\"/home/contrerasnetk/Documents/Classes/VisionArtificial/5.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intervalo 0-10 minutos:   a ver disculpen la demora pero vamos se me  ido de las manos  y ahí tenemos y la compartimos  podéis verla  si si si si si si si vale genial un participante habilitado transcripción  de subtítulos bueno no sé ven subtítulos no no está todo perfecto  si no se ve hasta ahora no se ve ninguno ya saben que me pueden interrumpir al  momento que quieran vale porque me gusta que las clases sean  siempre a menos vale pues nada vamos a comenzar con este tema  el tema 6 lo que sería el procesamiento de imágenes operaciones elementales vale  que son operaciones más sencillas que que se puede colocar a una imagen vamos a ir  viendo alrededor del tema y vamos a comentar también como me  dijeron algunos de ustedes darles un poco más de ejemplos de proyectos de  healthcare y también algunos civiles más que militares así que trataré de hacerlo  vale  entonces activo el punto estoy grabando no alguien que no confirme si señor  pero si señor  y colocamos el puntero laser y vale que hemos visto la clase anterior pues  hemos visto los tipos de ruido que había no el salpimiento que tanto me  gusta  de espectra y demás y también hemos visto cómo poder quitar ese ruido no  haciendo una serie de filtros haciendo un filtro de mediana de media etcétera  donde teníamos una respuesta mejor no pero claro aplicar esos filtros puede  degradar en algún momento nuestra imagen sí es verdad se ve menos con nieve pero  puede estar un poco más difuminada vale entonces digamos que no está mal pero  hay que ver cómo se aplica este tipo de filtros  vale dependiendo la imagen y dependiendo también el de los proyectos porque les  digo esto si analizamos proyectos de healthcare que apliquemos mal o bien un  filtro es muy muy importante porque eso nos puede servir para discernir un  posible tumor una posible una posible causa un posible problema en el cuerpo  en la parte civil pues no tendría mucha pupa es decir no haría mucho daño  pero sí influiría mucho en lo mejor tomar una decisión que no sea tan  crucial la parte militar y la parte de espacio pues no están no es tan  complejo es más en la parte de espacio se utiliza un poco bastante los tipos de  filtros estos que hemos visto la anterior clase sobre todo en el contexto de poder  analizar el tipo de no de imagen sino de luz es decir el rango de frecuencia que  nos llega la señal que para saber qué tipo de estrella para así decirlo qué  tipo de argumento cósmico se está viendo  con todo esto en esta clase que es lo que vamos a plantear vamos a abordar el  desafío de mejorar esta visibilidad de imágenes pero  teniendo en cuenta los detalles que hay en la imagen es decir tratar de aplicar  un filtro o mejorar a la imagen pero no con el ámbito no con el objetivo de a  lo mejor quitarle el ruido sino con el objetivo de que sean más visible para  nosotros y si para el ojo humano es más visible pues de cara a una red neuronal  pues va a ser mucho mejor  para resolver este problema o esta pregunta pues vamos a comenzar con la  introducción vamos a plantear los objetivos unas operaciones  que vamos a hacer a la imagen y vamos a hacer una pequeña implementación para  ustedes me lo van a decir que es mejor en la introducción pues hay que tener en  cuenta que lo que vamos a hacer es hacer un real ceder imagen vale son etapas  iniciales de el preprocesamiento después de la reducción del ruido es  decir toda esta parte de procesado de imagen es el lo que tenemos que hacer  con las imágenes antes de llevarla a la red neuronal algunos de vosotros me  dijeron la anterior clase que podríamos utilizar una red neuronal incluso para  hacer el propio procesamiento y efectivamente se puede hacer vale pero  antes de que surja la red neuronal pues nosotros tenemos que saber ya por lo  menos que herramientas vamos a utilizar vale vamos a ver también un poco la  diferencia entre la captura de información relevante versus la  información prescindible lo que les decía al principio que es lo que  necesito tener y que es lo que me daría un poco igual que se elimine o que se  difumine a la hora de tenerla el filtrado vale por otro lado como sé si una  información es relevante pues me va a depender un poco del contexto no del  tipo de proyecto del tipo de escenario en el día de móstico médico pues la  información necesaria va a ser las lesiones es decir los cambios de  textura y los cambios de textura como se definen en la parte médica pues los  cambios de colores en este caso de escala de grises en la vigilancia para  cámaras por decirles de control de personas en  lugares fronterizos llámense eeuu y demás pues me sirve para detectar  movimiento y dependiendo el tipo de movimiento pues puedo decir si es una  persona o es un animal no eso sería la información relevante y finalmente en  el rostro pues poder detectar aquellos detalles que puede hacer que un rostro  sea más semejante a otro y poder identificar personas no me va a servir  y  con todo esto pues vamos a realizar un realce de la imagen que no es más que en  aplicar una técnica o un conjunto de estas técnicas a fin de incrementar  algunas de las características en la zona de por ejemplo aquí claramente una  foto de un niño pequeño en el cual yo le he aplicado una serie de  de técnicas y me pueden decir bueno es lo mismo se ve la misma imagen pero si yo  les digo pueden visualizar mejor el escudo en cuál de las dos se ve mejor en  la primera o en la segunda que tengo en realce de imágenes obviamente la segunda  igualmente algunos detalles muy bien daniel algunos detalles como por ejemplo  si está con si las manos tienen guantes o la mano descubierta pues aquí  pareciera que tuviese guantes porque no se nota bien pero aquí puedo decir por  lo menos en este lado que no estaría utilizando guantes vale y así otros  detalles sin embargo pues también tengo pérdidas en algunas otras cosas no todo  es ganancia pero vamos con esto de realce pues me sirve para eso  luis que dices este gracias con el objeto de mejorar la percepción ante el  ojo humano o podría afectar al entrenamiento de una red neuronal en  realidad afectaría el entrenamiento del reloj  vale lo de la percepción del ojo humano eso está por sentado pero de que afecta  a la red pues sí porque lo digo esto porque hay algunos artículos sobre todo  cuando utilizan imágenes sobre imágenes de distintos sistemas de  adquisición óptica es decir una cámara rgb normal de toda la vida un infrarrojo  o un azar y tratan de fusionar ambas imágenes varias ambas muchas  tecnologías al fusionar estas imágenes es decir quedarme por ejemplo con el  canal rojo del rgb el canal de banda base de una imagen  y yo que sé está la escala de calor del de la ir o del calor y metría y  fusionan ambas imágenes el resultado para el ojo humano no es nada visible no  no se puede obtener nada pero sin embargo eso pasado a una red neuronal da  mucha información y sirve para discernir varias cosas  se aplica una parte de la foto a toda la foto a toda la foto vale siempre vamos  a hacerlo a toda la foto se aplica a una sola parte pues pues no tiene mucho  sentido porque sólo está mejorando una parte de la información la información  es todo y tienes que sacar la información de toda la imagen  por ejemplo imagen de la izquierda vale es una foto tomada de de un valle vale  claramente se puede ver que por características de la cámara y demás  pues la imagen es muy ompaca sin embargo la parte de la derecha pues después de  realizar ese realce pues podemos discernir los árboles que están  presentes si hay un camino no aquellos detalles de las colinas y las cosas  luís estas operaciones también pueden funcionar como data aumentación o la  red interpretaría como la misma imagen no sería un data aumentación  estamos haciendo un preprocesamiento el data aumentación dijimos aumentar  información a los datos es decir como colocar nieble o nieve o lluvia a una  imagen vale o incrementar los datos vale es es otra cosa esto es un pre  pre procesado hay que tener presente eso el hecho de coger las imágenes y\n",
      "Intervalo 10-20 minutos:  dejarlas bonita para la reno uronal por así decirlo ese esa acción es el  preprocesamiento el propel preprocesado de las imágenes el data aumentación es  como tengo pocos datos y demás pues voy a tener voy a coger lo que tengo y le  voy a poner más ruido le voy a girar le voy a invertir y voy a hacer varias  cosas no es lo mismo vale está por esa línea pero no lo es vale es como el  el refrán que dicen todas las mantas son rayas pero todas las rayas no son mantas  me refiero a una manta rayas entonces es es va por ese lado  con todo esto pues el ejemplo de operaciones de rease que se puede tener pues sería un poco  su presión el ruido realzar el contenido frente a la textura de la imagen el tener un ajuste de  intensidad un aumento del contraste incrementar ese contraste y por otro lado el tener ese  realce de bordes que me van a servir para las transiciones de elementos de imágenes  el renso en sí el redimensionamiento sería la parte del preprocesamiento si es lo que estamos hablando  entonces por qué tengo que realzar las estructuras de las imágenes esa pregunta me la tengo que hacer  para que no es uno para que el observador en este caso del ser humano pues las imágenes  sean más fácilmente digeribles no que puedan distinguirse con ellas con mayor nitidez esto me  refiero a la parte del ojo humano al ser humano para la máquina este tapa de procesado vale su  objetivo es tener los inputs adecuados en las etapas posteriores destinadas a extraer las  información extraer esas características vale como como detectar bordes o generar imágenes  que sean fácilmente interceptables por el ojo humano como para la persona pero ya les he dicho  cuando tengo que aplicar distintas cámaras o distintas técnicas de adquisición de imágenes  fusionarlas para poder obtener una un tipo de imagen que me va por así decirlo capturar todo el  todo el concepto de lo que se está viendo en ese tipo de estructuras de imágenes para el ojo humano  la imagen que está viendo que resultante no es nada pero nada nada parecida con el mundo real  sin embargo si tú le enseñas a la reina oral que lo que está viendo tiene la característica de lo  que se está viendo actualmente en el entorno pues te va a ayudar a que tu reino oral puede entrenarse  cuando la imagen que estás obteniendo tenga todo ese problema de entorno que se presente cuando  captura la imagen a qué me refiero imagínense que ustedes tienen una cámara de vigilancia de  fronteras vale y están en digamos en londres vale o cerca del reino unido entonces por el  hecho de que estoy ya en costa tengo niebla en alguna época del año por no decir en bastantes  bastantes épocas del año por otro lado es de noche entonces si ya tengo niebla una cámara  rgb normal de toda la vida ya puede ver muy poco por la niebla si es de noche peor todavía para ese  caso entonces necesitaría una fusión de una cámara que me tenga una cámara normal convencional  electróptica fusionada con una cámara infrarroja para poderla verla de noche o que tenga opciones  de night vision más una cámara sar vale de estilo de radar que pueda ver más allá de lo que ve un  imagen con niebla vale entonces teniendo en cuenta eso al funcionar las tres imágenes y obtener  una imagen completa de esas tres fusionadas para el ojo no va a ser cualquier cosa ser una  pantalla de nieve por así decirlo pero si le entrenó al reino lunar y le digo mira este lo  que está viendo aquí tiene estas características imagínense que tengo el resultado de la misma  posición de la de la de la foto de la cámara cuando tengo el medio condiciones medioambientales  iréales es decir buena luz y el electro la cámara electróptica o rgb la ve completamente  entonces la le puedo decir que lo que está viendo en la cámara fusionada es lo mismo que la cámara  normal la electróptica entonces si voy entrenándolas en paralelo y después las junto el resultado va a  ser que cuando vea toda la el resultado de la fusión de los de las tres cámaras por así decirlo  va a ver la cámara normal y si hay algún problema entre medias es decir que aparece una persona que  haya un coche un animal o algo se mueve entre medias pues ya lo va a ir de testa y creerme porque  ese proyecto ha funcionado y ha llevado un par de años funcionando en algunas fronteras ya sean  norteamericanas como europeas vale entonces nos sirve para eso con todo esto los objetivos  vamos a definir los realces vale como etapas de procesado de imagen vamos a definir las  operaciones de tipo punto a punto para qué nos van a servir y cómo lo vamos a usar vale y vamos  a conocer las técnicas principales del procesado punto a punto a juntas de intensidad las basadas  en histogramas y las operaciones aritméticas vale tienen más información también en la  documentación vale pero lo que me gustaría es dejar esto claro porque esto viene a ser la  base de lo que está puesto también en la literatura las operaciones punto a punto como su nombre lo dice  es coger el valor de un píxel multiplicarlo por una función x en este caso t y el resultado  colocarlo a otro el resultado es el píxel irá a otra imagen vale me arraigaba un conjunto de  puntos de la imagen multiplicado por t resultado mi imagen b o mi raíz que tiene la multiplicación  qué herramientas usas en tu trabajo para implementar las técnicas de las que nos hablas herramientas  de cámaras las que les he dicho electro ópticas que son las cámaras regi normales perdón si  hablo con terminología técnica pero es que es así infrarrojas o ir vale imágenes ar que tienen  a través de imágenes de radar o si de radar y también utilizamos las multies fractales  vale son cámaras no son tan grandes parecen unas cámaras reflex que tienen todo el tiene  un abanico de espectro para obtener imágenes eso se utiliza más que todo en espacio  sobre las operaciones punto a punto bueno es la astronomía exacto sobre las operaciones punto  a punto las propiedades pues se conoce que me va a ayudar para hacer transformaciones sobre todo  en escala de grises vale porque porque son más fácilmente procesadas luego procesables por  separado puede utilizarlas también para multiplicar imágenes a color y demás pero me estoy colocando  por así decirlo tres variables en las cuales jugar con la escala de grises solamente tengo  una variable vale entonces tener en cuenta eso mayormente vamos a trabajar en escala de grises  para hacer todo el propio el proceso pero no quita que se pueda hacer también en rgb en esos  colores entonces para el ajuste de intensidad por ejemplo aquí tenemos una imagen satelital  vale que tenemos claramente una cantidad elevada de blancos o sea se ve más cantidad de blancos que  que grises y demás entonces lo que yo voy a realizar aquí es una realización quedarme con  un margen de valores si desde 0 a 255 pues voy a quedarme entre 164 y 255 entonces con eso yo estoy  como que atenuando alguna intensidad y me quedo con lo más importante en este caso con lo que más me  estoy quedando pues hacer mi imagen satelital es con las imágenes de las nubes con las nubes altas  las nubes bajas pues ya me las eliminaría entonces con esto ya puedo por ejemplo hacer  pequeñas predicciones de movimiento de nubes y demás vale luis entrenar con escala de grises  permite clasificar imágenes de entrada color no estoy hablando de procesado vale no de entrenado  pero ya siendo procesado si tú procesas tu imagen en escala de grises después te lo puedes llevar  a color si quieres sabe o sea transformarlo vale  después seguimos con esto perdón con la urealización lo que yo puedo sacar son  histogramas de colores por ejemplo la imagen que hemos visto anteriormente pues tiene este  histograma vale entonces yo puedo discernir pues el histograma de color rojo el histograma de color  verde el histograma del color azul y demás y puedo realizarlo de tal forma de quedarme con un solo  conjunto de colores vale quedaron más con los rojos quedaron con los azules con los verdes etcétera  vale entonces voy a realizar mis valores de la imagen vale desde el punto de vista\n",
      "Intervalo 20-30 minutos:  entre entradas y salidas de grises con los valores y demás pues está bueno yo le llamo  mi tabla de ojo vale no me acuerdo ahora mismo el nombre pues me sirve para saber el tipo de  imagen o el tipo de un realización que puede ir consiguiendo no si sea más negativo sea más  positivo si puedo utilizar operaciones logarítmicas o de potencia vale para poder sacar todos los valores  vale entonces un poco de teoría pero para qué me sirve esto pues para poder un realizar y  quedarme con aquellas unas de información que me vayan a servir para para el proyecto o para  para el contexto que quiera implementar  después la parte de sacar un negativo de una imagen vale sacar el negativo de imagen pues es  básicamente lo que sabemos los negativos de las imágenes que están en una cámara  fotográfica y demás no es más que mi imagen imagen en función de u es igual a l menos la  función de u de la imagen que tengo donde eres el patrón vale esto se utiliza mucho  para realzar estructuras vale el ejemplo más claro es para estudio de mamografías y de radiografías  todos ustedes bueno no tanto la mamografía pero todos ustedes han visto una radiografía y lo que  se ve más es escala de grises y blancos que es lo que me sirve para realzar y todo eso sin embargo  cuando quiero analizarlo desde el ámbito de la red neuronal por lo menos hasta el año 2021 lo que  se trataba de hacer era invertir los colores vale para poder entrenar estas imágenes porque les  digo que se hacía esto número uno porque yo he tenido que hacerlo en el año 2015 vale como  mamografías pero básicamente porque esto pasado a un ordenador a una computadora que tenga las  tarjetas en vídeo y demás ocupaba menos información esta imagen a pasarla a la tarjeta gráfica que  pasarla en escala de grises perdón escala de grises en color normal vale o sea en positivo  no en negativo vale esto también sirve un poco para la parte de la parte de neurocirugía vale en  la parte de análisis de neuronas y demás en el cerebro también se utiliza bastante aquí ya son  un poco con imagen a color para el análisis pet en las máquinas de procesado que utilizan las  máquinas para los hospitales a día de hoy siguen utilizando lo mismo vale que significa cada término  de la ecuación está de aquí te refieres la ecuación si mi imagen te supone llamarlo esto vale donde  todos los valores de u vale te supone es esto y la negativa pues sería a cada uno de estos valores  de u pues lo estoy cambiando lo vale o sea le estoy restando por eso le estoy dando su positivo  negativo sumarle más 255 o restarle más 255 vale es una matriz vale y aquí es básicamente lo mismo  vale sólo que al estoy dando la variable de 1 vale aquí tenemos un real set de colores claros o grises  la implementación si lo quieren ver de forma más fácil tenemos el siguiente código que me lo hace  una pregunta y más es como una ayuda para mí en este momento porque yo tuve ahorita como  una entrevista en un call center donde me piden preprocesos de datos de audios de llamadas pero  como entender a todas las empresas en aquí en ecuador al menos la inteligencia artificial  es nueva entonces ellos no saben qué captura adora de audios da mejor calidad de audio o que  cámaras toma mejor calidad de cámaras entonces no ellos no están enterados ellos simplemente  tienen sus cámaras de siempre y ahí mira tengo estas imágenes ya entonces como yo identifico qué  técnica para procesar de datos debo utilizar según los datos que ya estoy viendo o es más  bien de prueba y error pero 20 técnicas la que mejor me predice esa utilizo o cómo yo ya puedo  identificar así rápidamente a no esta captura adora 10 20 está siempre le aplico esta técnica y ya  a ver no digamos que es prueba de error va a depender del tipo de proyecto que quieras hacer  o sea yo por ejemplo  si es para capturar imágenes en ambiente vale o sea al aire libre pues obtendré algún tipo  de captura adora que tenga un objetivo más amplio que uno más pequeño que tenga una relación  de rate yo que sé de 1 partido por f al cuadrado por ejemplo es algo de óptica sí pero si lo quiero  capturar más imágenes en la noche pues tendré que tener otro tipo de características es que va  a depender mucho del proyecto en el cual te centres lo mismo en el audio si es un call center y tú  tienes que grabar audio vale pues tu captura adora de la captura de audio que tengas porque  tendrás en la persona que está hablando también el parlante la altavoz o la bocina en la cual está  la persona al otro lado del teléfono tiene que ser capaz de capturar con rectamente el audio que  sale de la bocina o del altavoz vale porque si no captura el bien ese audio y solamente el tuyo  tu problema ya tienes un problema puedes capturarlo como si fuese un ruido y eso está mal vale entonces  no necesariamente hay un patrón fijo y tampoco es prueba y error es fijarse y esto es algo que  lamentablemente la mayoría de las empresas que se van adentrando cada vez más en la inteligencia  artificial y comprando cámaras comprando sensores y demás no sabe es que tiene que fijarse las hojas  y las características de los fabricantes para saber el ancho de banda las características que  tiene la velocidad de capturar imágenes por ejemplo si tengo que capturar imágenes en una  autopista pues los coches o automóviles o carros van a toda velocidad más de 100 kilómetros por  hora y si yo saco una fotografía con una cámara normal pues va a ser distinta con una que saque de  alta velocidad entonces depende mucho el contexto no es tanto prueba y error es fijarse las características  que te da el los sensores vale para poder resolver el objetivo si es eso lamentablemente uno piensa  que hay una un sistema para todo pero no es tanto así vale entonces yo como ingeniero de yo veo que  el tipo de data que ellos tienen actualmente no me sirve y no y no me va a apoyar para el objetivo  que ellos me están pidiendo entonces yo debo decir bueno ya no me no podemos con estas cámaras o con  este audio utilizar mi capturadora con esta especificación y comenzamos a dar datos es claro  es claro es mira si tú por ejemplo grabas con tu su celular tu móvil está este curso te vas a dar  cuenta que en algunos momentos a mí me va a detectar como si fuese el ruido y no se va a grabar  mi voz completamente porque porque el móvil tiene varios micrófonos para detectar el ruido vale y  poder tener una mayor transmisión de audio o captura de audio entonces está configurado para eso si tú  utilizas una capturadora de audio o una cámara que sea genérica pues va a cumplir con esas  características no con las que tu proyecto necesita entonces hay que tener mucho cuidado  ok entonces yo como ingeniero tengo que estar consciente de eso y en el caso de que no se no  me sirva pedir lo que me sirva no claro tú tienes que analizar tú como ingeniero tienes que analizar  qué características o qué cantidad de datos necesitas o cómo tienen que ser tus datos a  partir de los de las características de datos que necesitas tienes que ver qué instrumento  qué sensor te va a garantizar esos datos y una vez que sepas qué sensor instrumento y demás pues  ahí hables el fabricante y cuando veas el fabricante pues verás después el coste el precio vale siempre  se hace ese análisis que es lo que yo quiero pues tener estas cosas para tener esto que  y pues voy a necesitar pues con estas características que características tiene esto pues estas  cámaras necesito vale tengo mi conjunto de empresas que me da esas características de  cámaras o de audios a partir de ahí veo cuál es la que se acomoda mejor al precio porque no voy  a elegir una de un millón de euros versus una de 10 euros que a lo mejor me saca lo mismo claro  no me puesto no puedo gastar tanto entonces tienes que hacer ese análisis vale porque gracias  bueno seguimos a ver qué dice gino es una matriz y se todos estos procesos se aplican por programación\n",
      "Intervalo 30-40 minutos:  o directamente sobre aplicaciones ya existentes todos a ver no estamos inventando la rueda y  hay aplicaciones vale y hay librerías y demás pero lo que yo les estoy enseñando con mostrándoles  incluso las ecuaciones y demás es para que vean lo que las tripas que tienes vale para que no me  digan a pues mira esta cámara si le doy este botón me muestra el negativo de la imagen que  está capturando cómo lo hará ya saben que lo que hace es una sumatoria es una multiplicación  hace una suma es una resta yo que sé vale después que dice circo al momento de utilizar los filtros  de voces no afectan el resultado que se desea obtener depende del objetivo ser en el gal no  sé qué ves ahí pero bueno la cámara tiene función algunas cámaras aplican algoritmos y  en el caso de consulta compañero bueno ahí está es verdad que hay algunas cámaras que incluso las  naijok que son cámaras militares ya tienen la inteligencia artificial empotrada en las cámaras  hacen el seguimiento te hacen un tracking te hace una detección y demás claro te lo hacen todo y  creo que la clase anterior o hace dos clases les dije antes eso no existía entonces tú tenías que  montarlo en la computadora pequeña o lo que sea para que la cámara ahora y está todo metido ahí  vale lo que nos viene en el futuro a lo mejor es que directamente en un futuro tú le digas qué  cosa quieres entrenarlo o qué cosa quieres que haga la cámara y ella misma ya tiene internamente  varias inferencias o se ponga a entrenar una reina neuronal interna y te saque el resultado  la tecnología está avanzando vale que tener claro es bueno seguimos que nos liamos vale  con las transformaciones logarítmicas pasa lo mismo a mi matriz de imágenes del tu le voy a  aplicar una matriz logarítmica vale esto para qué va a servir pues los valores pequeños se van a  expandir se van a ampliar las diferencias y los valores grandes se van a concentrar y se van a  reducir las diferencias vale esto se emplea cuando se pretende expandir un rango de intensidades de  píxeles oscuros por ejemplo o en escala de colores también vale para qué me sirve esto  pues por un lado para visualizar imágenes en el domínio de la frecuencia donde voy a sacar un  espectro que va a ser los valores picos en bajas frecuencias y me va a servir para resaltar también  las diferencias con el contraste por ejemplo esta sería una imagen de un punto lumínico ideal vale  en una escala normal pero cuando lo paso a escala logarítmica esto se amplía y se amplía  bastante entonces ustedes pueden ver aquí el espectro de onda que causa ese pequeño haz de luz  en todo el en todo este sistema o en todo este contorno o contexto como quieran llamarlo podéis  ver el aquellos pequeños haces que van en la parte horizontal y vertical que son llamados los  lóbulos de rejilla y demás que se forman parte como los rebotes de la luz pero claro eso solo lo  veo cuando le ha aplicado este tipo de transformaciones en este caso logarítmicas por ejemplo volviendo a la  imagen original pues mi imagen original si le hago un estiramiento de contraste pues tengo un mayor  realce pero claro aquí por ejemplo la cabeza y ese va un poco difuminada con el fondo y si le hago  una transformación logarítmica pues ya puedo discernir algunas cosas y lo puedo ver mejor vale  ahora quien de ustedes me dice en qué tipo de imagen médica se podría utilizar esto  esta esta transformación logarítmica  en una red de la red que una radiografía la radiografía utiliza la normal  la sonora es tal vez  en la ecografía y sobre todo a día de hoy en la ecografía utánea de la piel para poder sacar  epidermis dermis y podermis y esta cosa que creo que es la glas para esto en la resonancia se  utilizaba pero no tanto es más la normal para el pet sobre todo cerebral también se utiliza esta  pero la de ultrasonido y esto les digo porque por un lado yo soy especialista en la parte de  ecografía y por otro lado mi esposa es es dermatóloga se utiliza bastante este análisis  a la hora de detección de melanomas vale que por lo menos aquí en europa que toda la gente  quiere ser morena y ponerse en el sol no entiendo por qué sirve para poder discernir lo que es un  melanoma muy muy malo que es casi metástasis de uno normal entonces en aplicaciones de ecografía  se utiliza bastante es más y hay equipos de ecografía sobre todo cutánea que ya tiene  implementado este de melo filtro vale pero eso más que una transformación a la hora de ayuda al médico  sí porque con eso va a poder discernir ya directamente esos pequeños niveles y ahora  por qué les digo más que todo de la piel porque saben que la piel es muy finita y aunque no lo  crean por lo menos la parte de los melanomas pues ustedes pueden ver aquí como digamos el pequeño  lunar como si fuese la yema de un huevo siempre me gusta usar ese ejemplo como si fuese la yema  del huevo pero por debajo se extiende como si fuese la clara del huevo y lo más malo lo más que  puede llegar a ser en una metástasis metiéndose en cualquier otra parte del cuerpo o sea que te  pueda llegar más allá pues está por debajo y eso a simple vista no se puede detectar y con el  ultrasonido de pie sí lo pueden hacer gracias precisamente a las transformaciones marínicas y  eso se está haciendo desde el 2012 si más no me equivoco y simplemente a los equipos del 2003 2014  vale y obviamente volviendo  bueno volviendo a la imagen del globo  todas estas transformaciones van de cara a la ley de potencias porque porque así como puedo sacar  una transformación logarítmica pues puedo sacar una transformación de potencia en algo que sea  exponencial vale las ventajas respecto a la logarítmica por ejemplo la parametrización  me sirve para ampliar la familia transformaciones mediante la variación de para esto que quiere  decir que si mi ley de potencias es menor que uno me aumenta la diferencia de contraste si mi ley  de potencias es mayor que uno me incrementa las diferencias es decir el contraste de los píxeles  vale porque no tocó tanto la parte de la logarítmica porque normalmente a la hora de hacer rendes  neuronales que lo habéis visto más son las aplicaciones gráficas de este tipo que las médicas  pero sin embargo ya saben se analiza la ley de potencias es lo contrario a la logarítmica  vale no sea bueno no lo contrario sino la otra cara de la moneda para la ley de potencias por  ejemplo para realzar el el ojo en la pupila o la pupila en el ojo vale teniendo una imagen normal  aplicando potencias puedo ir jugando con esos valores hasta obtener un valor más o menos  idóneo a nuestras características vale la implementación pues sería más o menos esta  en marla no la función que desde con la función la imagen ajust que hace todo esto vale ahora bien  aquí estamos viendo cómo ver o hacer estas transformaciones a toda la imagen  vale alguno me dijo y porque no hago un trozo pero yo he dicho no no se puede o sea pero  para qué precisamente porque para eso hay funciones que me definen a trozos lo que voy  a ir analizando me va a servir al final si o sea lo que he dicho antes es incorrecto pero  le he dicho más que todo para que no vean esta parte primero quería que vean que puedo analizar  todo del tirón y ahora como ya sé cómo analizar toda la imagen pues lo puedo diseñar a distintos  trozos vale porque porque me va a defender me va a ayudar para definir las mejoras dependiendo  el rango de exposición de una parte de los píxeles vale la zona principal y demás con  todo esto pues puedo a un lado se utiliza la ley de potencias y la de la transformación logarítmica  pues aplicar a distintos rangos o a distintos márgenes en toda la imagen también vale  entonces que dice ingeniero nos puede mostrar en vivo la ejecución de ese código pues en vivo  la ejecución de ese código oscar lo vamos a hacer en el laboratorio es más lo van a ir haciendo  ustedes y yo voy a ir poniendo en pantalla todos los códigos que ustedes quieran que probemos yo  lo voy a hacer en la larga y les he dicho porque porque para mí es más sencillo hacerlo o lo  podemos hacer en python si alguno quiere hacer en su ordenador en python lo puede ejecutar en  python se lo quiere hacer en se lo puede hacer en se o no sabe en cualquier lenguaje lo que importa  es que sepan el concepto y no soy ingeniero que significa el parámetro de la función que nos  enseñó que si al último gama gama eso que significa gama es la ley de potencias es el valor\n",
      "Intervalo 40-50 minutos:  de gama que voy a tener vale es decir el parámetro por el cual voy a aplicar esa potencia que está  aquí vale si mi matriz te supo le aplico un valor de promediado y le voy a aplicar esa ley de potencia  decir 0,5 1 2 o menos 0,5 pues ese se gama ese gama que dices aquí es el ro  vale es para eso porque utilizo gama podría haber utilizado ro podría haber utilizado  otro valor es solamente una letra que diera vale  a ver a ver disculpa que te interrumpen este tengo otra reunión en diez minutos pero este  no hemos revisado el tema del proyecto correcto todavía no tenemos un o sea el proyecto dice  el laboratorio el proyecto primero que tenemos que entregar en grupo si tenemos que tenemos una  clase exclusivamente laboratorio y he escrito yo a la coordinadora porque no me la han puesto  debería haber sido hoy a continuación de esta pero no aparece la pregunta la pregunta no es  porque bueno yo estoy aquí el año pasado el año pasado saqué la maestría en cyberseguridad por  lo general las clases que dan son dos semanas antes entonces si vas a recién se va a dar la  clase del cómo hacer el proyecto y la presentación creo que la tenemos en abril el 6 pero el mayo  6 se postergaría entonces tú la entrega de tu proyecto podrías por favor coordinar porque estaría  dentro tendríamos menos de una semana a ver para hacer el proyecto yo lo voy a plantear  pero no lo sé yo lo planteo no es tema de por favor de tomar ventana pero por lo general  las reglas esas las clases se dan se da dos semanas antes de entregar la fecha entrega el  proyecto y en la maestría pasada ocurrió algunas cosas por temas de entiendes conflicto de horario  de los profesores entonces lo que hacían era postergaban 12 dos semanas después de la clase  o sea una vez que tengas las clases que tengamos la clase del proyecto después dos semanas se hace  la entrega entonces por favor coordina para para estar de acuerdo porque tenemos una carga bien  grande el lunes ese lunes que vamos a presentar tres proyectos y primero el tuyo pero no hemos  recibido la dos semanas de tiempo vale pues yo te invito a que le escribas a tu coordinadora  mentora tutora como se diga y comentaselo también porque porque te digo porque le digo lo mismo al  resto porque mientras más somos más nos hacen caso y también somos menos hacen caso entonces  voy a enviar un y yo lo digo y lo hablamos para y yo estoy estoy de acuerdo contigo también  deberían haberlo ahora porque ahí sí estaríamos las dos semanas más o menos no no no no no no  en teoría debería muchísimas gracias a bien me despido tengo otra opinión vamos vamos con  el abrazo con todo saludos bueno adiós vale a ver felipe como seres humanos podemos diseñar  la mejor función a aplicar una imagen para pre procesarla ya que visualizamos las tareas de  interés a ver como seres humanos sí pero va a depender de tu del proyecto que quieras hacer  vale y dependiendo de eso va a depender también los sensores los sensores que utilices o sea  no es no es tan obvio pero necesitas hacer un estudio para poder ver qué métodos vas a necesitar vale  es el que es una constante bueno si la gama bueno aquí está la actividad de uno es para el 6 de mayo  tenemos poco tiempo y los otros trabajos en las otras áreas también están por el 6 de mayo es  que creo que todas están para ese mayo pero ya les he dicho si hablan con vuestros mentores y  demás y dicen miren es que en la clase los que estamos en la clase de javier porque yo  doy para dos aulas percepción computacional y visión artificial que se han juntado pues comentarlo  vale porque deberían la verdad es que deberían haberme puesto una clase ahora y no sé si me lo  van a poner la próxima semana que yo manda un correo para para informar eso pero si lo comentan  entre todos pues quien dice se dice una semana después después yo que sé a lo mejor lo se  desmueve y está yo les digo una cosa por mi parte siempre van a tener toda la ayuda que esté en mis  manos que pueda ser por ustedes vale incluso en el examen vale a que me refiero a que si en la  revisión de exámenes hay que tener más tiempo o ampliar el cupo de alumnos que quieran reclamar  por lo que sea yo lo voy a hacer que sacarme sobre horas porque porque yo he sido alumno igual  que ustedes bueno seguimos vamos a ver lo de operaciones punto a punto pero por el lado de  de la parte sistemática del histograma vale ahora sí que es el histograma de una imagen  seguramente varios de ustedes van a decir esto es lo que pasa cuando yo estoy tastando con mi reflex  y le doy al botoncito de la h y puedo ver cositas si se obtiene a partir de los valores de intensidad  de los píxeles en mi imagen vale me muestra la probabilidad asociada a cada uno de sus niveles y  es vale es aleatoria vale según la intensidad del píxel y esto por ejemplo es lo que se lo pueden  obtener en lo que se ha dicho en vuestras cámaras reflex y demás incluso en los móviles creo que  los iphone son los que tienen esto más bonito puesto creo que los sacan entonces puedo tener  esto a ver que dicen ingenieros ingenieros no vayan es ingeniero oscar soy jabir o sea que  esta no es la clase donde explica la tarea que hay que hacer no este es la del tema hay una  clase exclusivamente para eso para la para la práctica vale y no sé por qué me han puesto  no lo sé entonces si esta es mi imagen normal aquí estamos viendo y yo que se  un tren un coche perdido en medio del desierto vale cuando tengo una exposición de cero y saco su  histograma vale ya me lo hizo vale bueno sí ya me lo hizo los valores de menos 2 a 2 cuando es cero  pues el histograma que presenta pues es este normal vale y se voy aumentando la exposición  pues mi histograma se va yendo hacia la derecha no estoy incrementando exposición lo estoy haciendo  mucho más claro si lo hago más oscuro pues el histograma se va hacia la izquierda tiende hacia  la izquierda entonces mayor derecha menor izquierda quedarse con eso que puede ser un truco muy bueno  para más adelante vale  entonces el principio fundamental de este histograma pues que la variable aleatoria  resultante de transformarla vale va en función de la distribución del tipo el cual tenemos en el  caso del contraste pues el histograma es más distribuido no es menos concentrado está más  expandido vale y por otro lado pues se puede estimar esta función del histograma a partir  de aproximaciones de la función de densidad probabilidad y a partir también de la integral  de la función de densidad probabilidad a través de las funciones de distribución por ejemplo vale  aquí tengo una imagen original vale y este es el histograma resultante si yo trato de ecualizar  la imagen vale varios de ustedes me van a decir vale he cualizado la imagen tiene unas características  muy distintas a la original vale pero la forma es muy similar si les puedo decir que la forma  es muy similar no voy a negar nada pero por un lado si se dan cuenta los valores están como que más  uniformados sean más más achatados si lo quiero ver así no tiene tanto dos picos vale y hay más  distribución no uniforme aquí está muy compactado aquí está más estirado vale entonces este tipo  de resultados de ecualizar y del imagen original guardé celos vale estos ejemplos siempre guardé  celos por posibles aparecimientos en el exam ahora por otro lado  en la imagen que teníamos del niño vale es una si le saco su histograma pues es  un programa que tiene estas características y si trato de ecualizarlo más o menos se me expande un  poco más no se me distribuyen vale esta es una función de python que nos puede venir muy bien  para un tipo de malap que nos puede venir muy bien para hacer el este tipo de ejercicios  el walter que dices profesor el curso es visión artificial 2024 pero el ppt indica  percepción computacional y hoy nos toca el tema de detección y cancelación de alumbrias  no habrá confinición no tema 5  entonces la 37 vale  el tema 5 lo hemos visto la semana pasada\n",
      "Intervalo 50-60 minutos:  el les repito es percepción computacional y visión artificiales hecho porque hay  dos planes que están puestos yo llevo los los dos planes  y es el tema 6  porque el tema 5 ya lo hemos visto la semana pasada alguien que me corrija así es que me he equivocado  hoy es el tema 6  si la primera hicimos dos temas exacto  bueno entre ustedes a ver si darle la oreja a walter y si me equivocado  tirar a mí pero yo creo que vamos bien  vale seguimos con seguimos con esto vale  lo de los histogramas por ejemplo habitación de color naranjita y habitación de color azul vale  entonces como aquí está predominando más los tonos rojos como podemos ver aquí el histograma  está tendiendo más hacia el lado rojo el lado azul es más bajo vale perdón el lado  gris es más bajo y el azul tiene más a la izquierda a perderse hemos dicho que cuando  disminuye se de izquierda y cuando aumenta es la derecha vale entonces en este caso es al revés  tenemos el rojo que se está perdiendo que es azul el azul a la derecha porque está ganando y  el verde pues pichí picha más más que topar izquierda vale quedarse bien con esta parte que  les he explicado porque en el examen a lo mejor les muestro los histogramas y ustedes me tienen  que decir cómo sería la imagen qué colores más intensos tendría y cuáles serían los menos vale  por si acaso ahora pregunta  Juan puedes hablar tranquilamente o sea no hace falta que la mate la mano gracias y gracias  profesor es para tener en claro entonces el histograma lo que nos está mostrando es qué  cantidad de la imagen se encuentra en con un nivel de luz determinada por cada canal si es  interpretación claro en este caso si supuesen esta de grises claro supuesen escala de grises  pues ya no sería por cada canal escala de grises es un solo canal entonces voy a hacer entre  entreprete bien en el caso del rojo de la del primer cuadro lo que estamos mirando es que hay  muy muy pocos colores rojo de baja intensidad lumínica y hay muchos colores rojo con más  intensidad lumínica cierto sea a la derecha son colores rojo con más luz y a la izquierda con menos  y hacia arriba lo que nos dice es la cantidad de puntos en la imagen que cumplen con esa cantidad  de luz ahí está eso creo que finalmente lo entendí después de 44 años de existencia gracias  ya en la vida real para poder analizar estos histogramas tenemos que de ley tener normalizado  los pixeles de la imagen porque yo hacía simple vista si yo tengo una en unos pix digamos podemos  tener pixeles anómalos que me van a distribuir mal el histograma y voy a perder forma o porque  no los veo así amontonados hacia un cierto sector amontonados a otros sectores sector y yo creo que  antes de eso hay un paso de normalización de estandarización de pixeles en un rango a un rango  o siempre claro ahí claro hay una normalización interna que o previa que siempre está presente  vale siempre hay una normalización en este y esa normalización va a depender de la cantidad de  bits que voy a utilizar para para tener  se me ha ido la palabra para cuantificar cada color vale de 0 a 256 ponte  el rango vale siempre tienes que normalizarlo por eso incluso si volvemos un poco hacia atrás  no voy a darle los stars a estas y vemos la imagen de ultrasonido normalmente esas imágenes de  ultrasonido se normalizan que se expresan en decibelios normalmente se normaliza están  normalizadas porque precisamente para evitar lo que ha dicho tú porque si hay pixeles que que se  pierden y demás pues te influyen vale bueno ahora mini pregunta de estos tres canales rgb vale  e  no se tiene mayor entropía porque  alguien me puede resolver o lo dejamos para el foro  el rojo bueno lo dejamos para el foro vale rojo el rojo ah el rojo por qué  porque los colores bajos no tiene casi amplitud el histograma solo tienen las frecuencias altas  muy bien eres carlos no no sé por qué pero me persiguen los carlos la anterior se llama  la anterior cuatro meses también habían dos carlos que eran muy buenos muy buenas respuestas  bueno seguimos que madre mía que ya estamos pasando vale ahora operaciones punto a punto  pero con la parte de aritmética vale aquí me refiero con aplicaciones por colocar operaciones  aritméticas en este caso matriz de 2 por 2 vale quiero multiplicar o sumar cualquier tipo de  función vale en dos dimensiones porque es una imagen sobre esa imagen me dirán pero  para qué me sirve esto pues el mejor ejemplo es para esto  tengo bueno han debido a debido a ver ustedes esos vídeos de tiktok instagram plurio que sé donde  a través de tratar de sacar una imagen a una persona vale ya sea su pareja lo que sea su hija  y quieren quitarse las personas que están detrás entonces lo que comienzan a hacer es  como capturar las imágenes del fondo varias veces en varios tramos o en varias tomas temporales  vale y después en la que mejor cuadra pues se queda la persona y en la que menos persona había  o menos ruido de fondo había pues las ha ido componiendo lo que en realidad hace es hacer  operaciones aritméticas haciendo esa composición de imágenes vale haciendo ese procesado o esa  diferencia temporal de sobre pos un sobre posición de imágenes vale por ejemplo  esta es la siguiente imagen y esta es la que estoy la que procede a esta entonces si hago  una resta de ambas luego la diferencia de movimiento del coche y por lo tanto si quisiese sacar  solamente ese trozo que me ha servido de diferencia pues si me quedaría solamente con ese trocito que  me la tengo aquí y el resto sería en blanco no o en verde como quieran ver vale y por lo tanto teniendo  en cuenta eso pues voy a hacer también una substracción de imágenes y por lo tanto pasa  lo que les dije me quito el fondo y tendría el resultado pues una imagen sin la parte delante  o sin la parte de atrás vale entonces para que sepan esas aplicaciones que que ponen o esos  filtros que ponen tanto en los vídeos y demás de cómo me quito el fondo como quita las personas  que están detrás míos no es más que operaciones aritméticas que me hacen eso dentro de esa app  del nuevo vale ahora a cada uno de los píxeles yo le puedo aplicar las operaciones aritméticas  que yo quiera puedo hacer una suma de valores vale a la imagen puedo colocarle ruidos puedo hacer  todo lo que yo quiera entonces la operación aritmética más sencilla quedarse con la ecuación  ya que está discernido que lo que significa cada uno de los parámetros mi imagen xxb  va a ser igual a 1 partido por m de la sumatoria de cada uno de los píxeles x sub y de la imagen  original que tenga donde a sub x es la relación de los procesos estocásticos puedo tener la imagen  libre de ruido puedo tener ruidos de adaptaciones y demás vale es aplicar operaciones aritméticas  si es colocarle lo que yo quiera cada uno de los píxeles  porque les digo o para qué me sirve esto pues aplicar por ejemplo valores no digamos  aleatorios bueno si si aplicamos valores aleatorios a un imagen original imagines  este es la imagen original y coloco imágenes a unas funciones aleatorias valores aleatorios  a esa imagen utilizando lo que sería un operador aritmético pues tendría un error tendría un  ruido entonces teniendo en cuenta esa idea de si a la original le coloco algo más o menos\n",
      "Intervalo 60-70 minutos:  aleatorio o aleatorio y tengo el ruido para qué aplicación creen que se puede utilizar para sacar  el proceso inverso  es decir tengo mucho ruido y quiero limpiarlo un poco vale se le aplicó no funciona aritmética en  vez de que haga una sumatoria pues me haga una resta por ejemplo para qué me serviría para qué  tipo de aplicaciones a ver alguien que ya llevamos una hora de clase alguien que me lo diga a ver  no es médicas no médicas no porque a lo mejor una parte del ruido puede ser algo que esconder  mensajes esconder mensajes puede ser javier pero jose manuel rodriguez algo después la clavado  en el campo de las unidades generalmente las imágenes son capturadas con niveles muy bajos  con perdón con niveles de luz muy muy bajos causados causando mayor ruido en los sensores  estas técnicas de promedio es muy extendida en este campo sobre todo las técnicas aritméticas  promediado multiplicación suma de operaciones aritméticas vale para la astronomía viene  clavarísimo para que se hagan una idea antes que estén los telescopios ópticos bueno después de  que los telescopios ópticos eran ya la panacea en el mundo y utilizaron los radiotelescopios  encontraron que había muchos problemas de errores de ruido precisamente por la baja cantidad de luz  entonces haciendo estas operaciones aritméticas consiguieron sacar imágenes perfectas y en el  telescopio de un web no se equivoco pues utilizan bastantes operaciones aritméticas creo que en  parte del código es open source que está activo que está puesto en github si más no equivoco  cierren lo encuentra que lo comparten el foro pero estoy seguro que han compartido el corre porque  lo han hecho para precisamente hacer todo este tipo de filtrados o pre-processamiento de imágenes  a ver vamos algo rápido implementaciones  la herramienta de python que nos va a ayudar a hacer todas las implementaciones que hemos visto  pero de cajón es python  vale tiene el redscale intensity y tiene el ecuálice histograma vale esas las tiene las  dos funciones que les las he puesto ahí vienen ya clavadísimas las demás a lo mejor hace dos  operaciones en promedias o hace o tres yo que sé pero python utilizando la librería de sticky  pues ya tienes esas dos y te las hace clavadas y ya te maneja bastante vale en el laboratorio  vamos a revisar más que todas las de python más digo las demás precisamente porque quiero que  vean los pasos que se siguen no el encapsulado final y óptimo que te da paile vale más que todo  ahora la siguiente pregunta que les voy a hacer el que el primero en que me la responda en el foro  a ver no le puedo dar un punto porque sí pero como siempre les digo pues le ayudaré bastante en el  examen si es que tenía algún problema o en el laboratorio si tenía un problema vale necesito  que me diga de qué color es este vestido vale una ayuda lo pueden usar pueden utilizar o  haciendo operaciones logarítmicas o exponenciales o la de potencia vale entonces cuando lo pase a  pdf y lo cuelgue esta noche en el portal pues van a tener la imagen y les pediría que lo analicen  y me digan qué color es y nada para la próxima clase que lo que vamos a ver  o que lo que es la pregunta que nos vamos a hacer se ve negro ya pero no es negro  la pregunta que nos vamos a hacer para la próxima clase y la de estamos ahora como siempre es  cuál es la consideración de la información espacial en los procesamientos de imágenes y  cómo puede optimizar los resultados en aplicaciones como el reconocimiento de objetos y la segmentación  en las regiones de interés vale vamos a ver qué cosas vamos a poder sacar a partir de esta de esta  pregunta vale dice café oscuro azul cuando cuando lo pasen os vais a caer de la silla  porque no se lo van a creer y con todo esto preguntas dudas  vale necesito que me las consultéis ahora que ya nos hemos pasado que ya son las 5 y cuarto ya  va a ser aquí en españa dorado no se dorado a lo mejor no tiene vestido yo que sé es una técnica  esa técnica vale para lo del vestido que si el blanco es azul a ver no sé de qué color es pero  me tiene que decir que colores en perú las días van aquí la haciendo  alguna duda profe con este caso por ejemplo en la imagen que nos compartió la imagen  si por ejemplo ahí la imagen está pegada por ejemplo en un ppt pero he tenido la duda de si  estas imágenes tienen la información de sus píxeles únicamente en la imagen original o si  por ejemplo de una captura de una imagen también podemos realizar esas operaciones de intensidad  manteniendo  ten en cuenta luis cuando tu capturas imágenes la imagen vas a obtener todo este informe toda  esta información vale o sea vas a tener el valor del píxel que ves en la pantalla ahora mismo  no el mismo valor no pero digamos que es el mismo vale pero no tiene más información sigue siendo  una imagen es una imagen es una imagen a esto le colocan los filtros todos los que quieras y te  vas a dar cuenta lo que esconde la imagen por eso es que la anterior clase cuando uno de vuestros  compañeros me dijo esta clase de división artificial porque estamos viendo señales porque  estamos viendo porque no vemos imágenes y demás yo le dije mira antes de correr tendremos que  caminar y demás antes de volar correr no me acuerdo que le dije era precisamente porque yo  les estaba enfocando esas semanas atrás a que todas las señales en una dimensión que estábamos  viendo que eran valores vale que formaban parte de un conjunto de una señal pues esa señal si lo  pasas a dos dimensiones es esta imagen entonces si aplicas cada una de estas cosas a cada uno de  estos puntos es esa información que tienes son imágenes que tienen esa información ahora otra  cosa es que te esté colocando una imagen de un ultrasonido donde tú aplicas las mismas características  pero hay un canal que no se comparte y ese canal que no se comparte es un poco la intensidad que  tiene el ultrasonido la potencia que tiene tú aquí ahora mismo si capturas la imagen vas a ver rgb  o mira si el mejor ejemplo es este más que el ultrasonido esto es este es el mejor ejemplo  aquí estás viendo la imagen del espacio es rgb bueno escala de grises suponta que seré  gb los tres canales nada más pero al ser espacio tienes un canal más que es como si estuviéramos  hablando de el tipo de archivos tif que te da una posición en coordenadas del espacio o si es  una imagen satelital es una posición en las coordenadas sovídas y cas entonces ese canal  no lo tienes vale para un estudio muy muy exacto o sea muy analizando bien ese tipo de imágenes  si no tienes esa este otro cuarto valor por así decirlo sólo tienes una imagen de una fotografía  pero no tienes una imagen que referencia entonces lo que estamos analizando aquí es  netamente imágenes tal cual no están que referenciadas ni que analizadas en potencia  que serían las partes médicas vale eso es otra cosa la materia azul como lila  pero no lo ya me lo dirás no sé quién habló pero ya me lo dirás me lo pones en el foro entonces  cuando estamos hablando de computer vision estamos hablando de análisis de imágenes  en el caso de la materia percepción computacional o visión artificial estamos viendo imágenes ya un  poco más adelante vamos a agregar a lo mejor este canal que nos falta para hacer un trabajo un estudio  una pequeña práctica yo que sé vale para que se den cuenta que cada uno de los valores de los  píxeles rgb por ejemplo pues tienen su otro valor georeferenciado que me va a dar más información  de eso y el tratamiento viene a ser lo mismo sólo que éste no se tocaría pero me serviría para  referenciar los respecto al resto entonces en este caso no estaríamos perdiendo esas señales que  dices o sí o no perdiendo sino no contando con esas señales vale  no sé alguna otra duda consulta antes de que a mí la directora del máster me ha tirado las orejas\n",
      "Intervalo 70-80 minutos:  siempre me paso más de no pues nada si no hay más dudas comentarme ahora  sí podría ser la posición javier la posición de la fuente de luz o la posición del objeto si  hablamos ya de técnicas de imágenes de drones o de aviones o tripulados o militares lo que quieras  ver los satélites esa otra sería la georeferenciada respecto a las medidas atlona longitud latitud  altura geodésicas de la tierra vale el vestido al ser este hoy que no sé qué decirte  pues nada si no hay más dudas lo dejamos ahí  y ya nos hemos vuelto a pasar vamos a dejar compartir  vale lo paramos\n"
     ]
    }
   ],
   "source": [
    "current_interval = 0\n",
    "accumulated_text = \"\"\n",
    "\n",
    "for segment in result5[\"segments\"]:\n",
    "    start = convert_to_min_sec(int(segment[\"start\"]))\n",
    "    end = convert_to_min_sec(int(segment[\"end\"]))\n",
    "    text = segment[\"text\"]\n",
    "\n",
    "    # Verificar si el segmento sigue dentro del intervalo actual de 15 minutos.\n",
    "    if start // n == current_interval:\n",
    "        accumulated_text += \" \" + text\n",
    "    else:\n",
    "        # Imprimir el texto acumulado para el intervalo actual.\n",
    "        print(f\"Intervalo {current_interval*n}-{(current_interval+1)*n} minutos: {accumulated_text}\")\n",
    "\n",
    "        # Actualizar el intervalo y reiniciar el texto acumulado.\n",
    "        current_interval = start // n\n",
    "        accumulated_text = text\n",
    "\n",
    "# No olvidar imprimir el último intervalo acumulado fuera del ciclo.\n",
    "print(f\"Intervalo {current_interval*n}-{(current_interval+1)*n} minutos: {accumulated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "result6 = model.transcribe(\"/home/contrerasnetk/Documents/Classes/VisionArtificial/6.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intervalo 0-10 minutos:   Grabamos.  Aprendemos pantalla.  Vale, el chat.  Se ha escuchado, se ve perfecto. Buenas tardes.  Antes de empezar, ya sabéis que el próximo lunes tenemos clase de laboratorio y se ha extendido hasta el día 17.  Vale.  Entonces, vale. Lo podéis ver, lo podéis oír.  ¿Sí?  Eso bien se escucha perfecto. Vale, lo del laboratorio ya sabéis el próximo lunes.  Y vamos para allá.  Es un tema muy bonito, a mí me gusta.  Vamos a ver un poco de teoría y práctica tapio y nominamente, como siempre.  Pero me gusta porque voy a sacarles algunos proyectos que ustedes se den cuenta, proyectos o aplicaciones de uso.  Vale.  Entonces, vamos con el tema 7 que es procesamiento de imagen, operaciones espaciales.  Espaciales del espacio. Vale.  Pues debería aparecer la sesión de lunes, a mí ya me aparece, Diego.  Deberías tenerlas. Vale.  Así que revisarlas, actualizarlas. A lo mejor les aparecerán las próximas horas, yo creo, pero yo ya tengo la clase.  Pues nada.  A ver, para recordar lo de la semana pasada, lo que hicimos era tratar de poder mejorar un poco lo que serían las imágenes.  Bueno, un poco.  Hacer ese filtrado que decíamos, ¿no?  Toda la imagen como tal.  Vale.  Un tipo de filtrado que era de mediana, de media y demás, donde podíamos sacar aquellas características un poco o centrarnos aquí en las características más importantes y poderlas realzar.  ¿No?  Entonces, con lo que hemos visto anteriormente, ahora nos preguntamos cómo es que la consideración de la información espacial en el procesamiento de imágenes, espacial, mírame ese, el espacio de una imagen.  Vale.  X y Z.  Vale.  Todo esto se puede optimizar resultados en aplicaciones en el reconocimiento de objetos, sobre todo en la segmentación de regiones de interés.  O sea, ¿cómo lo vamos a poder hacer?  ¿Cómo vamos a poder discernir algunos detalles al respecto?  Entonces, con todo esto, lo que vamos a hacer, vamos a hablar sobre una introducción, unos objetivos, unos operadores espaciales, que son los filtros paso alto, paso bajo.  Vamos a hacer la parte de detección de bordes y algunos ejemplos de uso.  Vale.  En la introducción, tenemos que tener en cuenta lo que es el realce de estructuras en una imagen, ¿vale?  Como una etapa de preprocesado.  Entonces viene a ser el resaltar los elementos de interés, identificar bordes, el ruido de captación, ¿vale?  O sea, lo que sería preparar una imagen para etapas posteriores, ¿no?  Para las nuevas operaciones de procesado o para una inspección visual de un observador humano, porque sería el data curation.  Entonces, estamos ahora mismo en ese punto, ¿vale?  En este preprocesado de imagen, vamos a distinguirlo de dos formas.  Uno, el dominio espacial, donde son operadores punto a punto, que lo hemos visto, es el tema anterior, operaciones elementales.  Y el de ahora son operadores espaciales, este tema 7, ¿vale?  Y el siguiente preprocesado tendría a ser con el filtrado de Fourier, en el dominio de las frecuencias.  Ese sería el tema 9, ¿vale?  Para adelantarnos un poquito.  ¿Vale? Entonces, hemos tenido una clase de captura de información.  ¿Qué sensores íbamos a hacer? ¿Cómo los íbamos a capturar? ¿Cómo íbamos a digitalizarlos?  ¿Cómo íbamos a darle los valores?  Toda esa parte, estamos ahora mismo en la...  Hemos visto ya el preprocesamiento.  Estamos incluso ahora tocando todavía una parte del procesamiento y vamos a ver lo que es la segmentación y el filtrado, ¿vale?  Para posteriormente irnos a la distracción de características y a la toma de decisiones.  Estamos, por así decirlo, casi en la mitad, por así decirlo, ¿vale?  Entonces, con todo esto, recordando nuevamente el valor del operador punto a punto, pues me cogía un pixel de la imagen, x y z, ¿vale?  Que es una función única, la cogía, la multiplicaba por un...  Por una escala T, una función aritmética, geométrica, que la habíamos descrito anteriormente en la clase anterior.  Y teníamos el pixel de salida, que hay un pixel trasfondo.  ¿Vale? Entonces, con todo esto, esto era el operador punto a punto.  ¿Cómo sería?  Disculpenme, estoy con alergia, o sea que estoy un poco, digamos, aca atarado, pero es síntoma alérgico, ¿vale?  Con todo esto, ¿qué es lo que vamos a hacer en el operador espacial?  Pues el operador espacial es el valor del pixel resultante en función a la imagen original, ¿vale?  Pero implica ahora la vecindad este, en este caso un conjunto de puntos o de pixels alrededor de este.  Entonces, si cojo un punto cualquiera, mi imagen original, lo voy a aplicar por la operación T,  pero aparte de ese punto cualquiera voy a tener aquellos vecinos cercanos, ¿vale?  Que me van a ayudar a dar esas características necesarias para dar el nuevo valor de la imagen de salida, ¿vale?  Es decir, que el pixel es correspondiente a la vecindad.  ¿Va a tener un grado de complejidad? Sí, ¿vale?  Pero ahora vamos a ver para qué nos va a servir el hecho de que tengamos la influencia de los pixels cercanos.  Para hacer, por así decirlo, el how to o el to do de hacer esta operación de este operador espacial,  pues voy a seleccionar un punto, ¿vale? Ya me sé, en el centro, en X y Y.  Voy a llevar a cabo una operación T que me va a involucrar los pixels que se encuentran en la vecindad o que son vecinos, este punto central.  Y el resultado de dicha operación es de la respuesta a dicho punto, lo que hemos visto anterior, ¿vale?  Y voy a repetir eso por cada uno de los pixels de la imagen, ¿vale?  Para tener lo que es el procesado según vecindad y el filtrado espacial, ¿vale?  Ahora, para estos operadores espaciales, pues tenemos el tipo lineal, ¿vale?  Los operadores lineales de interés consisten en multiplicar cada pixel de mi vecindario, ¿vale?  De mi pixel, lo que está alrededor, por el correspondiente coeficiente.  Y sumar para obtener una respuesta del punto central, del nuevo valor que voy a tener, ¿vale?  Ahora, los coeficientes, ¿vale?  Lo que voy a tener, este T de coeficientes, es una matriz llamada filtro, máscara, kernel o ventana.  Podría llamarle cualquiera.  Yo algunas veces le llamo el filtro, algunas veces le llamo el kernel, ¿vale?  Quédense con eso, ¿vale?  Esto es importante que se den cuenta.  En este caso, en principio sería 3 por 3, pero puede variar.  Ahora, si mi imagen original tiene las características, este sería el punto del origen, ¿vale?  Tengo las dimensiones x y, ¿vale?  Y tengo mi punto x y en el espacio, ¿vale?  La ventana que voy a tener, que es esta, a partir del coeficiente central,  Cada uno de estos valores, están viendo x menos 1 y menos 1, y hasta x1 y 1, ¿vale?  En x y en y, ¿vale?  Entonces, cada uno de estos valores me va a servir para poder enmascarar, ¿sí?  Para crear mi máscara, ¿sí?  Esta máscara ahora vamos a encontrar qué tipos de máscara hay,  pero estoy recorriendo yo cada uno de los puntos, ¿vale?  Voy desde el menos 1, 1, menos 1, menos 1, hasta el 1, 1, ¿vale?  Ahora, si mi tamaño de imagen o de máscara es m, n,  pues tengo esta fórmula, para m y para n, ¿sí?  Donde a y b son los enteros positivos, que serían cada uno de los puntos de la imagen, ¿vale?  Quédense con esto, ¿vale?  Ahora mismo van a decir, pero ¿cuál es la máscara?  Ya la vamos a ir definiendo, pero quédense con el concepto de cómo debería ir recorriéndola así.  ¿Este es de negativo o está positivo?  Por aquí, por aquí y por aquí, ¿vale?  Ahora, también existe el tipo lineal, pero de correlación.  En este caso, para una función x y, normalmente, como antes he dicho,  una doble sumatoria de todos los elementos que van a ir correspondiendo a este filtro o a este kernel, ¿vale?  Donde tenemos estos valores, ¿vale?  En el ejemplo anterior que teníamos, en el caso de a1 y b igual a1,  pues tenemos una máscara de 3 por 3, pues tendría estas características, ¿vale?  Quédense con la ecuación, ¿vale?  Quédense con la ecuación, pero esto sería rellenar estos valores.  Para desde el menos 1 hasta el más 1, ¿vale?  En la máscara, por la función x y, ¿vale?  Ahora, también existe el operador espacial de convolución.  Es lo mismo, la única diferencia.  En vez de sumas, son restas.  Y para llevarlo a esto, a una forma más lineal, tenemos esta otra ecuación.  Si se dan cuenta, es lo mismo, pero cambian los signos, ¿vale?  Cambian los signos.\n",
      "Intervalo 10-20 minutos:  En la correlación es negativo y en la convolución positiva es aquí negativa, ¿vale?  Cambian los signos. Quedarse con eso, ¿vale?  Nada más.  Cosa de que cuando queramos implementar esto,  en MATLAB, en Python, en C, en Cobol, en lo que quiera,  es solamente seguir esta función, ¿vale?  Para hacer un filtro, ¿vale?  Ahora, con todo esto, ¿vale?  Les he dicho, teniendo en cuenta el filtrado espacial o lineal, ¿vale?  Existen dos conceptos.  La correlación, que es el proceso de desplazar el filtro a lo largo de la imagen, ¿vale?  Y calcular la suma de los productos de cada posición X y de cada punto de la imagen.  Y la convolución, que es un mecanismo similar a la correlación,  con la diferencia de que el filtro o el kernel está rotado 180 grados  antes de iniciar el proceso.  Es decir, si antes...  Tengamos el móvil aquí, ¿vale?  Yo creo que se ve mejor así.  Si antes mi filtro no iba a ir así, ¿vale?  Para hacer la convolución, lo que estoy haciendo es girarlo 180 grados.  Y va a seguir en la misma trayectoria, ¿vale?  Pero, ¿qué sucede?  Solamente al girarlo, ¿vale?  Cada uno de los valores del kernel, pues me va a dar un resultado distinto de la convolución.  Y eso ya me influye.  Y van a ver por qué.  ¿Vale?  Hasta aquí, ¿todo bien?  Bueno, yo creo que todo bien.  Vale.  Antes de seguir, algunas aclaraciones.  Termino de filtro, nos referimos a un operador espacial.  No confundir con filtros de frecuencia, los filtros digitales y demás.  No, no.  Estamos hablando ahora de filtro espacial, ¿vale?  Domino espacial no es lo mismo que domino de frecuencia.  Domino espacial es en el espacio de la imagen.  En los puntos X, Y y Z.  Si hubiese un cubo de puntos, por ejemplo, en imágenes médicas  o en imágenes astrofísicas, ¿vale?  Entonces, los píxeles cercanos al borde, ¿vale?  La distancia va a ser menor a P menos 1 partido entre 2 por cada máscara de P por P.  ¿Vale?  De 3 por 3, de 4 por 4, de 2 por 2, etc.  ¿Vale?  Necesitamos procesar solamente los píxeles a una distancia mayor a esta matriz, ¿vale?  Y hay que tener en cuenta que los píxeles cubiertos por la máscara, por el kernel,  tienen el mismo tamaño, ¿vale?  Y después el padding, es decir, añadir ceros, ¿vale?  Afilar las columnas con un valor 0, ¿vale?  Porque vamos a llegar a los bordes, ¿sí?  Nadie me ha dicho nada de los bordes.  Pues cuando llegamos a los bordes, pues tenemos los paddings,  que tenemos agregar ceros, ¿vale?  Con todo esto que hemos visto de introducción, pues nos vamos a marcar unos objetivos, ¿vale?  Vamos a ver más o menos lo que sería el operador espacial versus el punto a punto,  aunque ya nos hemos mencionado antes, pero vamos a sacar ahí un ejemplillo.  Vamos a conocer cómo aplicar estas técnicas de filtrado espacial sobre una imagen,  mediante la operación de los operadores en el dominio espacial, ¿vale?  Vamos a conocer algunas técnicas de detección de bordes, ¿vale?  Los operadores espaciales, obviamente, y el algoritmo Kani.  El algoritmo Kani es un algoritmo el más utilizado ahora mismo en la literatura,  y cuando apareció fue el boom, el no va más, hace unos, bueno,  más de 5 o 10 años atrás, que fue igual que el segmentation anything,  que fue un algoritmo que fue rompedor, fue disruptivo, pues el Kani hizo lo mismo, ¿vale?  Entonces nos vamos a centrar en el Kani porque es el más utilizado.  Vamos a ver estas técnicas de filtrado, ¿vale?  Volviendo al contexto anterior, pues hemos dicho la correlación,  desplazar el filtro a lo largo de la imagen.  La convolución, lo mismo, pero la diferencia es que la máscara está rotada a 180 grados, ¿vale?  Entonces con todo esto, pues, la función de correlación de un filtro,  pues es el desplazamiento del filtro, ¿vale?  Si este es mi filtro y esta es mi imagen, pues cada vez que voy pasando  cada uno de estos valores, pues voy a tener un distinto valor, ¿no?  Aquí son ceros y justo cuando mi filtro es el valor a 1,  aquí es cuando pasa todos los valores de mi máscara o de mi VW, ¿vale?  De mi ventana. Entonces como mi ventana es 1, 2, 3, 2, 8,  o está rotada, pues 8, 2, 3, 2, 1, todo mismo, ¿vale?  Entonces la correlación del filtro VW, ¿vale?  De una función F en los puntos es el resultado de copiar lo mismo, pero rotado.  Están viendo aquí cómo lo he sacado.  Ahora, esta función F que es igual 00001000 es una función delta de Dirac.  Es un pulso, es un pulcito que tengo, ¿vale?  En la parte, a ver, los que sean electrónicos o los que son físicos,  saben ese pulso de Dirac. Así como hay una función escalón,  hay una función triangular, una cuadrada y demás,  hay una función pulso o delta de Dirac que es un punto hacia arriba, ¿vale?  Es esta, ¿vale? Para que se ubiquen que filtrar no es más que multiplicar  dos funciones que ya se los he comentado, creo que la segunda, la tercera clase.  Y mi función es 1 y el resto son ceros. ¿Ceros y valor?  Entonces, con todo esto, la correlación con una función de impulso me va a devolver  la imagen rotada, ¿vale? Sobre la posición del impulso.  Después, la convolución de una función de impulso me va a devolver la función  en la posición del impulso, obviamente, ¿vale?  Y después, el carné de convolución es un nuevo de correlación, pero rotado 180 grados.  ¿No? Diferencias principales entre estas dos, entre W1 y W2,  que son los filtros, pues, lo tenemos aquí. Podemos tener la propiedad asociativa,  por así decirlo, donde podemos asociar W1 por W2 y multiplicarlo por F, ¿vale?  Con esto, vamos a ver las técnicas de filtrado que podemos ver.  Por ejemplo, tenemos una imagen que tiene estos valores.  Ya me sé, yo qué sé, escala de grises. Una imagen con escala de grises.  Desde 0 que sea blanco hasta 255 que sea negro o al revés, da igual, ¿vale?  Y tengo un carné, ¿vale? El carné es la ventana.  Ahora, quiero sacar los valores justamente en el último,  en la última matriz de 3 por 3, como vemos aquí.  Para hacer esto, ¿qué es lo que tengo que hacer primero? Tengo que rotarlo.  Tengo esto, lo voy a rotar. Entonces, tengo 294,  294, 753, 618. El carné está ahí afuera, ¿vale?  Y los valores iniciales de mi imagen es 1, 8, 15 y demás.  Ahora, cuando adopto la convolución, pues, a esta posición que hemos dicho,  pues, multiplico 1 por 2, 8 por 9, 15 por 4, etcétera, ¿vale?  Hasta tener un valor de 575. Ese sería el valor de mi pixel central,  de mi pixel en la posición 2,5. Si lo quiero hacer aquí,  pues, sería recorrer esta ventana, obviamente, rotada a 180 grados,  y tendría 24 por 2, 1 por 9 y 8 por 15, y así sucesivamente.  Y el valor resultante que tenga aquí, pues, será ese valor, ese filtro, ¿vale?  Seguimos. Entonces, recordad que en el filtrado en dominio espacial  no es necesario transformar la imagen al dominio de la frecuencia, ¿vale?  No tenemos que hacer ningún filtrado o ninguna transformación de frecuencia.  Y por otro lado, el filtro opera directamente solo sobre los pixels de la imagen,  es decir, sobre cada uno de los puntos de la imagen.  ¿Qué dice? El rotado del filtro en la convolución si hace por algún motivo en especial,  es decir, produce algún beneficio. Quédate que si no lo filtras es el correlativo, ¿vale?  Si no lo rotas es de correlación, y si lo rotas es de convolución, ¿vale?  Quédate solo con eso. Ahora vamos a ir llegando a los beneficios, ¿vale?  Entonces, quedarse con lo que he dicho y quedarse con esto.  El filtro opera directamente sobre los pixels de la imagen.  No estoy haciendo ninguna transformación, estoy trabajando sobre la imagen.  Ahora, algo que tiene que tener en cuenta es que este valor de 14, por ejemplo,  se ha transformado a 575. Entonces, estoy subiendo los valores.  Pero estoy subiendo, voy a subir los valores de toda la imagen,  pero el hecho de multiplicarlo por el kernel, ¿vale?  Va a ser que esa imagen tenga unas características distintas.  Ya quitando el valor que en vez de 14 es 575 y el resto de valores a subir,  va a variar, pero me va a dar una alegría al final porque estoy procesando la imagen.  Entonces, quedarse con eso todavía.  Tengamos una aproximación intuitiva, ¿vale?  ¿Qué operación matemática se ha de utilizar para implementar los filtros  en el domingo de espacial?  Pues por un lado, promediado, integrado.  Elimina variaciones bruscas, ¿vale?  Me va a atenuar el contraste de los píxeles que están próximos entre sí.  Es decir, que las frecuencias altas de la señal se las va a cargar,\n",
      "Intervalo 20-30 minutos:  las va a filtrar. Sería un filtro paso bajo.  Y la subtractión que vendría a ser la derivada,  me va a resaltar esas diferencias o cambios bruscos.  Va a mejorar el contraste y va a atenuar los valores similares de intensidad,  lo que sería quitarse las frecuencias bajas. Es un filtro paso alto.  Entonces, ya estamos viendo que hay dos tipos de filtrado.  Con todo esto, por ejemplo, de estas tres imágenes,  díganme cuál es la imagen original.  El A, obviamente.  ¿Y cuál es la B? ¿Cuál sería?  ¿Será un promediado o un sustractivo?  O sea, uno que promedia o que baja.  Promediado, integrado. ¿Funcionaría como promedio móvil de una serie?  Bueno, sí.  ¿El B cuál sería? ¿El sustractivo, el que sustrae o el que promedia?  ¿Cuál es el B? Sustractivo, vale. ¿Y el C?  El promediado. Perfecto.  Ahora, ¿se están dando el B el que promedia?  ¿Y el C el que sustrae? ¿Y por qué no al revés?  Vamos con el concepto nuevamente.  Promediado. Elimina las variaciones bruscas.  Atenúa el contraste entre los píxeles.  Frecuencias altas de la señal.  El promediado. Elimina variaciones bruscas.  Llámese bordes.  Y el de substracción. Resalta las diferencias o cambios bruscos.  Enlace el contraste y atenúa valores similares de intensidad entre píxeles.  Vale.  Entonces, el C es el promediado y el B el que sustrae.  ¿No?  El hecho que me quite las variaciones significa que voy a tener como un suavizado.  Y en el otro caso voy a tener como un resaltado de esas variaciones.  Si se dan cuenta, no voy a entrar a más detalles, en uno me están mostrando más los bordes.  Porque si vemos la imagen original, aquí podemos ver uno de los cables de tensión y demás, pero no veo los cables de tensión verticales.  Pero aquí los estoy viendo.  Entonces estos filtros me están ayudando a resaltar esos detalles que no los podía ver en lo original.  Y por otro lado, el otro tipo de filtrado, pues lo que ha hecho ha sido como que suavizar.  Vale.  Entonces si tengo muchos cambios bruscos y me los ha suavizado, pues voy a tener una imagen más...  ¿Cómo les digo? Más suavizada, más de gusto al ojo.  En el caso de que tenga una imagen que tenga muchos errores o mucho ruido.  Con todo esto, el filtro paso bajo en el dominio espacial.  El filtro paso bajo es el suavizado de la imagen, el smoothing, como se llama.  Esta va a reemplazar el valor de intensidad de un pixel por el promedio de los pixels vecinos.  La vecindad, el tamaño de la máscara, en este caso 3x3, porque lo he puesto yo de ejemplo, pero podría ser mayor.  Donde la suma de todos los coeficientes va a ser 1.  Si se dan cuenta, si esta es mi imagen normal, si multiplico 1 partido entre 9,  en mi máscara todos van a sumar 1.  Si este es mi máscara y multiplico por 1 partido entre 16, voy a sumar.  Todos los valores van a sumar.  En mi filtro paso bajo, imagen de la luna espacial, creo que es del 90 y pico.  Este es el original.  Aquí le estoy aplicando un filtro más o menos de un noveno.  Y de aquí de un 16.  Díganme cuál de los dos se parece más al original.  Díganme cuál de los dos se parece más al original.  A ver, cuál de los dos se parece más.  El C, ¿por qué el C? ¿Y por qué no el B?  ¿Y por qué no este?  Mantiene más luzbordes.  O sea, Manuel, o lo has acertado, ha sido un Homer Simpson, un Homero Simpson,  que ha tenido chilipa, pero sí, es que me mantiene más luzbordes.  Entonces, este mantiene un poco más luzbordes.  Entonces, el que tenga menos solizados me va a depender de esto, porque voy partiendo.  Entonces, me sirve para poder hacer ese filtrado.  Si los bordes no fuesen más que errores en la imagen,  dame una foto muy antigua, muy retocada o lo que sea,  pues me va a filtrar y va a tener mejor resultado.  ¿Se acuerdan la imagen del niño de la anterior clase?  Estaría más suavizado, se notaría mejor.  Eso sería el filtro paso.  En contraposición, el filtro paso alto, dominio espacial, también llamado el sharpening.  Una vez es smoothing y la otra es sharpening.  Se basa en la aproximación de la derivada de primera y segunda orden en señales discretas.  Ya sabemos lo que es discretizar una señal.  En primer orden sería la gradiente.  Y en segundo orden sería la apreciación, las derivadas.  ¿La derivada de la primera o la segunda?  Generalmente la derivada de la segunda orden es más sensible a los cambios bruscos.  Es decir, el aplaciano es más sensible a los cambios bruscos.  ¿Qué significa que sea más sensible a los cambios bruscos?  Detección de líneas, de contornos.  El filtro paso alto entonces.  Tiene los operadores gradiente y aplaciano.  Los coeficientes van a ser igual a cero.  Menos uno más uno, sí.  Menos uno más uno, uno.  Menos uno más uno, sí.  Todo es cero.  Entonces, este sería por ejemplo un filtrito.  Menos uno más uno, más uno, más uno.  Y ocho, cero.  Los coeficientes de aquí, cero.  Y de aquí, también.  Entonces, ya tenemos kernels que pueden tener esos valores.  ¿Sí?  Ahora, con este filtro paso alto.  Ya que he dicho que todos los coeficientes van a ser cero.  Y tengo mi vecindad, por ejemplo, 3x3.  Ha perecido agente tipo Sobel.  Que utiliza estos coeficientes.  Esta sería su función en i, en x.  Para poder filtrar mi imagen.  Y es un kernel fijo.  A ver si he dicho a un hombre que ha dicho, mira, yo le voy a poner esto.  A ver qué resultado sale.  Pues esa persona es Sobel.  Que ha sacado ese filtro.  Ahora.  En la imagen original.  Que tenía de la luna.  Pues le voy a aplicar un laplaceno con estas características.  Y el resultado es este.  Aquí estoy detectando más los contornos que otra cosa.  Pero contornos, pero todavía con mucho error.  ¿Saben?  Ahora.  En la imagen de aquí abajo es un laplaceno.  ¿Vale?  Con esta máscara, con este kernel.  Y me da un resultado mejor.  ¿Vale?  Y esta pues es un rescalado de la imagen del laplaceno.  ¿Vale?  Estirando, jugando con los valores y he sacado un ruido.  ¿Vale?  Pero es para que se den cuenta como.  El hecho de tener una máscara u otra.  Me sirve para obtener imágenes distintas.  ¿Vale?  Por ejemplo.  Mi imagen original.  Y aquí le he puesto una máscara de esta de Sobel.  Si se dan cuenta.  Estoy obteniendo más las líneas.  Los contornos.  ¿Vale?  Ahora.  Si este Sobel lo giro.  Estoy detecto más los otros contornos.  Uno los verticales y otro los horizontales.  ¿Vale?  No sé si se dan cuenta.  La magia de multiplicar por un kernel y después rotar.  Lo he rotado.  ¿Vale?  Ahora, ¿y cómo hago con los de 45 grados?  Pues.  O 135.  Yo qué sé.  Pues si esta es mi imagen original.  Pues con este kernel.  Pues tendré.  Un poco más detalladas las imágenes a 135.  ¿Vale?  Que tendré esta imagen.  Y con este kernel.  Con la misma imagen.  Pues obtengo mejores bordes a 45.  ¿Vale?  Y solo es rotar.  Si se dan cuenta.  Es solamente.  Solamente estoy rotando la imagen.  Digo el kernel.  ¿Vale?  Y estoy jugando con la imagen.  Y estoy obteniendo.  Los bordes.  ¿Vale?  Eh profesor me perdí un poco en.  En cómo hace referencia al paso.  No me acuerdo muy bien.\n",
      "Intervalo 30-40 minutos:  El paso alto.  La integral.  Y el paso bajo.  La derivada.  Yo no he hablado de integral ni derivada.  He visto solamente.  Ahí tenía.  Las derivadas.  Solo he visto derivadas.  No puedes.  Que sí.  Pero.  En una para el alto.  En un slide.  Ahí decía integral y derivada.  Entre paréntesis.  Por eso.  Pregunto.  Vale.  Tú quédate con el paso alto.  Ahora mismo.  ¿Vale?  Es que tú te estás haciendo hasta atrás.  ¿Vale?  Y yo no quiero irme hasta atrás.  ¿Vale?  ¿Por qué?  Porque lo que más prima.  Lo que más prima.  Es que tengas en cuenta.  Las gradientes y la placina.  Que es la primera derivada y la segunda derivada.  Del paso alto.  ¿Vale?  Tú ten en cuenta esto.  ¿Por qué?  ¿Por qué te digo que no te metas tanto con la integral ahora mismo?  No vale la pena.  Y ya lo vamos a ver más adelante.  Incluso en clases posteriores.  ¿Vale?  Lo que quiero que se den cuenta es cómo.  Es más.  Es más.  Sin aplicar.  Todo esto a la función.  Porque no estoy aplicando esa función.  Estoy haciendo un filetado espacial.  Esto es la forma fórmula matemática.  Que siempre le digo.  Quédase con la parte matemática.  Pero aquí.  Estoy aplicando.  En esta por ejemplo.  A esta.  Marea de puntos de mi imagen.  ¿Vale?  Estoy multiplicando por esta máscara.  Y estoy obteniendo este resultado con estos bordes.  Y si la roto.  Obtengo esto.  También estoy hablando de filetados espaciales.  Estoy haciendo solo multiplicaciones.  Si quieres.  Lo que hemos visto aquí.  A ver.  A ver para aquí atrás.  Estoy haciendo esto.  ¿Vale?  Estoy haciendo esto.  ¿Sí?  La operación matemática ahora mismo.  Ya les he dicho.  Guárdense la.  Solo quiero que vean.  ¿Cómo?  Es el hecho de.  Espacialmente multiplicar.  Una matriz.  Por un kernel.  Voy sacando eso.  Obviamente el kernel va recorriendo.  ¿Sí?  ¿Y qué es lo que pasa?  El filtro paso alto.  Que es el que más me gusta.  Me está sacando.  Los contornos.  Los bordes.  ¿Sí?  Con la detección de bordes.  Tú ahora mismo.  Con lo que te he dicho.  Con lo que he dicho hasta aquí.  Dime.  ¿En qué te puede servir la detección de bordes?  ¿Qué aplicaciones puedes tener con una aplicación.  Con detección de bordes.  A ver que alguien me diga.  ¿Por qué me va a servir de detectar bordes?  ¿Nadie?  Limitar áreas.  Vale.  Pero.  Para.  Una aplicación.  Desenfocar figuras.  No.  La calidad.  Calidad de telas.  No sé.  Podría ser.  Calidad de telas.  Pero si te muestro esto.  ¿Qué estás detectando?  Si muestras eso podríamos detectar el borde de.  Del dedo.  No.  Lo que estamos al frente.  La huella.  La huella.  La huella.  Entonces.  Sin necesidad de irme a las redes neuronales.  Estoy detectando ya.  Los contornos de la huella de áquila.  Es decir.  Y esto.  Aunque no crean.  Era tan sencillo como escanear.  El dedo.  Vale.  Aplicar un filtro de estos.  Filtro paso o alto de.  De.  De.  De.  De.  Y ya tengo los bordes de la huella de actilar.  Y con eso.  Esa imagen.  Me la paso a mi base de datos.  Y sé que fulanito tiene estas características.  Sutanito tiene estas características.  Menganito tiene estas características.  Mi base de datos.  Entonces.  Cuando venga la persona a trabajar.  Y ponga la huella de actilar.  En vez de fichar.  Pon la huella de actilar.  Y ya sé quién ha venido a tal hora.  Y ya tengo la coronación.  Ya tengo.  Al qué hora he entrado y al qué hora he salido.  Vale.  O sea.  Alguna vez les he dicho.  Bueno.  No alguna vez.  Varias veces les he dicho.  Lo que yo les estoy enseñando son.  Las tripas.  De lo que se hacía antes.  Para que vean como la red neuronal.  Va a utilizar todo eso.  A posteriori.  Va a utilizar todo ese conocimiento.  Ustedes tienen que conocer.  Ustedes tienen que conocer todo esto.  Ahora.  Si me acerco mi ojo.  Y ustedes tratan de verlo.  Ya no van a detectar bordes.  Si eso le coloco el otro filtro.  Voy a detectar bordes.  Y voy a detectar contrastes.  Casi segmentación.  Por eso es lo que decía.  Hay clases.  En otros temas vamos a ver eso.  Que exista segmentación.  Con eso.  Segmentando retina.  Vale.  Voy a poder hacer.  Mi misma base de datos.  En lugar de la voy a detectar.  En los ojos.  Vale.  Para detectar.  Personas.  Para fichar en una empresa.  O lo que sea.  Vale.  Ahora.  Reconocer regiones.  Perfecto.  ¿Por qué?  Imágenes satelitales.  Para describir.  El crecimiento de una ciudad.  Y demás.  Desde el satélite.  Con los bordes de las calles.  De los edificios y demás.  Voy a encontrar.  La opción de poder segmentar.  Todo eso.  Vale.  En el Amazonas.  Está deforestado cada vez más.  Solamente con el filtrado paso alto.  Ya puedo detectar.  Como hay zonas.  Que se han ido.  Desforestando.  Mayor o menor calidad.  O sea.  Me sirve para hacer todo eso.  Vale.  Identificar la placa de los coches.  De los autos.  También.  Lo mismo.  Ahora.  Una red neuronal comprende.  Las características.  De las que se han ido.  De las que se han ido.  De las que se han ido.  De las que se han ido.  De las que se han ido.  De las que se han ido.  De las que se han ido.  De las que se han ido.  De las que se han ido.  La red neuronal comprende.  Las características.  Del.  Del álvaro.  Que dice.  De zapato pelota.  Claro.  Comprende las características.  Sí.  Pero yo tendré que.  Explicarle cómo hacer todo esto.  Sí.  Entonces.  Para eso son estas clases.  Para que vayan detectando.  Vale.  El que me ha puesto.  Detector de.  Contorno de tumores.  Eh.  Mayor.  No sé.  Como pronunciar tu nombre.  Patricia.  Me saco el sombrero.  Las clavadas.  Es que se utilizaba.  Les digo.  Antes de que la red neuronal sea tan.  Tan inteligente.  Se utiliza ahora para todo.  Se utilizaba ese detector.  De bordes.  Más que contornos de tumores.  Aja.  Foto escándales.  Que llaman.  Es para.  Eh.  Te colocan a ti.  Una parte de ti.  En un.  Por decir.  Eh.  Casi.  Cámaras.  360.  Grados.  Lleno de cámaras.  360.  Cámaras.  360.  Cámaras.  360.  Arg Lady.  Rpdjn como un agsoft.  Sobre todo de lo que dice.  Siickets.  ¿�Siumbing ti?  Así usan la fraud.  Como measure  Así.  es mucho más rápido, mucho mejor y el accuracy es más alto, pero para que vean lo que se usaba antes.  Eso es lo que quiero que se queden, que sepan que la red neuronal no lo es todo, claro, lo es todo ahora mismo,  pero ha tenido de dónde adquirir ese conocimiento.  Ahora, el kernel, los kernels que vamos a ir presentando, la parte de ejemplo,  pues incluso en el laboratorio, el día lunes, podemos hacer un ejemplo de esto.  Es más, yo voy a hacer lo que ustedes me vayan diciendo y vamos a tratar de modificar un kernel, a ver qué nos sale, qué nos saca.  A lo mejor encontramos un kernel que ni siquiera está catalogado en la literatura y nos da buenos resultados, yo qué sé, pero vamos a experimentar con él.  A ver, Edwin, ¿cuántos canales se aplican normalmente y cómo sabe cuáles debe usar?  Debe usar, eso es lo que te digo, lo que vamos a ver.  Normalmente se utiliza uno o dos encadenados, yo quiero ver así, ¿vale? Pero no más.  ¿Librerías de terminar, cómo? ¿Lo podría utilizar para terminar letras Braille?  Sí, la verdad es que sí, Milton, ¿por qué?  Para hacer un traductor, para hacer un traductor Braille español, por ejemplo, con las sombras.  Claro, con las sombras. Y vamos a ver una opción para detectar, por decirte bordes, y voy a detectar la sombra de lo que se han hecho las perforaciones, ¿vale?  Que, bueno, no son tan perforaciones, sino que les saquen relieves, ¿vale? Pero sí, Milton, es que puede ser.  Pero el ejemplo más básico que les he dado de la huella, que nadie me lo ha dicho, o que creo que alguien estaba ahí hablando justo en ese momento, es uno de los más básicos.  Es uno de los más básicos. Y yo, por lo menos, cuando estudié la carrera en los 90 en Bolivia, porque soy boliviano,  había una empresa que comenzó a sacar fotos de esto, de la huella de actilar, para hacer un control de acceso, o sea, para fichaje, ¿vale?  Y yo decía, ¿cómo les va a servir esto? Es el escáner de una huella. ¿Cómo lo detectan? No sé qué. Aquí tiene que haber algo más.  Y cuando lo entendí dije, pues es sencillo. Y son cosas sencillas que las empresas utilizaban antes para hacer todo eso.  Al día de hoy es colocar redes neuronales en ordenadores pequeños, ordenadores embebidos. Pero ya les he dicho,  el futuro es que un chip ya tenga esa red neuronal metida, incluso, o sea, si es que ya no existe, si es que existe a lo mejor ahora mismo, ¿vale?  Los que están en la carrera de eso, es envidia, ¿vale? Están utilizando los ordenadores muy, muy pequeños, los Jetson.  Comenzó desde la Jetson normal, la TX2, la Jetson Nano, y ahora están con la Jetson Orin, que son muy buenos.\n",
      "Intervalo 40-50 minutos:  Y los ordenadores son pequeños, ¿vale? Pero bueno, seguimos, ¿vale?  Y sí, en las imágenes de radio X también se utilizan los detectores de bordes.  Vale, entonces, filtro paso bajo, ¿vale? Reducción de ruido, suavizado, ¿vale?  Es cuando no disponemos las escenas muestras, ¿vale? Es una escena y las podemos ir promediando. Es cuando la imagen está muy destruida, muy borrosa.  Aplicamos el filtro promediado para diferentes escalas de P y observamos su efecto, ¿vale?  Esto sería el suavizado, ¿vale? Que como si se dan cuenta, ya hemos visto suavizado con el filtro promediado y de mediana, pues básicamente lo mismo.  Pero ahora lo que nos está gustando es que podemos resaltar bordes sin contrastes con el filtro paso alto, ¿vale? Con un afilado, ¿vale?  Utilizamos la imagen resultante para modificar la original, resaltando estos bordes, ¿vale?  Ahora, aquí lo que les decía, esto es una imagen tal cual, es una imagen original, satelital, ¿vale?  Si yo me pongo a investigar esta imagen satelital y digo, uy, mira, este es el cauce del río.  Si he tenido que pasar a hacerle un zoom y demás porque no sabía que era un río, ¿no? O no tenía muy buenas características, ¿vale?  Pero, ¿qué sucede si a esta imagen original le ha aplicado el filtrado y he obtenido un suavizado, ¿no?  Para un filtro paso bajo, para utilizarlo, un suavizado.  Y a esta imagen original también le ha aplicado filtro paso alto y si se dan cuenta, ya puedo discernir cómo va el río, las características, la forma que tiene el río, ¿vale?  Una forma mejor, aquí lo veía muy recto, no puede ser que un río vaya muy recto a estas alturas.  Siempre tiene curvas vertientes y claro, desde el satélite no puedo dibujar casi una línea recta.  A ver, es obvio que la imagen satelital me dé estas características.  Pero con un filtrado en paso alto, pues veo que tiene, no es tan, que el río no tiene esa rectitud, pero tiene esos pequeños matices de un río normal, ¿vale?  Me sirve para eso.  Ahora, lo mismo aquí.  Imagen original, ¿vale?  Le aplico un Sobel.  Esto es un Sobel, ¿vale?  Para sacar esta imagen.  Con esto yo ya estoy viendo este, llámese Puente o...  Sí, llámese Puente, ¿vale?  Las características que tiene, ¿vale?  Si saco un filtro de mediana, un suavizado, yo qué sé, pues puedo verlo, pero no con la misma casi exactitud.  Por así decirlo, que me da el Sobel.  El Sobel ya me está dando el reconocimiento de los bordes.  Como tal.  Pero, repito, no deja de ser un filtro.  Hay detectores de bordes, ¿vale?  Que me permiten hacer eso.  ¿Cuáles son estos detectores de bordes?  Tenemos los siguientes.  Son basados en cálculos diferenciales.  ¿Vale?  Utilizan distintos operadores, distintas máscaras, ya les he dicho.  El Sobel es uno de ellos.  Pero está el Roberts, está el Prewitt y el algoritmo de Kani.  El algoritmo de Kani cuando surgió hace unos diez años, un poco más.  Sí, bueno, más de diez años, ¿vale?  Fue el Noa más.  ¿Vale?  Por ejemplo, los kernels de Roberts.  Uno menos uno.  Ya saben que tiene que ser cero.  Y después rotado.  Es una máscara de dos por dos.  Me sirve para identificar bordes a 135 y a 45 grados, ¿vale?  La respuesta es muy débil a los bordes y sensible a la presencia de ruido.  Entonces, me puede servir para algo.  Dependiendo del tipo de imagen que tenga, me puede servir más un tipo de filtro que el otro.  No siempre es el mismo para todos.  El de Prewitt es una máscara de tres por tres que tiene estas características.  Para bordes horizontales y verticales.  ¿Vale?  Uno menos uno, uno menos uno, uno menos uno.  Es cero.  La respuesta sigue siendo cero.  ¿Vale?  Del filtro a paso alto.  Recuerden, filtro a paso alto.  La respuesta es cero.  Y el paso bajo, uno.  ¿Vale?  Nos quedamos con...  Ahora mismo estamos viendo siempre el paso alto porque el tema lo pide.  ¿Vale?  El de Sobel.  Ya lo tenemos.  ¿Sí?  Tenemos el de Sobel.  Es la máscara de tres por tres que vamos a sacar.  Ahora, el operador de Sobel.  Mi imagen original.  ¿Vale?  Con Sobel en horizontal y en vertical.  ¿Sí?  Me da estas dos respuestas.  Pero si la asumo...  Fíjense qué bonito.  Me ha dado mejor detalle.  ¿Vale?  Ahora, para el que me decía de las imágenes en Braille, las sombras y demás.  Las imágenes en Braille.  El traductor de Braille a texto, por así decirlo.  Con un Sobel.  Como tengo esos contornos.  Y con un Roberts también.  ¿Cuál es mejor?  ¿Un Roberts o un Sobel?  A ver, que algunos se mojen.  ¿Cuál le gusta más?  El Sobel pareciera más detallado.  El Sobel parece más.  Principalmente.  Pero si se dan cuenta, el Sobel te da más detalle aquí.  Pero el borde exterior, el borde exterior está mejor definido en un Roberts.  Aquí me pueden decir, sí, el Sobel pero el superior no.  Pero el inferior está mejor que este.  Sí.  Vale.  Entonces.  Es lo que decía.  Dependiendo el tipo de proyecto.  El tipo de problema.  El tipo de investigación que vayan a hacer.  Pues un filtro será mejor que el otro.  O sea, un kernel va a ser mejor que el otro.  Sí, eso es así.  No les puedo decir cuál es el...  Juan, puedes hablar tranquilamente.  Vale.  No les puedo decir cuál es el juego.  Profe, ¿podríamos de pronto entonces juntar los dos algoritmos?  Digamos hacer las dos imágenes y luego juntarlas.  Para coger los botes que están abajo en uno y en otro.  Podemos hacerlo.  A ver, ahora no porque la clase es de 45 minutos.  Pero en el laboratorio podemos ir haciéndola.  Es más.  Yo les voy a pedir que ustedes también vayan desarrollándolo.  Y lo vamos a presentar en el laboratorio del lunes.  Vamos a ir presentando todo esto.  Ahora.  Un original.  Tenemos mi laplaciano.  Y tenemos mi Sobel X y mi Sobel Y.  El resultado va a ser mucho mejor.  Obviamente.  Pero si se dan cuenta un Sobel NX.  ¿Qué contornos me los da mejor que el Sobel NX?  ¿Vale?  Pasa eso.  Ahora, por ejemplo.  En esta de...  Del dedo.  Decirme cuál detecta mejor los bordes.  Un Sobel.  Un Roberts.  Un logarítmico.  O sea, estoy dando algunos nombres que no los hemos visto.  Un Pre-Width y demás.  Porque me he centrado en los más básicos.  O un Cani.  Sí, Cani.  ¿Por qué?  Se ven todos los bordes están completos sin perderse en ninguna parte.  Ahí le has dado.  Incluso la del tendón aquí que podéis ver.  Incluso éste le ha cazado.  Por eso les he dicho.  Cuando surgió el Cani.  Ya fue el no va más.  ¿Vale?  Hay otras máscaras.  ¿Vale?  De su espacio de bordes.  ¿Vale?  De sus espacios de líneas y promediados.  Hay varios.  ¿Vale?  Lo podéis ver.  Si más no me equivoco lo tenéis también en la documentación.  ¿Sí?  Hay varios.  Aquí les he dejado uno donde van a poder ver más máscaras.  Más piernes que van a poder utilizar.  ¿Vale?  Hay de desenfoque, detección de bordes.  Tipo Sobel.  O sea, hay todos.  ¿Vale?  Ahora, el operador de Cani.  No nos vamos a ir por las ramas.  ¿Vale?  Viene a ser un filtro paso bajo de tipo gausiano suavizado con reducción de ruido.  Un cálculo de gradiente diferencial.  Intensidades todo a cero.  Entre medias hay umbralización y binarización de opinión.  Bueno, van a binarizar los píxeles también de la imagen.  Se elimina de las estructuras más débiles.  Hace virguerías.  ¿Vale?  Que es algo más robusto.  Se puede decir que esta detección de bordes es muy complejo.  Pero actualmente, no pensé ver que está en esta función en Python.  Sobel.cani.  O el etch en Matlab.  ¿Vale?  Es aplicarlo.  No nos vamos a ir a destriparlo.  Pero que sepáis que es eso.  ¿Vale?  No hay donde perderse.  El Cani tiene estas características.  Mi operador Cani de las monedas.  Si se dan cuenta, las clava.  O sea, no vamos a decir que no.  O sea, es muy bueno.  ¿Sí?  Es el más utilizado a día de hoy.  Incluso hay implementaciones hardware.  ¿Vale?  Sobre todo en algoritmos de FPGAs.  O sea, kernel se llama.  ¿Se llama kernel?  No, Ips.  Creo que se llama Ips.  Programitas que van ya precalculados, prediseñados en una FPGA, por ejemplo.  Que te viene este detector de bordes.  Que se utiliza bastante, pero bastante en espacio.\n",
      "Intervalo 50-60 minutos:  ¿Vale?  Y también se utiliza muchísimo, muchísimo en imagen médica.  Sobre todo en PET y en...  ¿Cómo se llama?  Sí, bueno, en PET y en SAT.  ¿Vale?  En PET a drive.  Entonces...  ¿Crees que este tipo de detección de bordes sirva para este proyecto?  Lo que pasa es que actualmente yo estoy trabajando en una empresa atunera.  Y el proceso de clasificación al momento que el barco ingresa es manual.  Lo clasifican por tamaño.  Y se me ocurre que este tipo de detección de bordes coge la atún.  Y puede por tamaño detectar cuánto es su talla.  Digamos poner una etiqueta según su tamaño.  ¿Tú crees que sea buena idea este tipo de detección para ese ejemplo particular?  A ver, para detectar el peso, ten en cuenta que vas a tener también una coordenada más.  O sea, no detectar el peso, sino que detectar su talla.  Pues sí, vas a detectar los bordes de la atún.  Ten en cuenta que el cani, como te detecta todo muy bien y está puesto aquí la mano,  puedes tener muchos ruidos que a lo mejor no te vale la pena.  Por ejemplo, a lo mejor un shovel, como solo tienes el borde de adelante y el borde de atrás, por ejemplo, ya te vale.  Un cani, a lo mejor tener mucho ruido entre medias,  que se te haya colado la parte de la red del peso o lo que sea, ya te influye.  Entonces, yo te digo, y es lo que se hace, es más, yo lo he hecho,  para detectar tamaños de llantas, de coches aquí, de...  ¿cómo se llama esto? De Michelin.  De Michelin, para ver, lo hacían por tamaño, para ver el desgaste, que se decía.  No de Michelin, de la Gutia.  Por tamaño, para ver el desgaste, se utilizaba la detección de bordes.  Y no nos íbamos a un cani, porque ya se había visto que el cani daba mejor precisión, mejor calidad de bordes y demás,  pero a veces menos es más, como dicen.  Entonces, se aplicaba solamente un Roberts, porque con eso ya era suficiente para sacar el diámetro de la llanta y ver su desgaste.  El cani me daba más información de las que quería y a veces, incluso el hecho de que tenga algo pegado entre medias,  como me lo detecta tanto, pues me daba más falsos positivos que otra cosa.  Entonces, va a depender un poco, yo te diría, utiliza varios y compáralos entre ellos.  Es como una prueba y error, pero es que lamentablemente no hay un idóneo para un tipo de aplicación.  Ok, pero sí va enfocado a este tipo de técnicas, ¿no?  Claro, claro, va enfocado.  Para el tamaño.  Pero también enfocadísimo, porque vas a tener altura y anchura, ¿sabes?  Entonces, vas a tener eso.  Lo único que te digo, prueba cuál es el que se va a adaptar mejor a las características que tienes.  No te puedo decir que uno sea mejor que el otro.  A ver, para detectar las cosas sí, pero para tu aplicación a lo mejor no necesitas detectar todo, porque te da más bien ruido.  ¿Sabes? Y ese ruido son errores.  Vale.  Entonces yo te diría que los pruebes.  Ahora, unos links de interés.  Hay un vídeo muy interesante de YouTube que les muestra todo cómo hacer toda esta parte de los filtros.  Y está aquí también la librería de Stick Image, donde habla de cómo hacer la implementación.  Vale.  Echarles un vistazo, jugar con ellas.  Llámense Matlab.  Recuerden que Matlab, por estar en la universidad, tienen la licencia de Matlab.  Se pueden descargar en Matlab, lo pueden utilizar.  Pueden usar la versión online de Matlab, que yo la uso y demás.  Pueden hacer eso.  Entonces, hay esas opciones.  Que no les gusta Matlab, quieren usar Python, que es buy free, usar Python.  Que no les gusta los dos, que son de la vieja escuela, quieren usar C.  Usar C tiene no P, se huben C también.  Entonces, usen todo eso.  Ahora, un ejercicio para el foro.  Si podéis hacerlo, si no, no pasa nada.  Usar la operación de Spicy.  Que tenemos esta.  Para obtener los bordes de los filtros en Sobel.  En las distintas direcciones.  Ya sea vertical, horizontal, diagonal y inversa.  Y tratar de sacar el resultado.  Si podéis, colocadme las imágenes.  En Matlab es el COM2.  Tratar de hacer eso.  Para detectar los bordes.  A ver que tal les va.  Para que vean, para que jueguen un poco.  Es importante.  Esta clase nos ha servido para ver cómo los filtros,  uno me suavizan y otro me detectan los bordes.  Y dependiendo del kernel, si los roto, si no los roto.  El tipo de kernel que tengas, si es de dos dimensiones, de tres.  Los valores que contenga cada kernel.  Me da un resultado mejor que el otro.  Entonces, con esto ya tenemos herramientas para detectar bordes.  Sin necesidad de haberme ido en orden neuronal.  Pero ya me sirve incluso para poder preprocesar, hacer el data curation.  De la imagen que me va a servir para meter a la red neuronal.  ¿Por qué?  Porque a lo mejor la red neuronal lo que va a hacer es detectarme.  Por la mano qué tipo de persona soy utilizando un kernel.  Por ejemplo, lo puedo hacer.  Entonces mi red neuronal me va a detectar ya las personas con esto.  ¿Sí?  Se están dando cuenta, no necesitan más.  Voy a detectar si soy Javier, soy Luis, soy José, soy Edison, soy Daniel.  Hago un data curation, le hago un kernel.  Y hago mis base de datos solamente con este tipo de imágenes.  Y ya voy a entrenar mi red neuronal.  Y ya tengo otra forma de identificarlo.  Entonces, para la próxima clase.  Vamos a aprender cómo reducir la red neuronal.  Y cómo reducir la red neuronal.  Entonces, para la próxima clase.  Vamos a aprender cómo reducir el tamaño de los datos.  En el espacio de Fourier sin comprometer la información crucial.  ¿Por qué?  Porque si se han dado cuenta ahora mismo.  Si tengo que hacer un filtrado de una imagen.  De, yo qué sé.  Que sea de un millón por un millón de píxeles.  Que lo hay.  ¿Vale?  Las imágenes espaciales.  Y voy a aplicar estas máscaras.  ¿Van ustedes cuánto voy a tardar en recorrer todas las imágenes?  ¿Para poder resolver eso?  Pues ahí sí entra Fourier.  Ahora mismo hemos visto operaciones espaciales.  Ya nos vamos a meter ahora en las de frecuencia.  ¿Vale?  Pero eso ya es la próxima clase.  A ver, ¿qué dices?  Como para detectar facial también podría aplicarse un Kani.  Te diré que hay aplicaciones de detección de rostros por Kani.  Que las primeras que surgieron.  Incluso con redes neuronales utilizaban un Kani.  Pero siempre se chocaban con la pared.  ¿Por qué?  Porque lamentablemente es mucha información.  A veces detectar un borde.  Un rostro no necesita necesariamente un Kani.  Por eso les digo, a veces menos es más.  O sea, a veces no tener tanto o tanta basura entre medias.  Me da mejores resultados.  Y es verdad.  Antes toda la gente no quería utilizar detección de rostros.  O distingir rostros.  Porque aplicar filtros Kani que estaban de modo y demás.  Todo el mundo colocaba eso.  Porque mira que lunarcito.  Que estito, que lotito.  Pues daba más información.  Y los resultados eran muy malos.  A día de hoy ya no existe eso.  ¿Por qué?  Han implementado otros que no se han basado tanto en el Kani.  Y ya me he encontrado buenos resultados.  ¿Vale?  ¿Kani tiene mayor costo de procesamiento comparado con las otras?  Pues sí.  Pues sí tiene mayor coste.  ¿Vale?  Y ya me he pasado.  Y eso es todo.  No sé.  ¿Tenéis alguna otra duda?  Andrés, una consulta.  ¿No se va a tratar del tema de la resolución de la actividad 1?  O llegué tarde.  No hemos hecho la actividad 1 todavía.  Y la clase la vamos a hacer el martes.  Digo el lunes.  Perdón.  Carlos.  Si me baso en el Kani para detectar bordes de productos.  O algo similar.  ¿Cómo interpretaría el cómputo que necesito?  ¿Interpretaría el cómputo que necesito?  O sea, el tipo de imagen que tengas multiplicado por un Kani o implementado un Kani.  Tienes que tener en cuenta que tienes que hacerlo por todas las imágenes y por cada trozo de la imagen.  Entonces, el coste computacional lo tienes que calcular tú.  Yo no lo puedo calcular.  Porque no sé qué resolución tiene tu imagen.  O cuántas imágenes vas a tener y demás.  Eso no tienes que calcular tú, Carlos.  Recuerda.  Y esto quiero que lo tenga muy presente.  Está en redes neuronales.  Creo que tiene una o un máster de guía.  Y tiene una materia.  No sé cuál es.  Donde les dice, creo yo que les dice, el coste computacional que tiene hacer una red neuronal.  Entrenarla.  Y si tienen un millón de imágenes, pues tardará mucho más.  Ahora, si esa imagen es demasiado grande.  Pues tiene que multiplicar por ese tamaño.  Ahora, algo que no hemos visto, pero lo tienen que saber ustedes.\n",
      "Intervalo 60-70 minutos:  Si voy a descomponer esa imagen en números enteros.  Vale.  Mi resultado va a ser más rápido que si utilizo flotantes.  Es decir, como flotante.  Si utilizo un double antes que un float.  Pues ya tengo el doble de precisión.  Y es más procesamiento que tengo por detrás.  Sí.  Esos cálculos.  Vale.  Los tenéis que tener en cuenta.  Sí.  ¿Por qué?  Si todos lo van a entrenar en la nube.  Vale.  Saben que todo el coste de los entrenamientos es por tiempo.  Si mi imagen es muy grande.  Utilizo algoritmos muy pesados.  Y demás.  Para operar.  Si en imágenes a lo mejor me tardo una hora.  Si reduzco el tamaño de imagen.  Y reduzco el algoritmo que no sea tan pesado.  Imagínese.  En vez de un cane utilizo un rover que es más rápido.  Dos por dos.  Pues a lo mejor me ahorro el 90% del tiempo.  Sí.  Tienen que tener en cuenta.  Todas esas características.  ¿Y por qué les digo eso?  Porque eso es lo que estamos viendo ahora.  Hemos comenzado la materia.  Destinando el tipo de.  ¿Qué tipo de sensores iba a utilizar?  ¿Cuáles iban a acomodarse mejor?  Vale.  Hemos comenzado a darse repulguesera.  Por así decirlo.  Después hemos comenzado.  ¿Cómo los voy a.  A discretizar.  ¿Qué valores voy a darles?  De 0 a 1.  De 0 a 256.  De 0 a 1024.  Por ejemplo.  Eso influye.  Entonces.  Mi.  Digitalización de los valores.  Analógicos a digitales.  Me influye.  Con eso ya sé más o menos.  ¿Qué cámaras puedo utilizar?  ¿Qué micrófonos puedo utilizar?  Después.  Ahora estamos viendo.  ¿Qué operadores.  Existen.  Sin necesidad de aplicar.  O.  ¿Qué filtros?  Sin necesidad de aplicar redes neuronales.  Y demás.  Tengo filtros.  Que por.  Endex son mucho más rápidos que una red neuronal.  ¿Por qué?  Porque ya no.  Tiene ese.  E.  Entrenamiento de medias.  Ya me dan resultados.  Que para.  Para mis imágenes.  Me sirve ya para.  Darle la imagen.  Lo mejor.  Mente posible.  O lo más comidito posible a la red neuronal.  Si voy a detectar.  Huellas.  Dactilares.  Pues ya saben.  Haciendo la detección de.  Con un cani.  O con un rover.  Por ejemplo.  O con un shovel.  Ya tengo la imagen.  Ya tengo la imagen ya más preparadita.  Y el resultado es mucho más rápido que.  Utilizar una red neuronal.  Para que me detecte bordes.  ¿No?  Entonces.  Están.  Dando esos pasos.  Indirectamente.  O que no se den cuenta.  Están aprendiendo todo ese preprocesamiento.  Cosa de que.  Cuando tengamos que aplicar ya después la red neuronal.  Pues.  Es aplicar las dos tres líneas que te viene.  E.  En los.  E.  Frameworks de.  De que eras.  O de paitos.  Y demás.  Ya te viene todo eso.  Pero tú ya le estás dando la imagen.  Lo va a necesitar.  Vale.  Pues nada.  Que nos hemos vuelto a pasar como siempre.  Si no hay dudas.  Pues.  Lo dejamos ahí.  Vale.  Vamos a parar.  De compartir.  Y de.  Vamos a parar.  De compartir.  Y de.  Vamos a parar.  De compartir.  Y de.\n"
     ]
    }
   ],
   "source": [
    "current_interval = 0\n",
    "accumulated_text = \"\"\n",
    "\n",
    "for segment in result6[\"segments\"]:\n",
    "    start = convert_to_min_sec(int(segment[\"start\"]))\n",
    "    end = convert_to_min_sec(int(segment[\"end\"]))\n",
    "    text = segment[\"text\"]\n",
    "\n",
    "    # Verificar si el segmento sigue dentro del intervalo actual de 15 minutos.\n",
    "    if start // n == current_interval:\n",
    "        accumulated_text += \" \" + text\n",
    "    else:\n",
    "        # Imprimir el texto acumulado para el intervalo actual.\n",
    "        print(f\"Intervalo {current_interval*n}-{(current_interval+1)*n} minutos: {accumulated_text}\")\n",
    "\n",
    "        # Actualizar el intervalo y reiniciar el texto acumulado.\n",
    "        current_interval = start // n\n",
    "        accumulated_text = text\n",
    "\n",
    "# No olvidar imprimir el último intervalo acumulado fuera del ciclo.\n",
    "print(f\"Intervalo {current_interval*n}-{(current_interval+1)*n} minutos: {accumulated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "result7 = model.transcribe(\"/home/contrerasnetk/Documents/Classes/VisionArtificial/7.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intervalo 0-10 minutos:   la pantalla. A darme un segundo y vamos a presentar el tema. Avisarme si lo podéis  ver. ¿Lo veis? ¿Sí? Vale, los que dicen lo que se han matriculado en diciembre 2023  y demás, pues claro, es visión artificial, pero los que se han matriculado antes que  han arrastrado la materia, pues sí, es percepción computacional. Es por eso. Igualmente, si  tenéis alguna duda, podéis hablarlo con sus mentoras que seguramente les van a explicar  mejor que yo. Vale, darme un segundo que voy a poner aquí. Vale, me podéis interrumpir  cuando queráis y demás. Vale, poner el control laser. Es un tema rápido por otro lado también.  Vale, solo lo único que necesito es que se abstraigan un poco en una imagen espacial,  es decir, en una imagen pixelada que existe, que podemos tenerla en una imagen cualquiera.  Y les voy a decir por qué. Porque los tres últimos slides nos ayudan bastante en la  parte de ejemplos prácticos que les suelo pasar. Vale, entonces vamos a ver lo que sería  una introducción, los elementos estructurales, los algoritmos, la morfología matemática  que les digo, que es erosión, dilatación, apertura, clausura, que son los más usados,  pero hay varios más que los vamos a revisar. Pero a partir de estos cuatro se mezclan todos,  vale, y nos dan la opción de poder analizar con más detalle las otras variantes y nos  ayuda para varias cosas, vale, desde la parte de healthcare hasta la parte de astrofísica  o de espacio. Vamos a ver, la morfología matemática tiene el fin de eliminar o potenciar,  aumentar esos pequeños detalles en las imágenes, vale. Se utiliza también para separar objetos  después de hacer una segmentación, vale, me adelanto de esa palabra ahora mismo de  segmentación, pero háganse la idea que puede eliminar ese ruidillo que también puede existir,  vale, o quitar, llámese filtrado, sí. Por otro lado, se puede utilizar antes o después de  segmentaciones y no existe limitaciones, por eso es que mi intención de ponerlo antes de todo el  análisis de frecuencia es porque permite esa flexibilidad de movimiento, ese análisis es  flexible. Por otro lado, es complejo computacionalmente, sí, siempre les he dicho,  en todo proyecto que vayáis a hacer, el hecho de tener una infraestructura en condiciones es lo que  prima, pero si mi infraestructura es decir, mi potencia de computación está reducida,  pues tengo que utilizar algún método o otro, vale. Ahora, cuando les digo que es complejo  computacionalmente, también les digo que a día de hoy los ordenadores, las computadoras,  no solamente hacen procesamiento en CPU, también hacen en GPU. Entonces, esta morfología matemática  en GPU es mucho más rápida que hacerlo en CPU y les garantizo que incluso usando GPUs de  teléfonos móviles, porque lo hemos usado en dos proyectos, es factible, vale. Entonces,  por otro lado, también están diseñados con algún conocimiento de formas cercanas con  propiedades geométricas, vale, a partir de las cuales voy haciendo todo este análisis.  La morfología matemática se basa en la teoría de conjuntos, lo que hemos aprendido en colegio,  en la universidad. Un conjunto es un objeto de imágenes en el cual le puedo aplicar  geometría integral y le puedo hacer álgebra de retículo, lo que sería el látiz álgebra,  vale, los que han avanzado a geometría descriptiva seguramente lo habéis visto,  eso en la carrera. Después, la morfología matemática también se puede aplicar a cualquier tipo de  imágenes desde una dimensión, dos dimensiones, tres dimensiones e incluso, alguna vez les dije,  que hay una agrupación de imágenes geotiff que tiene, aparte del RGB y demás, tiene la  geoposicionamiento, o sea, tiene una variable más entre medias y se puede aplicar eso. Para espacio  se aplica incluso esa morfología matemática porque es análisis espacial de la imagen y no  importa que esté presente la segmentación o la transformada en frecuencia antes o después,  esto se puede colocar en cualquier lado, vale, el único problema que tiene es el computo,  pero ya les he dicho que gracias a la GPU eso es, al día de hoy, muy fácilmente de resumir, vale.  Por ejemplo, para la eliminación de pequeñas estructuras, ya les he dicho,  estos pequeños, ese pequeño ruido del fondo, ya ven, ese ruido salpimienta, si lo queréis ver,  es factible eliminarlo, pero claro, también en contraposición me resta algunas cosas.  Imagínense, por ejemplo, que aquí ustedes tienen granos de sal o de, perdón, de arroz, vale,  y haciendo una corrección de iluminación y demás, pues esos granos de arroz que por la luz se ven  más gordos, pues haciendo esa corrección por estas operaciones matemáticas morfológicos,  pues los puedo reducir y dejarlos en la forma original que tienen.  Los operadores morfológicos que existen, erosión, sorche enemigo, dilatación,  apertura, sorche enemigo, clausura, o sea, los opuestos, vale. Por otro lado, no todos los  operadores van a mantener una proporción inicial, los únicos, apertura y clausura, nada más,  ellos sí los van a mantener, pero el resto van a modificarse, sí, tenerlo en mente.  Hasta este punto, en todo lo que estamos viendo en la materia, estamos en la parte de segmentación  infiltrado, todavía no hemos ido a extracción de características, segmentación infiltrado es la  parte más gorda, estamos en esta parte y vamos a ir avanza. Vamos a ver primero qué es un elemento  estructural. Un elemento estructural, si hacemos un recordatorio de clases pasadas, vendría a ser  como nuestro kernel, es decir, nuestra máscara que se va a aplicar a cada uno de los puntos de la  imagen, entonces es un conjunto de una forma conocida que me va a recorrer toda la imagen.  Por otro lado, el tamaño y forma de este elemento estructural se adapta a propiedades  geométricas de los objetos, llámese una cruz, llámese una matriz 3x3 o de 5x5 o de 7x7,  cual es quiera. Por ejemplo, tengo estos dos elementos estructurales y tengo una imagen,  cuando recorre los elementos estructurales sobre una imagen, imagínese que sólo tengo un punto en  el centro o un cuadro en el centro, cuando los hago mover, es decir, muevo mi kernel o elemento  estructural por toda la imagen, lo que voy a ir generando es una imagen resultante que tenga  unas características cuadradas. Si mi elemento estructurante tiene la forma de cruz y lo voy  moviendo alrededor de la imagen, del espacio, voy a tener más o menos el resultado de lo que sería un  diamante, pero si hago una fusión de ambas, primero un elemento estructurante cuadrado 3x3 y después  uno en forma de cruz, puedo tener una forma hexagonal. Les repito, elementos estructurales,  llámese la máscara o el kernel del filtro. Es importante tener en cuenta esa visión y ya  vamos a ir avanzando poco a poco. ¿Dónde lo voy a aplicar? Pues lo puedo aplicar en imágenes binarias,  que sería lo más fácil, ceros y unos, donde uno es que existe y cero que no. Imágenes con escalas  de grises, que ya tendría una pequeña dimensionalidad y también lo puedo hacer en imágenes a color.  Lo más utilizado, pero es escalas de grises e imágenes binarias. Ya vamos a ir viendo por qué.  Ahora, por ejemplo, aquí tenemos a un gatete de toda la vida. ¿Qué elementos estructurales creen  que se han utilizado en estas dos imágenes? Los que hemos visto ahora. ¿Alguien tiene una idea?  Nada. Cuadrado a la izquierda, combinación de los dos, derecha la cruz. Sí, correcto. Juan Carlos,  correcto. Muy bien. La izquierda ha sido combinación de los dos, cuadrado y una cruz,  que decíamos que era tipo hexagonal y la derecha más que todo uno triangular. Pero la derecha  también podría haber sido un cuadrado rotado. Es un elemento estructural que lo puedo rotar.  Estamos hablando de que me estoy moviendo en espacio. Entonces, si tengo espacio, puedo rotar  las cosas. Pero es valido la derecha, la cruz, el rombo. Bueno, sí, vale, un cuadrado. Tenemos  esos elementos estructurales. Ahora, vamos a ver qué es la dilatación y el erosión. Dilatación,  llámese un operador, soma circular y erosión resta circular. Son las formas de interpretar.\n",
      "Intervalo 10-20 minutos:  Por ejemplo, para una imagen binaria, perdón, y si tengo mi elemento estructural que es un círculo  y voy pasando el círculo por todo este elemento, solamente la dilatación va a ser que cada  cada par de valores que caiga justo en el círculo, pues se va a mantener. Y los que no,  pues se van a volverse. Entonces, si estoy haciendo esa dilatación, pues voy a ampliar la imagen.  Ahora, si es erosión, todo lo que caiga en el elemento estructural va a ser solamente unos,  el resto va a ser cerros. Entonces, la erosión me va a comprimir, me va a restar. No sé si se...  Es como si estuviera respirando. Mi dilatación es como cuando infló el aire en los pulmones.  Entonces, digamos que la imagen se agranda, se mantiene la forma. En este caso, como hay  una imagen que está compuesta por dos tramos de imágenes, si coloco aquí la parte circular,  como vemos aquí, pues se está juntando y se une. Y en el caso contrario, cuando hago la erosión,  es como si estuviera haciendo lo contrario a la dilatación de imagen. Se comprime,  se hace más pequeño. Es como un pincel cuando uno añade o resta una máscara. Juan Carlos,  ahí le has dado. Es el mejor ejemplo que podrías haber puesto. Muy bien, es eso.  Cuando hacemos lo de la máscara del Jim, es esta. Para restarla, es el pincel mágico también que  aparece en el... ¿Cómo se llama este? En el Adobe, en el Photoshop. Es eso, en el Illustrator. Muy bien,  gracias Flipe. Por eso es que quería tocar este tipo de tema antes de friar, porque me parece  más corto, más sencillo. Y el otro ya lo dejamos para la clase que viene, que es más tocho. Pero  esto, desde mi punto de vista, me gusta siempre darlo en este tema, no sea un antes de friar.  Teniendo en cuenta y a todos a lanzar la cabeza y dar gracias a Juan Carlos, que la ha clavado,  es como si tuviéramos el pincel este de la máscara del Jim o del Illustrator. Es hacer  dilatación y erosión. Ampliar, reducir, ampliar, reducir. Quedarse con eso, que es lo mejor. Y van  a ir entendiendolo todo esto muy fácil. La dilatación, me va a potenciar aumentar los  contornos de los objetos, magnificar los detalles, fusiona objetos, si hay muy juntos, que están  separados inicialmente, pero están muy cerca, pues los fusiona y los expande, pero encorciendo  el fondo, si están juntos. En caso contrario, la erosión, pues me elimina las estructuras que no  pueden contener la imagen, me reduce los contornos, me separa los objetos y encoge los objetos  expandiendo el fondo. Función matemática es esta. Quedarse con la función matemática, pero para mí,  quédense con la idea de que suma y que resta, o sea que amplía y que comprime. Hemos comenzado  diciendo lo de la dilatación, que es un conjunto de x puntos de un elemento estructurante B, que  puede ser un círculo, un cuadrado, un rombo y demás. Y se lo va a aplicar en un lugar geométrico,  en una figura, en una imagen. Tal que los puntos x de esta imagen, que toca por el elemento  estructurante B, pues si coinciden con ellos, es un 1. Si no coincidan, pues es un 0. Entonces,  teniendo en cuenta eso, mi imagen se amplía. Quedarse con eso, quedarse con ese concepto que  es importante. Tener en cuenta que el valor dilatado de un píxel, pues es el valor máximo  de la imagen por la ventada de definidad por el elemento estructurante, lo que hemos estado  hablando. Esto es un poco de matemática, pero hay que tener en cuenta que más allá de los bordes,  de los paddings que se le asigna a los valores mínimos, que sería el fondo, pues se han ido  eliminando porque he puesto la imagen que he incrementado. Esto sería más o menos el pincel  del GIMP o la máscara del GIMP para sumar o restar, donde voy recorriendo por cada uno de los píxeles  de los elementos estructurantes. Entonces, si pasa cerca de un valor 1, en el caso de algo binario,  pues el contorno va creciendo. Si es una imagen en escala de grises, los valores igual se irán  incrementando. Voy ampliando las cosas. En un lado binario y en el otro lado escala de grises.  Ahora con esto, pues mira, Juan Carlos, yo creo que gracias a ti, para mis próximas clases,  voy a utilizar un ejemplo con el GIMP en forma un poco más dinámica donde se pueda ver eso.  Mi elemento estructurante es una cruz. Está pasando por cada uno de los puntos de mi imagen.  Ya les he dicho, es un cuadrado en mi imagen original. Voy pasándola, voy pasándola,  voy pasándola. Y cuando encuentro un 1, aquí estaría la cruz, mis 1 estarían comprometidos.  Entonces, los puntos donde no toca los 1, pues se volverían blancos en este caso. Y mi elemento  estructurante estaría cambiando la imagen original. Es decir, lo estaría ampliando.  Mi resultado sería más o menos una cruz grande, gorda, tocha. Este es el original. Y con mi elemento  estructurante que era la cruz, pues he llegado a este informe. Eso sería la dilatación ampliar.  Erosión. Todo lo contrario. Lo recordamos rapidillo. Es como utilizar el restador del GIMP,  del Illustrator, donde en este caso el elemento estructurante que es el círculo me va a pasar  por todo el portado de la imagen y me lo va a reducir. A tal punto que cuando toque justo  esta imagen pequeña, como vemos aquí, cuando lo vaya recorriendo por estos puntos, pues se va  a ir de gran anor imagen hasta desaparecer completamente. ¿Vale? Tener en cuenta que el valor  relacionado de los píxeles X, que es el valor mínimo de la imagen, gracias a la ventana definida  por elemento estructurante que se ha tenido, pues es la resta, no es la diferencia. En la escala de  grises, pues se recorre la ecuación esta y en blanco y negro, pues se elimina. Se dilata solo  cuando el punto medio de la cruz pasa por el elemento de la imagen o vale cualquier punto.  Cualquier punto del elemento estructurante. Es cualquier punto del elemento estructurante.  Es que no tengo el Illustrator ni tampoco tengo el GIMP, pero si utilizas el pincel del GIMP,  que es como un circulito y pasas por toda la imagen, te vas a dar cuenta cómo en la  dilatación se incrementa. Cuando pasa por el, ya está pasando por un borde ese circulito y ya  se incrementa. ¿Por qué dices de la cruz? El de aquí. A ver, José María, habilita el micrófono y explícalo.  Aquí. Ten en cuenta que tiene que tocar cualquiera de los cuatro puntos.  No va a ampliar, ten en cuenta también que no va a tener toda, por ejemplo,  cuando toca un punto, por ejemplo, este, si este punto toca este cuadrado, no va a tener toda esta  forma los, no va a tener toda esta parte de blancos, va a tener los vecinos. No es todo el  elemento estructurante que se vuelve blanco, sino los vecinos.  Vale. Perdón si no lo he expresado bien, pero claro, son los vecinos del píxel del elemento  estructurante. Si toca este punto que toca justo aquí en la esquina, pues es este, pues si es  justamente este tocando aquí, pues este y este. Vale. O solamente este, o sea, es eso. Con el  paint también se puede evidencia, bueno, luego lo vemos. O sea que no es necesariamente que tiene  que tocar el centro, claro, no. Uno de los píxeles. Vale. Por eso, uno de los píxeles.  Y para la resta es lo mismo. También tocando uno de los píxeles lo reducimos. Vale. Aquí es lo mismo.  Tenemos mi elemento estructurante, en este caso una línea, lo voy a ir recorriendo en imagen binaria,  en escala de grises. Vale. En este caso, si este es mi elemento estructurante, paso la imagen del  erosión, de perdón, paso la imagen del elemento estructurante que sería la cruz por toda la imagen  y voy borrando, por así decirlo. Vale. Voy borrando aquellos puntos que no coinciden y obtengo mi  imagen original. Vale. O sea, se puede decir que he partido de esta con la dilatación y con la\n",
      "Intervalo 20-30 minutos:  erosión, he vuelto a lo mismo. Vale. Por eso decía su archinemio, sería lo contrario. Vale. Pero  quedarse con eso. Lo del paint, bueno, ahora mismo por tiempo no lo puedo hacer porque tendría que  abrirlo y ver en qué pate es. Pero si dices tú, Milton, te invito a que te hagas un videito y lo  coloques en el foro. Vale. Ahora, por ejemplo, tenemos mi imagen. Mi imagen original. Díganme  dónde se ha utilizado erosión y dónde se ha utilizado dilatación.  Cuál ha dilatado y cuál ha erosionado. A erosión. A erosión  y de dilatación. Vale. Ahora, si pensamos en lo contrario, imagínense blanco sobre negro,  me refiero, véanlo como esto invertido, que el blanco es 1 y lo negro es 0, o sea, la inversión.  ¿Cómo lo verían? ¿Cuál sería dilatación y cuál sería erosión? A dilatación.  Exacto. Y se ve eso entre todos. ¿Por qué les digo esto? Porque en realidad,  en realidad lo que yo tengo que visualizar que es mis objetos. Y mis objetos van a estar en blanco.  Vale. Por ejemplo, si tengo una cadena de monedas unidas y las estoy erosionando o las estoy  dilatando, pues tengo este resultado. Vale. Al contrario. Perfecto. Hace eso. Ahora, aquí.  Si tengo mi imagen original y hago una dilatación, tengo un comportamiento,  eso obviamente, lo contrario a la erosión, pero me da una imagen un poco más clara. Y la erosión  más oscura. Les recuerdo, estamos hablando blanco y negro. ¿Qué pasa en escala de grises? ¿Para qué  me va a servir esto de dilatar y erosionar? Si se dan cuenta, si esta es la imagen original, si la estoy  aplicando dilatación o erosión, lo que voy a hacer es o incrementar los detalles que están en el fondo,  llámense los marcos de las ventanas e incluso ver con mayor detalle las ramas de este árbol,  este aquí que es un árbol seco, pero estoy viendo con mayor detalle, le estoy dando más  importancia a los bordes. Vale. Versus darle más importancia al fondo, es decir, la forma del edificio  como tal, si hay nubes o lo que sea o si hay un árbol que tiene poca información, pero el fondo  me dice que está un día nublado. ¿Para qué me serviría esto? Les pregunto. A ver, a los que dicen  cosas de healthcare y a los que les gusta el tema de revisión de objetos raros con cámaras, ¿para qué les serviría?  Yo creo que independientemente de esos dos temas, esto puede tener aplicaciones en el campo médico  en tema de radiología. Por eso he dicho el de healthcare, healthcare es la parte de médico. Para radiología.  Radiología, MRIs, MRI scans, CAT scans y todo eso. El MRI es el que más lo utiliza, hay un ejemplo al final  de MRI, es el que más lo utiliza. Y ahora los de las cámaras que les gusta el espacio, la NASA utiliza en el HABEL  bastante procesamiento matemático para eliminar esos ruidos y poder ver nebulosas. Les he puesto una fotito  ahí en blanco y negro, en escala de grises se utiliza, pero se utiliza bastante eso. Es más, desde mi punto de vista  la NASA ha sido la precursora de utilizar estos operadores morfológicos para que pueda venir healthcare  en la parte médica y utilizarlo en MRI. Porque, y en parte en radiografías también, porque sin ese conocimiento  o sin ese interés de utilizarlo para ver el fondo quitando el ruido que viene por delante y demás, yo creo que  parte de healthcare no hubiera podido avanzar. Detección de otros objetos para encontrar fáciles objetos. Exacto.  Encontrar o visualizar mejor, o sea, resaltar más el fondo. Con esto ya dilatación e erosión. Vamos a ver qué es la apertura.  Antes de la apertura, pues hay que saber que la erosión de una imagen no solo elimina las estructuras que no pueden  contener el elemento estructurante, pero también encoge las demás estructuras. Por otro lado, la búsqueda de  operador que permite recuperar estas estructuras perdidas por la erosión, pues dio lugar a la apertura.  Y estamos viendo que la apertura tiene algo de erosión. En pocas palabras, no existe una transformación inversa  a la apertura. Y la apertura de una imagen F por un elemento estructurante B, que hemos dicho cuadrado, círculo,  triángulo, rombos, lo que sea, pues está definida por esto. Sin embargo, para que quede más claro, la apertura no es más  que una erosión más una dilatación. Primero erosión y después lo dilato. Es decir, primero lo comprimo y después amplío.  Entonces, voy a tener unas características mucho más idóneas, si se dan cuenta, para poder visualizar objetos.  Por ejemplo, bueno, este es el caso más clásico, donde tengo ruidos en el fondo que me gustaría eliminarlos,  pero que ese ruido también está presente justo en la imagen que quiero ver y me daría igual que esté presente.  Eso no lo quiero eliminar porque a lo mejor ese ruido es información propia de la imagen.  Ya me sé mamografía, ya me sé en my right. Entonces, este tipo de operaciones me sirve para detectar.  Puede que ese ruidito de fondo que digo dentro de la imagen, puede que no sea el ruido, puede que sea un calcinoma,  puede que sea algún problema baso celular, que esté presente cuando hago imágenes y demás.  ¿Estás simplificando la geometría? Exacto, estoy simplificando todo esto.  Vale, entonces la apertura, como hemos dicho que es la mezcla, pues primero comprimo, amplío y tengo una mejora de la imagen.  En pocas palabras, la apertura de una imagen binaria de un elemento estructurante cuadrado me va a ayudar a quitarme ese ruido de fondo,  pero el ruido presente en la parte de la entera de la imagen, la parte que mayor información tiene para mi proyecto, para mi objetivo,  la mantiene tal cual y no se vea muy influencia.  Vale, caso contrario, la clausura, la clausura de un elemento estructurante B notada por las ecuaciones, define como la dilatación seguido de una erosión.  Primero dilato, que me puedo comer un poco más la imagen que venga o el ruido y demás y después erosión, después vuelvo a comprimir.  Es como amplío y desinflo y desinflo.  Yo creo que es el mejor ejemplo.  Y puedo obtener las imágenes que quiero.  Ahora, ¿quién me puede decir cuál es clausura y cuál es apertura?  ¿Cuál de las dos?  Alguien de apertura y el A.  Clausura y apertura respectivamente. Muy bien.  Recuerde que tenemos que ver en colores inversos.  Eso es lo que me he olvidado comentarles también aquí.  Pero bueno, si invertimos colores, pues obviamente es tal cual.  Ahora, en esta imagen, piensen en inverso.  Vuelca en imagen o vuelca en cerebro.  O haganse un inversa de imagen.  ¿Dónde estoy aplicando la clausura?  ¿Y dónde la apertura?  Piensen en inverso.  Entonces, apertura, izquierda y clausura derecha.  Carlos, lo has hecho bien.  Ahora, lo que decía, pensemos al revés.  Para poder, si esta es mi imagen original,  si este es mi imagen original,  aquí hay pequeños detalles que, bueno,  usando el GIMP hubiera sido mucho más idóneo el ejemplo.  Me puede servir esta imagen para hacer una erosión  y poder separar aquellos elementos que están casi juntos\n",
      "Intervalo 30-40 minutos:  y poder discernir algunos problemas.  Y también, si no hay una erosión,  si no hay una erosión,  si no hay una erosión,  si no hay una erosión,  si no hay una erosión,  si no hay una erosión,  si no hay una erosión,  si no hay una erosión,  si no hay una erosión,  si no hay una erosión,  si no hay una erosión,  si no hay una erosión,  si no hay una erosión,  si no hay una erosión,  si no hay una erosión,  si no hay una erosión,  mira, quédate con esto,  apertura es sumar erosión más dilatación,  pero en ese orden, primero contraigo y después dilato,  eso es apertura.  Y clausura, primero dilato y después erosiono, ¿vale?  Si te quedas con eso, Edwin,  ya lo tienes, ¿vale?  Eso es lo principal que tienen que saber, ¿vale?  Ahora, las aplicaciones, ¿para qué lo voy a utilizar?  ¿Para qué me va a servir?  Reducción de ruido, ¿se acuerdan lo que les dije?  De la huella dactilar, que era el ejemplo más sencillo,  para la detección de huella dactilares,  para eliminar el ruido,  porque hay ruido presente,  que alguien ha puesto su dedo,  que alguien ha soplado, que hay polvo y demás,  pues utilizando apertura y clausura,  que viene a ser la conjunción de todas estas,  pues puedo obtener una imagen más idónea de la huella dactilar.  ¿Quién dice huella dactilar?  Dice imagen PET o MRI también, ¿vale?  Ahora, estas son las tres básicas, las cuatro básicas,  pero también hay otras.  Aquí está el gradiente morfológico, ¿vale?  El gradiente morfológico, por ejemplo,  me sirve para los límites de objetos  que están asociados con variaciones a altos niveles de grises,  normalmente con valores de grises, ¿vale?  No tanto como blanco y negro, ¿vale?  El gradiente morfológico es la clave para resaltar  estas variaciones de escalas de grises  y por otro lado resalta las variaciones,  las necesidades de píxel dentro de un entorno definido, ¿vale?  Para ello hay que tener en cuenta que uno,  primero hay que diferenciar la dilatación y la erosión,  que se conoce como gradiente de Bauchche, ¿vale?  Después la diferencia aritmética  entre dilatación y la imagen original,  es decir, ya estoy jugando con imágenes originales  más los resultados de las imágenes ya aplicadas  con operadores morfológicos, ¿vale?  Digamos que este gradiente morfológico es una fusión  o es una combinación de imágenes  entre resultados con lo original, ¿vale?  Por ejemplo, esta es una imagen de diferencia  entre dilatación y erosión, ¿vale?  Pero quedándose con los contornos,  entonces estoy aplicando el gradiente morfológico  que me sirve a través del uso de la imagen original, ¿vale?  Estoy sumando o restando la imagen original.  ¿Para qué me sirve esto, si se dan cuenta,  estos gradientes de erosión?  ¿Para qué me sirve si me quedo con el contorno morfológico?  Pues lo primero que se le tiene que venir a la mente  es cartografía, ¿vale?  Me sirve para poder quedarme con los contornos,  con el ground truth de una imagen satelital, ¿vale?  Que me va a servir para análisis cartográfico,  que me va a servir para análisis de ciudades  en aplicaciones militares, de objetivos militares,  cartografía, incluso la parte de orto rectificación  de imágenes, ¿vale?  Utilizando esta combinatoria, ¿vale?  Pero el gradiente morfológico no es el único.  También está el top hat, ¿vale?  Estos filtros morfológicos me ayudan, o sea, son combinaciones.  El operador top hat me va a recuperar la estructura  mediante diferencias aritméticas entre imágenes  y aperturas o clausuras, ¿vale?  Lo bueno de esto es que me va a dar un tipo de desenfoque  a las imágenes que es fácil, que es muy relevante, ¿vale?  Me va a eliminar directamente los objetos que no son relevantes.  Se va a quedar con lo principal, con lo que estaría delante, ¿vale?  Esto sería más o menos los dos operadores tipo top hat  que están presentes, el white top y el black top hat, ¿vale?  Quedarse con las ecuaciones, pero esto es básicamente  más informativo, más que me digan que les voy a poner  ecuaciones en el examen, ¿no?  Pero es para que lo sepan cómo se podría mejorar las imágenes.  Y el resultado, por ejemplo, del top hat es la que vemos aquí, ¿vale?  Tengo mi imagen original y la apertura, ¿no?  Que las mantengo en alta frecuencia.  Entonces, el resultado de la combinatoria de ambas  me va a resaltar los bordes como tal.  La clase pasada vimos detección de bordes, ¿no?  Y yo les decía, vale, el tipo de detección de bordes  me va a ser útil dependiendo del tipo de proyecto  o del problema que quiera resolver.  En este caso, con operadores morfológicos también los puedo usar.  Pero el costo computacional en CPU es más grande.  Si utilizo GPU, mucho más rápido.  Quedarse siempre con eso.  Aquí tenemos, por ejemplo, la imagen original, ¿vale?  Aquí tenemos una clausura con el large square, ¿vale?  Con un operador.  Hacemos un black top hat.  Y puedo hacer incluso una división de ellas.  Entonces, teniendo en cuenta esto,  estoy obteniendo las imágenes que están al fondo  y me he quitado el ruido de adelante gracias a mínimos cuadrados.  Y puedo ver la imagen mucho mejor que la original.  Y esto ya me sirve para mis análisis, ¿vale?  Esto ya me sirve para entrenar modelos o lo que sea, ¿vale?  Solamente con operadores morfológicos.  No estoy haciendo nada más.  No estoy haciendo otro tipo de operación, ¿vale?  Javier, una pregunta.  Y ese tipo de combinaciones,  cuando hace ese tipo de combinaciones,  ¿no altera el documento original, la imagen original,  que puede llevar a que llegue a una interpretación diferente  a lo que en realidad es?  A ver, podría darse eso.  No te niego porque la exactividad del 100% no existe.  Pero si te das cuenta,  esta pequeña degradación que existe en la imagen  puede ser no tanto por problema,  o sea, no puede ser que la imagen original, ¿vale?  O lo que estéis viendo, lo que vea el documento,  antes de lo que vea la máquina,  de esta degradación a este nivel, ¿vale?  No sé si te das cuenta, esta degradación se mantiene.  Es decir, que la imagen que has capturado  tiene un ruido externo al sistema,  llámese la luz, un elemento de adquisición de imagen  que esté gastado o cualquier otra cosa,  que me ha producido ese ruido.  Entonces, con operadores morfológicos,  lo que ha sido, si lo quieres ver,  ha sido resaltar esa parte que estaba, en este caso, afectada  y ha salido a la luz todo ese ruido de fondo, ¿vale?  Bueno, era ruido de fondo,  pero ha salido a la luz como parte de la imagen, ¿sí?  Ahora, repito, no es el 100% exacto,  no podemos asegurar que sí haya sido problema de la máquina.  Puede que ese problema médico o ambiental en el espacio,  lo que sea, tenga esas características,  pero por lo menos ya me sirve para discernir algunas cosas, ¿sí?  Vale, por ejemplo,  para la parte de detecciones de letras, el OCR,  bien se puede aplicar este tipo de cosas  para hacer las letras más gordas, más gruesas y demás  y hacer un análisis.  ¿Para qué me sirve esto?  Imagínense, estoy utilizando una imagen  que no tiene mucha velocidad en la captura de matrículas de coche,  pues obtendré una imagen muy distorsionada, muy ensanchada,  y aplicando algunos operadores morfológicos,  pues puedo tener ya la matrícula o la patente del coche  o del carro o del auto,  y puedo hacer un OCR y saber qué coche era el que comete la infracción.  Edwin, ¿como ejemplo puede ser alguien  que pulsó un dedo sobre la cámara celular en su...  Sí, puede ser un ejemplo.  O un maillot ropturador, claro, perfecto.  Bueno, seguimos.  Otros operadores que existen,  el smoothing, el suavizado, el gradiente y el aplaciano.  Estas son sus figuras.  Las características que tiene son las que vemos aquí.  Una dilatación, está ensanchando, la erosión me la recorta,  el smoothing le da un borroso,  el gradiente se me está quedando ya con los bordes,  y el aplaciano queda a los bordes, pero está resaltando el fondo.  Ahora, mis aplicaciones reales  que puedo enseñar y que se los he dicho,  figura de la NASA, 1983.  Esto es una supernova que fue sacada con rayos X.  Para poder ver a mayor detalle, con mayor exactitud,  la belleza de esa supernova que se estaba produciendo,  se han utilizado estos operadores matemáticos  para poder reducir ese ruido que no me dejaba ver.  Este cúmulo, pues claro, es las microexplosiones  que van surgiendo, pero la explosión grande,  sí, la explosión grande, al final la logro obtener.\n",
      "Intervalo 40-50 minutos:  Puedo ver esa belleza del espacio.  Lo mismo pasa con la parte de Hellcat.  Esto sería un... es un Tacx, si no me equivoco.  O un CT, es un CT de scan.  Y con esto puedo ver aquellos detalles.  Y finalmente, esta es la que más me gusta,  porque es una aplicación donde se ha utilizado la dilatación  para una mejora del MRI, para una imagen cerebral,  para detección de inflamaciones a través de golpe.  Esto es un ejemplo que se ha sacado en un artículo del...  creo que es del año 2008, 2010, no estoy seguro,  donde se analizaba qué problemas tenían los boxeadores  después de la lucha.  Entonces, estos operadores morfológicos me sirven para todo esto.  Sí, es la clave para poderlos implementar en algunos ejercicios,  bueno, en algunas imágenes, perdón,  que puedan ser de mayor ayuda a obtener información necesaria.  Les recuerdo, no estamos aplicando filtros,  estamos aplicando solamente, bueno, es un filtro,  el operador morfológico, ¿vale?  Pero no me estoy yendo a un análisis en frecuencia,  no me estoy yendo a un análisis de webles ni nada,  es solamente espacial, ¿vale?  Como hemos estado viendo el análisis espacial,  pues esto viene aquí con nivel de... ahora.  Por ejemplo, esto es tarea para la casa,  este ejercicio y el siguiente.  Esta es mi imagen original, díganme cuál de estos es dilatación,  erosión o clausura.  O bueno, los cuatro principales, ¿vale?  Me los decidís en el foro, ¿vale?  Este es el original, ver cuál amplía, cuál reduce,  cuál es la combinación de un lado por el otro lado, etcétera, ¿vale?  Esto por un lado y este, el que pueda decirme el primero de ustedes,  no ahora, sino en el foro,  ¿cuántos puntos hay?  Tiene 0,000, un punto más extra en el examen, ¿vale?  Ya les he dicho todas las respuestas que están dando,  yo me voy apuntando todos los que están participando  de cara que el examen final, pues,  esas décimas que les falte, yo que sé, para redondearlo y demás,  pues lo puedo poner.  5,000, casi la verdad, Carlos, pero no.  ¿Vale?  Y nada, eso es todo.  Les he dicho un tema rápido en el dominio del espacio  y he preferido ponerlo aquí antes de metérnos en frecuencia.  Más que todo porque es muy espacial,  es muy, como dijo,  se me ha ido, como dijo Juan Carlos,  es el pincel o el magic del Illustrator o del GIMP, o sea,  eso es lo que hace, es un operador morfológico lo que hace esa herramienta, ¿vale?  Y ahora mismo se ha dado cuenta que nos ayuda.  5,000, uno por dos, no.  ¿Puntos de luz o pixel?  No, esta aquí son pixels.  Aquí, bueno, a ver,  cuando me refiero a los puntitos, tendrán que hacer un operador morfológico,  por ejemplo, para que esto que se ve aquí, que parecen que son cuatro,  pues sean tres distintos, ¿saben?  Y contarme los puntos, ¿vale?  Si podéis, si no, no pasa nada, ¿eh?  Yo les digo, hacer un operador morfológico y después contar las circunferencias  o reducirlos hasta tener un pixel y contar los pixels.  ¿Vale?  ¿Dudas, consultas?  Antes de que terminemos, que ya nos hemos pasado un poquillo.  ¿Hay otra forma de contar figuras de forma automática?  Si le preguntas a nuestro amigo que a veces nos ayuda con las cosas,  a lo mejor lo encuentras.  Hay un método de más, la Vienotron Python, que son directos,  que utilizan OpenCV. No digo más.  Pues nada, muchísimas gracias.  Consultar a vuestros mentores, por favor.  Yo también voy a preguntar qué fecha vamos a tener la clase  para los del Per 2024.  Y a lo de 2017, es este viernes 17, ¿eh?  ¿En colapos usas CV?  Sí. Las herramientas de IA que usan la médica  para, por ejemplo, ecografía, obtener más información,  usan estos operadores.  Algunos sí.  Hay una red neuronal de IA de imagen que utiliza este operador,  el de clausura, creo, si más no me equivoco.  ¿Qué pasa con la fecha de límite? Pues se tendrá que estirar.  Deja de compartir.  Las notas de la asistencia es la clave en vivo.  No se están registrando.  Si no se están registrando,  si nos echas una mano.  No se preocupen por eso.  Ustedes, por la asistencia, hay una herramienta que me dice a mí.  Este alumno ha venido más veces, este menos veces,  y de ahí se saca esa nota, ¿vale? No se preocupen.  Pues si no hay dudas, aquí lo dejamos.  Nos vemos la próxima semana.  Vamos a ver FURIER ahí la próxima semana.  Venirse despejados.  Es un pelín espeso, no les digo que no.  A mí me gusta más.  Paramos la grabación.\n"
     ]
    }
   ],
   "source": [
    "current_interval = 0\n",
    "accumulated_text = \"\"\n",
    "\n",
    "for segment in result7[\"segments\"]:\n",
    "    start = convert_to_min_sec(int(segment[\"start\"]))\n",
    "    end = convert_to_min_sec(int(segment[\"end\"]))\n",
    "    text = segment[\"text\"]\n",
    "\n",
    "    # Verificar si el segmento sigue dentro del intervalo actual de 15 minutos.\n",
    "    if start // n == current_interval:\n",
    "        accumulated_text += \" \" + text\n",
    "    else:\n",
    "        # Imprimir el texto acumulado para el intervalo actual.\n",
    "        print(f\"Intervalo {current_interval*n}-{(current_interval+1)*n} minutos: {accumulated_text}\")\n",
    "\n",
    "        # Actualizar el intervalo y reiniciar el texto acumulado.\n",
    "        current_interval = start // n\n",
    "        accumulated_text = text\n",
    "\n",
    "# No olvidar imprimir el último intervalo acumulado fuera del ciclo.\n",
    "print(f\"Intervalo {current_interval*n}-{(current_interval+1)*n} minutos: {accumulated_text}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
